{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "POZtiiUy0BHS",
        "Jr4PQPY40Ly-",
        "VRi87ivfpF5l",
        "lfdYf0QVl71U"
      ],
      "authorship_tag": "ABX9TyPEfktcftinb4nLBnmlL5fL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marioskyriacou/Bipartite_Graphs_Manual/blob/main/Classes_Bipartite_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "POZtiiUy0BHS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RocKeywfrOK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76eeb88d-d87d-4c95-c811-f2dcaab096fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpyro in /usr/local/lib/python3.10/dist-packages (0.15.3)\n",
            "Requirement already satisfied: jax>=0.4.25 in /usr/local/lib/python3.10/dist-packages (from numpyro) (0.4.33)\n",
            "Requirement already satisfied: jaxlib>=0.4.25 in /usr/local/lib/python3.10/dist-packages (from numpyro) (0.4.33)\n",
            "Requirement already satisfied: multipledispatch in /usr/local/lib/python3.10/dist-packages (from numpyro) (1.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from numpyro) (1.26.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from numpyro) (4.66.6)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.25->numpyro) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.25->numpyro) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.25->numpyro) (1.13.1)\n",
            "Requirement already satisfied: funsor in /usr/local/lib/python3.10/dist-packages (0.4.5)\n",
            "Requirement already satisfied: makefun in /usr/local/lib/python3.10/dist-packages (from funsor) (1.15.6)\n",
            "Requirement already satisfied: multipledispatch in /usr/local/lib/python3.10/dist-packages (from funsor) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.10/dist-packages (from funsor) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from funsor) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from funsor) (4.12.2)\n"
          ]
        }
      ],
      "source": [
        "# Installations\n",
        "!pip install numpyro\n",
        "!pip install funsor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import numpyro\n",
        "import os\n",
        "import jax\n",
        "import time\n",
        "import pickle\n",
        "import networkx as nx\n",
        "import matplotlib\n",
        "import seaborn as sns\n",
        "import scipy.io\n",
        "\n",
        "print(f'Pandas: {pd.__version__}')\n",
        "print(f'Numpy: {np.__version__}')\n",
        "print(f'Numpyro: {numpyro.__version__}')\n",
        "print(f'Networkx: {nx.__version__}')\n",
        "print(f'Jax: {jax.__version__} // Jax Devices:{jax.devices()[0]}')\n",
        "print(f'Pickle: {pickle.format_version}')\n",
        "print(f'Matplotlib: {matplotlib.__version__}')\n",
        "print(f'Seaborn: {sns.__version__}')"
      ],
      "metadata": {
        "id": "9wIJFQMDfr_Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bcf3cea-9617-444a-b209-20f0630f5c51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pandas: 2.2.2\n",
            "Numpy: 1.26.4\n",
            "Numpyro: 0.15.3\n",
            "Networkx: 3.4.2\n",
            "Jax: 0.4.33 // Jax Devices:TFRT_CPU_0\n",
            "Pickle: 4.0\n",
            "Matplotlib: 3.8.0\n",
            "Seaborn: 0.13.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Jax\n",
        "from jax import numpy as jnp\n",
        "from jax import random as jrandom\n",
        "from jax import vmap\n",
        "import jax.scipy.special as special\n",
        "import jax.random as random\n",
        "\n",
        "\n",
        "# NumpyRO\n",
        "import numpyro.distributions as dist\n",
        "from numpyro.diagnostics import hpdi\n",
        "from numpyro.infer import (\n",
        "    MCMC,\n",
        "    HMC,\n",
        "    MixedHMC,\n",
        "    init_to_value,\n",
        "    NUTS,\n",
        "    DiscreteHMCGibbs)\n",
        "from numpyro.infer import Predictive\n",
        "from numpyro.distributions import constraints\n",
        "\n",
        "## Generate a graph from the prior model\n",
        "from jax import random as random_jx\n",
        "from jax import lax, jit, ops\n",
        "from numpyro.infer import Predictive\n",
        "\n",
        "# table creation\n",
        "from tabulate import tabulate\n",
        "\n",
        "#mount the drive so that I can read and write on files on my drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqqZELxcfr8-",
        "outputId": "5a22cca3-015b-40be-ed62-40bdce3a24b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Classes"
      ],
      "metadata": {
        "id": "Jr4PQPY40Ly-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I create the etBFRY function by making a class\n",
        "class etBFRY(numpyro.distributions.Distribution):\n",
        "    arg_constraints = {\n",
        "        # \"sigma\": constraints.unit_interval,\n",
        "        # \"alpha\": constraints.positive,\n",
        "        \"tau\": constraints.positive\n",
        "    }\n",
        "    support = constraints.positive\n",
        "    reparametrized_params = [\"sigma\", \"alpha\", \"tau\"]\n",
        "\n",
        "    def __init__(self, L, alpha, sigma, tau):\n",
        "        #read parameters\n",
        "        self.L = L\n",
        "        self.sigma = sigma\n",
        "        self.tau = tau\n",
        "        self.alpha = alpha\n",
        "        self.t_as = (self.sigma*self.L/self.alpha)**(1.0/self.sigma)\n",
        "        super().__init__(batch_shape = (1, self.L), event_shape=())\n",
        "\n",
        "    def sample(self, key, sample_shape=()):\n",
        "        #generate from the distribution\n",
        "        shape = sample_shape + self.batch_shape\n",
        "        G_dist = dist.Gamma(1.0-self.sigma, 1.0)\n",
        "        G = G_dist.sample(key,shape) if isinstance(self.sigma, float) else G_dist.sample(key,(1,))\n",
        "        U_dist = dist.Uniform()\n",
        "        U = U_dist.sample(key,shape)\n",
        "        R = G*((1.0-U)*(self.t_as+self.tau)**self.sigma + U*self.tau**self.sigma)**(-1.0/self.sigma)\n",
        "        return R\n",
        "\n",
        "    def log_prob(self, value):\n",
        "      #evaluate the log probability density\n",
        "      # return jnp.log(self.sigma)+(-1-self.sigma)*jnp.log(value)-self.tau*value+jnp.log(1-jnp.exp(-value*self.t_as))-special.gammaln(1-self.sigma)-jnp.log((self.tau+self.t_as)**self.sigma-self.tau**self.sigma)\n",
        "      return jnp.log(self.sigma / ((self.tau+self.t_as)**self.sigma-self.tau**self.sigma))+(-1-self.sigma)*jnp.log(value)-self.tau*value+jnp.log(1-jnp.exp(-value*self.t_as))-special.gammaln(1-self.sigma)\n",
        "\n"
      ],
      "metadata": {
        "id": "T7fpQz0vfr6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class check_values():\n",
        "  def __init__(self, rng_key):\n",
        "    self.rng_key = rng_key\n",
        "\n",
        "  def check_alpha(self, alpha, value_name):\n",
        "    #create a smaple from the below distribution , if a value is not given\n",
        "    # Distributions: dist.HalfNormal(20) (mean=talpha*1.25=30*1.25 ~37) (var = (1-2pi)scale*2) //  dist.HalfCauchy(20) // dist.Gamma(90,3)\n",
        "    if alpha is None:return numpyro.sample(str(value_name), dist.HalfNormal(20), rng_key= self.rng_key)\n",
        "    else: return numpyro.deterministic(str(value_name),alpha) # if given a value\n",
        "\n",
        "\n",
        "  def check_sigma(self, sigma, value_name):\n",
        "    #create a smaple from the below distribution , if a value is not given\n",
        "    # Distributions:  dist.HalfNormal(1) 1 - sigma ~ Improper Unif //  dist.Uniform(0,1) //  TruncatedDistribution(dist.Normal(1,1), low=0, high=1) //\n",
        "    # truncated_normal_model(num_observations, high=1, x=None) //  TruncatedNormal(loc=1, scale=1,low=0, high=1) //  dist.TruncatedDistribution(dist.Normal(0.5, 1), low=0.01, high=2)\n",
        "    if sigma is None:return  numpyro.sample( str(value_name), dist.Beta(2,10), rng_key= self.rng_key) # (1/9, 1) (1,4)\n",
        "    else: return  numpyro.deterministic(str(value_name), sigma)\n",
        "\n",
        "\n",
        "  def check_tau(self, tau, value_name):\n",
        "     #create a smaple from the below distribution , if a value is not given\n",
        "      if tau is None:return numpyro.sample(str(value_name), dist.HalfNormal(1), rng_key= self.rng_key)\n",
        "      else: return numpyro.deterministic(str(value_name),tau)\n",
        "\n",
        "  def check_a(self, a, p, value_name):\n",
        "    if a is None:return numpyro.sample( str(value_name), dist.HalfNormal(2), sample_shape=(p, ), rng_key= self.rng_key)\n",
        "    else:return numpyro.deterministic(str(value_name), a)\n",
        "\n",
        "  def check_b(self, b, p, value_name):\n",
        "    # b = numpyro.sample(\"b\", dist.Gamma(0.01, 0.01), sample_shape=(p,))\n",
        "    if b is None:return numpyro.sample( str(value_name), dist.HalfNormal(2), sample_shape=(p, ), rng_key= self.rng_key)\n",
        "    else: return numpyro.deterministic(str(value_name), b)\n",
        "\n",
        "  def affiliation_scores(self, scores, W, alpha, sigma, tau, a, b, nodes, scores_name, w_name):\n",
        "    if scores is None:\n",
        "        # Ditributions: dist.Beta(a, b), sample_shape=(L,p) // dist.Gamma(a, b), sample_shape=(L, p) // dist.HalfNormal(4), sample_shape=(L,)\n",
        "        scores = numpyro.sample(str(scores_name), dist.Gamma(a, b), sample_shape=(nodes, ), rng_key= self.rng_key) # if a, b vectors\n",
        "    else:\n",
        "        scores = numpyro.deterministic(str(scores_name), scores)\n",
        "\n",
        "    if W is None:\n",
        "      W = numpyro.sample(str(w_name), etBFRY(nodes, alpha, sigma, tau), rng_key= self.rng_key)\n",
        "      W = W.reshape(nodes,)\n",
        "    else:\n",
        "      W = numpyro.deterministic(str(w_name), W)\n",
        "\n",
        "    return scores, W"
      ],
      "metadata": {
        "id": "DqBLyIeR-Y1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Details Class"
      ],
      "metadata": {
        "id": "VRi87ivfpF5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Network_details():\n",
        "\n",
        "  def add_zeros(adj_matrix, ratio=1.1):\n",
        "      # add zeros to the adj matrix based on a given ratio\n",
        "      print(f'Adj matrix: {adj_matrix.shape} ')\n",
        "      # Dimenshion Shapes\n",
        "      L_upper, L_lower = adj_matrix.shape[0], adj_matrix.shape[1]\n",
        "      # Dimenshion new shapes\n",
        "      L_upper_new, L_lower_new = int(L_upper * ratio), int(L_lower * ratio) # ratio of update shape 1.5\n",
        "      # new matrix\n",
        "      matrix_new = np.zeros((L_upper_new, L_lower_new))\n",
        "      matrix_new[0:L_upper, 0:L_lower]  = np.array(adj_matrix)\n",
        "      print(f'New Adj matrix: {matrix_new.shape} ')\n",
        "      return matrix_new, L_upper_new, L_lower_new\n",
        "\n",
        "  def network_statistics(typeA_nodes, typeB_nodes, adj_matrix):\n",
        "      # main Vairbles\n",
        "      num_edges = np.sum(adj_matrix)\n",
        "\n",
        "      #Statistics\n",
        "      top_average = num_edges / typeA_nodes\n",
        "      bottom_average = num_edges / typeB_nodes\n",
        "      graph_average  = (2*num_edges) / (top_average + bottom_average)\n",
        "      density = num_edges / (typeB_nodes * typeA_nodes)\n",
        "      sparsity = num_edges / (typeB_nodes + typeA_nodes)**2\n",
        "\n",
        "      # Prepare data for tabulation\n",
        "      table_data = [\n",
        "          ['# Top Nodes', typeB_nodes],\n",
        "          ['# Bottom Nodes', typeA_nodes],\n",
        "          ['# Edges', num_edges],\n",
        "          ['Top average degree ', round(top_average, 4)],\n",
        "          ['Bottom Average degree', round(bottom_average, 4)],\n",
        "          ['Graph Average degree', round(graph_average, 4)],\n",
        "          ['Density', round(density, 4)],\n",
        "          ['Sparsity', round(sparsity, 4)]]\n",
        "\n",
        "      print(tabulate(table_data, headers=['Metric', 'Value'], tablefmt='grid'))\n",
        "\n",
        "  def plt_deg_distr(deg, title, binned=False):\n",
        "    # Plot Degree Distributions:\n",
        "    # Count the degrees of each nodes for one type of nodes (A or B) -> eliminate the 0 connections\n",
        "    # plot the degree distribution\n",
        "    deg = deg[deg > 0]\n",
        "    num_nodes = len(deg)\n",
        "    freq = pd.Series(deg).value_counts().to_dict()  # Count the occurrences of each degree\n",
        "\n",
        "    if binned == True:\n",
        "        freq = [x / num_nodes for x in list(freq.values())] ## Normalize frequency values\n",
        "        bins = np.exp(np.linspace(np.log(min(freq)), np.log(max(freq)), 20)) #logarithmic bins for frequency values\n",
        "        sizebins = (bins[1:] - bins[:-1]) #width of each bin\n",
        "        # sizebins = np.append(sizebins, 1)\n",
        "        counts = np.histogram(freq, bins=bins)[0]\n",
        "        freq = counts/sizebins #Normalize counts by bin size\n",
        "        freq = freq/sum(freq) #Further normalize frequency so the total sums to 1\n",
        "\n",
        "        plt.figure()\n",
        "        plt.plot(bins[:-1], freq, 'bo', label='empirical')\n",
        "        plt.legend()\n",
        "    else:\n",
        "        plt.figure()\n",
        "        plt.plot(list(freq.keys()), [np.exp(np.log(x) - np.log(num_nodes)) for x in list(freq.values())], 'bo', label='deg nodes')\n",
        "        plt.legend()\n",
        "\n",
        "    plt.title(f'{title} Degree Distribuiton')\n",
        "    plt.xscale('log')\n",
        "    plt.yscale('log')\n",
        "    plt.xlabel('deg')\n",
        "    plt.ylabel('frequency')"
      ],
      "metadata": {
        "id": "npAAgc70pKON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simulation Class"
      ],
      "metadata": {
        "id": "lfdYf0QVl71U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Bipartite_Network():\n",
        "  def __init__(self, args):\n",
        "    self.args = args\n",
        "    print(f'Arguments Parameters:\\n{self.args.items()}')\n",
        "    self.L, self.L_prime = self.args['L'], self.args['L_prime']\n",
        "    self.p = self.args['p']\n",
        "\n",
        "    # hyperparameters of the BFRY upper & bottom\n",
        "    self.talpha, self.talpha_prime = self.args['alpha'], self.args['alpha_prime']\n",
        "    self.tsigma, self.tsigma_prime = self.args['sigma'], self.args['sigma_prime']\n",
        "    self.ttau, self.ttau_prime = self.args['tau'], self.args['tau_prime']\n",
        "    self.wi0, self.wj0 = self.args['wi0'], self.args['wj0']\n",
        "\n",
        "   #levels of affiliations to communities\n",
        "    self.ta, self.ta_prime = self.args['a'], self.args['a_prime']\n",
        "    self.tb, self.tb_prime = self.args['b'], self.args['b_prime']\n",
        "    self.scores, self.scores_prime = self.args['scores'], self.args['scores_prime']\n",
        "    # adjacency matrix\n",
        "    self.Z_obs = self.args['Z_obs']\n",
        "\n",
        "    # key used for samples with jax\n",
        "    self.rng_key, self.rng_key_predict = random_jx.split(random_jx.PRNGKey(22))\n",
        "\n",
        "  def update_args_dict(self):\n",
        "    # update the arguments dictonary with the new values\n",
        "    self.args['adj_matrix'] = self.adj_matrix\n",
        "    self.args['scores'] =  self.scores\n",
        "    self.args['scores_prime'] =  self.scores_prime\n",
        "    self.args['wi0'] = self.wi0\n",
        "    self.args['wj0'] = self.wj0\n",
        "    return self.args\n",
        "\n",
        "  ### Return Functions####\n",
        "  def get_update_args(self):\n",
        "    return self.args\n",
        "  def get_draws(self):\n",
        "    return  self.Bipartite_graph_draws\n",
        "\n",
        "  def simulate_bipartite_network(self):\n",
        "    # Draw bipartite graph\n",
        "    Bipartite_graph_predictive = Predictive(self.bipartite_network_model, num_samples=self.args[\"batch_size\"])\n",
        "    self.Bipartite_graph_draws = Bipartite_graph_predictive(self.rng_key_predict)\n",
        "    self.adj_matrix = self.Bipartite_graph_draws['Z_obs'][0] # get adj matrix\n",
        "\n",
        "\n",
        "  def bipartite_network_parameters(self):\n",
        "    check = check_values(self.rng_key_predict)\n",
        "    self.tau, self.tau_prime = check.check_tau(self.ttau, 'tau'), check.check_tau(self.ttau_prime, 'tau_prime')\n",
        "    self.sigma, self.sigma_prime = check.check_sigma(self.tsigma, 'sigma'), check.check_sigma(self.tsigma_prime, 'sigma_prime')\n",
        "    self.alpha, self.alpha_prime = check.check_alpha(self.talpha, 'alpha'), check.check_alpha(self.talpha_prime, 'alpha_prime')\n",
        "    self.a, self.a_prime = check.check_a(self.ta, self.p, 'a'), check.check_a(self.ta_prime, self.p, 'a_prime')\n",
        "    self.b, self.b_prime = check.check_b(self.tb, self.p, 'b'), check.check_b(self.tb_prime, self.p, 'b_prime')\n",
        "\n",
        "    print(f'Upper hyperparameters:\\n tau:{self.tau}, sigma:{self.sigma}, alpha:{self.alpha}, a:{self.a}, b:{self.b}')\n",
        "    print(f'Bottom hyperparameters:\\n tau:{self.tau_prime}, sigma:{self.sigma_prime}, alpha:{self.alpha_prime}, a:{self.a_prime}, b:{self.b_prime}')\n",
        "\n",
        "    ######################## Affiliation Scorres for Upper Nodes & Wi0 ########################\n",
        "    self.scores, self.wi0 = check.affiliation_scores(scores=self.scores, W=self.wi0,\n",
        "                                                     alpha=self.alpha, sigma=self.sigma, tau=self.tau,\n",
        "                                                     a=self.a, b=self.b, nodes=self.L,\n",
        "                                                     scores_name='scores', w_name='wi0')\n",
        "\n",
        "    ###################### Affiliation Scorres for Lower Nodes & Wj0 ##########################\n",
        "    self.scores_prime, self.wj0 = check.affiliation_scores(scores=self.scores_prime, W=self.wj0,\n",
        "                                                     alpha=self.alpha_prime, sigma=self.sigma_prime, tau=self.tau_prime,\n",
        "                                                     a=self.a_prime, b=self.b_prime, nodes=self.L_prime,\n",
        "                                                     scores_name='scores_prime', w_name='wj0')\n",
        "\n",
        "    print(f'Scores: {self.scores.shape} Wi0:{self.wi0.shape}\\nScores_prieme:{self.scores_prime.shape} Wj0:{self.wj0.shape}')\n",
        "\n",
        "  def bipartite_network_model(self):\n",
        "    ############################## Simulate the adj matrix #################################\n",
        "    Wi = jnp.transpose(jnp.multiply(jnp.transpose(self.scores), self.wi0))\n",
        "    Wj = jnp.transpose(jnp.multiply(jnp.transpose(self.scores_prime), self.wj0))\n",
        "    self.W_adj = jnp.matmul(Wi, jnp.transpose(Wj))## (L,T,p) x (L_prime,T,p)\n",
        "    #numpyro.sample(\"N_obs\", dist.Poisson(ww), obs=N_obs)\n",
        "    numpyro.sample(\"Z_obs\", dist.Bernoulli(1-jnp.exp(-2*self.W_adj)), obs=self.Z_obs)\n"
      ],
      "metadata": {
        "id": "hJVtLKev0rEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Infer Class"
      ],
      "metadata": {
        "id": "yCrLAX3_gtKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Bipartite_Network_model(infer_args):\n",
        "    L, L_prime = infer_args['L'], infer_args['L_prime']\n",
        "    p = infer_args['p']\n",
        "\n",
        "    #hyperparameters of the BFRY upper & bottom\n",
        "    talpha, talpha_prime = infer_args['alpha'], infer_args['alpha_prime']\n",
        "    tsigma, tsigma_prime = infer_args['sigma'], infer_args['sigma_prime']\n",
        "    ttau, ttau_prime = infer_args['tau'], infer_args['tau_prime']\n",
        "    wi0, wj0 = infer_args['wi0'], infer_args['wj0']\n",
        "\n",
        "   #levels of affiliations to communities\n",
        "    ta, ta_prime = infer_args['a'], infer_args['a_prime']\n",
        "    tb, tb_prime = infer_args['b'], infer_args['b_prime']\n",
        "    scores, scores_prime = infer_args['scores'], infer_args['scores_prime']\n",
        "\n",
        "    # Adj Matrix\n",
        "    Z_obs = infer_args['Z_obs']\n",
        "    ###########################################################################\n",
        "    # Check parameters\n",
        "    rng_key, rng_key_predict = random_jx.split(random_jx.PRNGKey(2))\n",
        "    check = check_values(rng_key_predict)\n",
        "    tau, tau_prime = check.check_tau(ttau, 'tau'), check.check_tau(ttau_prime, 'tau_prime')\n",
        "    sigma, sigma_prime = check.check_sigma(tsigma, 'sigma'), check.check_sigma(tsigma_prime, 'sigma_prime')\n",
        "    alpha, alpha_prime = check.check_alpha(talpha, 'alpha'), check.check_alpha(talpha_prime, 'alpha_prime')\n",
        "    a, a_prime = check.check_a(ta, p, 'a'), check.check_a(ta_prime, p, 'a_prime')\n",
        "    b, b_prime = check.check_b(tb, p, 'b'), check.check_b(tb_prime, p, 'b_prime')\n",
        "\n",
        "    print(f'Upper hyperparameters:\\n tau:{tau}, sigma:{sigma}, alpha:{alpha}, a:{a}, b:{b}')\n",
        "    print(f'Bottom hyperparameters:\\n tau:{tau_prime}, sigma:{sigma_prime}, alpha:{alpha_prime}, a:{a_prime}, b:{b_prime}')\n",
        "\n",
        "    ######################## Affiliation Scorres for Upper Nodes & Wi0 ########################\n",
        "    scores, wi0 = check.affiliation_scores(scores=scores, W=wi0,\n",
        "                                                     alpha=alpha, sigma=sigma, tau=tau,\n",
        "                                                     a=a, b=b, nodes=L,\n",
        "                                                     scores_name='scores', w_name='wi0')\n",
        "    ###################### Affiliation Scorres for Lower Nodes & Wj0 ##########################\n",
        "    scores_prime, wj0 = check.affiliation_scores(scores=scores_prime, W=wj0,\n",
        "                                                     alpha=alpha_prime, sigma=sigma_prime, tau=tau_prime,\n",
        "                                                     a=a_prime, b=b_prime, nodes=L_prime,\n",
        "                                                     scores_name='scores_prime', w_name='wj0')\n",
        "\n",
        "    print(f'Scores: {scores.shape} Wi0:{wi0.shape}\\nScores_prieme:{scores_prime.shape} Wj0:{wj0.shape}')\n",
        "\n",
        "    ############################## Simulate the adj matrix #################################\n",
        "    Wi = jnp.transpose(jnp.multiply(jnp.transpose(scores), wi0))\n",
        "    Wj = jnp.transpose(jnp.multiply(jnp.transpose(scores_prime), wj0))\n",
        "    W_adj = jnp.matmul(Wi, jnp.transpose(Wj))## (L,T,p) x (L_prime,T,p)\n",
        "\n",
        "    #numpyro.sample(\"N_obs\", dist.Poisson(ww), obs=N_obs)\n",
        "    numpyro.sample(\"Z_obs\", dist.Bernoulli(1-jnp.exp(-2*W_adj)), obs=Z_obs)"
      ],
      "metadata": {
        "id": "LDD50uQV8m56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INFER BIPARTITE NETWORK**"
      ],
      "metadata": {
        "id": "3IMbwNHOnGk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Infer_Bipartite_Network():\n",
        "\n",
        "  def __init__(self, infer_args):\n",
        "    self.infer_args = infer_args\n",
        "    item_key_lst  = [(i, j) for i, j  in self.infer_args.items()]\n",
        "    print(item_key_lst)\n",
        "\n",
        "    # adjacency matrix\n",
        "    self.Z_obs = self.infer_args['Z_obs']\n",
        "    # Infer Parameters\n",
        "    self.num_samples = self.infer_args['num_samples']\n",
        "    self.num_chains = self.infer_args['num_chains']\n",
        "    self.thinning = self.infer_args['thinning']\n",
        "\n",
        "    # key used for samples with jax\n",
        "    self.rng_key, self.rng_key_predict = random_jx.split(random_jx.PRNGKey(2))\n",
        "\n",
        "  #################### INFER NETWORK #########################\n",
        "  def infer_network(self, num_warmup=100):\n",
        "    kernel = NUTS(Bipartite_Network_model)\n",
        "    mcmc = MCMC(kernel,\n",
        "              num_warmup = num_warmup,\n",
        "              num_samples = self.num_samples,\n",
        "              num_chains = self.num_chains,\n",
        "              thinning = self.thinning)\n",
        "    mcmc.run(self.rng_key_predict, infer_args=self.infer_args)\n",
        "    mcmc.print_summary()\n",
        "    self.samples = mcmc.get_samples()\n",
        "\n",
        "  def get_samples(self):\n",
        "    return self.samples\n",
        "\n",
        "  def Plot_Traces(self):\n",
        "    Plot_MCMC_Traces(mcmc_samples=self.samples, arguments=self.infer_args)\n",
        "\n",
        "  def Plot_Histograms(self):\n",
        "    Plot_MCMC_Histograms(mcmc_samples=self.samples, arguments=self.infer_args)"
      ],
      "metadata": {
        "id": "ISZxioN9nHXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PLOT MCMC TRACES**"
      ],
      "metadata": {
        "id": "nVriYOUND357"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Plot_MCMC_Traces(mcmc_samples, arguments):\n",
        "    # Plot the trace plots for MCMC samples of specified parameters\n",
        "    # Variables needed/ # Calculate the number of iterations for each parameter\n",
        "    num_chains = arguments['num_chains']\n",
        "    iters = int(mcm_samples['wi0'].shape[0] / num_chains)\n",
        "    iters_prime = int(mcm_samples['wj0'].shape[0] / num_chains)\n",
        "\n",
        "    # Identify the parameters that need to be plot\n",
        "    parameters_list = ['alpha', 'sigma', 'tau', 'a', 'b', 'alpha_prime', 'sigma_prime', 'tau_prime', 'a_prime', 'b_prime']\n",
        "    plt_params = [key for key, value in arguments.items() if arguments[key] is None and key in parameters_list]\n",
        "\n",
        "    # Plot each parameter in a graph\n",
        "    for param in plt_params:\n",
        "      ## Check if the parameter has multiple subplots\n",
        "      if len(mcm_samples[param].shape) > 1:\n",
        "        num_subplots = mcm_samples[param].shape[1]\n",
        "        fig, axes = plt.subplots(1, num_subplots, figsize=(15, 5))\n",
        "        axes = np.array(axes).flatten()\n",
        "        for i in range(num_subplots):\n",
        "            for j in range(num_chains):\n",
        "              #Plot the MCMC trace for each chain\n",
        "              axes[i].plot(np.arange(iters), mcm_samples[param][:, i][j*iters_prime:(j+1)*iters_prime], 'o-', alpha = 0.8,  label=f'{param}[{i}]')\n",
        "              #plt.plot(np.arange(iters), samples[param][i*iters:(i+1)*iters], 'o-')\n",
        "            axes[i].set_title(f'{param}[{i}]')\n",
        "      else:\n",
        "        #For single-dimensional parameters, create a new figure\n",
        "        plt.figure()\n",
        "        for i in range(num_chains):\n",
        "          plt.plot(np.arange(iters_prime), mcm_samples[param][i*iters_prime:(i+1)*iters_prime], 'o-', alpha = 0.8)\n",
        "        #plt.axhline(y=np.mean(args[param]), color='red')\n",
        "        plt.title(param)"
      ],
      "metadata": {
        "id": "YWwW5U2EDUB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PLOT MCMC HISTOGRAMS**"
      ],
      "metadata": {
        "id": "uNsyOCwcEnMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Plot_MCMC_Histograms(mcmc_samples, arguments):\n",
        "    # Plot the trace plots for MCMC samples of specified parameters\n",
        "    # Variables needed/ # Calculate the number of iterations for each parameter\n",
        "\n",
        "    # Varibles\n",
        "    num_bins=20\n",
        "    num_chains=arguments['num_chains']\n",
        "    iters = int(mcmc_samples['wi0'].shape[0] / num_chains)\n",
        "    iters_prime = int(mcmc_samples['wj0'].shape[0] / num_chains)\n",
        "\n",
        "    # Identify the parameters that need to be plot\n",
        "    parameters_list = ['alpha', 'sigma', 'tau', 'a', 'b', 'alpha_prime', 'sigma_prime', 'tau_prime', 'a_prime', 'b_prime']\n",
        "    plt_params = [key for key, value in arguments.items() if arguments[key] is None and key in parameters_list]\n",
        "\n",
        "    # Plot each parameter in a graph\n",
        "    for param in plt_params:\n",
        "      if len(mcmc_samples[param].shape) > 1:\n",
        "        num_subplots = mcmc_samples[param].shape[1]\n",
        "        fig, axes = plt.subplots(1, num_subplots, figsize=(15, 5))\n",
        "        axes = np.array(axes).flatten()\n",
        "        for i in range(num_subplots):\n",
        "            for j in range(num_chains):\n",
        "               #Plot the MCMC histogram  for each chain\n",
        "              axes[i].hist(mcmc_samples[param][:, i][j*iters_prime:(j+1)*iters_prime], num_bins, alpha = 0.5, label=f'{param}[{i}]')\n",
        "            axes[i].set_title(f'{param}[{i}]')\n",
        "      else:\n",
        "        plt.figure()\n",
        "        for i in range(num_chains):\n",
        "          #For single-dimensional parameters, create a new figure\n",
        "          plt.hist(mcmc_samples[param][i*iters:(i+1)*iters], num_bins, alpha = 0.8, label = str(param) )\n",
        "        #plt.axvline(x=np.mean(args[param]), color='red')\n",
        "        plt.title(param)"
      ],
      "metadata": {
        "id": "12D4gpR6Et3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## INFER-SISMULATE PLOTS\n",
        "\n",
        "This section focuses on plotting the inference and true values for the posterior distributions and the predicted post-degree distribution. Both must use the given ground truth values and the predicted values."
      ],
      "metadata": {
        "id": "Kzr-qaGrnMQd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Posterior Distribution PLots and (Log Plots)**"
      ],
      "metadata": {
        "id": "hVvP-dF19hJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Posterior_distribution(samples_vals, original_val, num_samples, thinning, num_chains, L, L_prime):\n",
        "  # Plot the posterior distribution for the top m nodes based on estimated weights wi0 and wj0.\n",
        "  tgraph_index = 1\n",
        "  m = 50 ## Number of top nodes to plot\n",
        "\n",
        "  # Extract true and estimated values for Wi0\n",
        "  Wi0 = original_val['wi0']\n",
        "  wi0_est = samples_vals['wi0']\n",
        "  # Reshape and calculate the 95% highest posterior density interval (HPDI) for Wi0\n",
        "  wi0_est = wi0_est.reshape(int(num_samples/thinning*num_chains), L)\n",
        "  wi0_est = hpdi(wi0_est, prob=0.95)\n",
        "\n",
        " # Extract true and estimated values for Wj0\n",
        "  Wj0 = original_val['wj0']\n",
        "  wj0_est = samples_vals['wj0']\n",
        "  # Reshape and calculate the 95% highest posterior density interval (HPDI) for Wi0\n",
        "  wj0_est = wj0_est.reshape(int(num_samples/thinning*num_chains), L_prime)\n",
        "  wj0_est = hpdi(wj0_est, prob=0.95)\n",
        "\n",
        "  print(f'Wi0 Shape:{wi0_est.shape}\\nWj0 Shape:{wj0_est.shape}')\n",
        "\n",
        "  #indices of the top m nodes by sorting Wi0 in decreasing order\n",
        "  ind_max_wi0 = np.argsort(-Wi0[tgraph_index, 0, :])[:m] #[tgraph_index, 0, :]\n",
        "  w0i_ci_max = wi0_est[:, ind_max_wi0]\n",
        "\n",
        "  #indices of the top m nodes by sorting Wj0 in decreasing order\n",
        "  ind_max_wj0 = np.argsort(-Wj0[tgraph_index, 0, :])[:m] #\n",
        "  w0j_ci_max = wj0_est[:, ind_max_wj0]\n",
        "\n",
        "  #x-axis for each node index and y-axis for confidence intervals\n",
        "  x = [(i, i) for i in range(m)]\n",
        "  y = [(w0i_ci_max[0, i], w0i_ci_max[1, i]) for i in range(m)]\n",
        "  y_prime = [(w0j_ci_max[0, i], w0j_ci_max[1, i]) for i in range(m)]\n",
        "\n",
        "  #Plots\n",
        "  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\n",
        "  # Upper Nodes\n",
        "  for i, j in zip(x, y):\n",
        "      ax1.plot((i[0], i[1]), (j[0], j[1]), color='blue')\n",
        "      ax1.scatter(i[0], Wi0[tgraph_index, 0, ind_max_wi0[i[0]]], color='red')\n",
        "\n",
        "  ax1.plot((i[0], i[1]), (j[0], j[1]), color='blue', label='95% CI')\n",
        "  ax1.scatter(i[0], Wi0[tgraph_index, 0, ind_max_wi0[i[0]]], color='red', label='true')\n",
        "  ax1.title.set_text('Upper Nodes')\n",
        "  ax1.set_xlabel('index of nodes sorted by decreasing wi0')\n",
        "  ax1.set_ylabel('wi0')\n",
        "\n",
        "  # Lower Nodes\n",
        "  for i, j in zip(x, y_prime):\n",
        "      ax2.plot((i[0], i[1]), (j[0], j[1]), color='blue')\n",
        "      ax2.scatter(i[0], Wj0[tgraph_index, 0, ind_max_wj0[i[0]]], color='red')\n",
        "\n",
        "  ax2.plot((i[0], i[1]), (j[0], j[1]), color='blue', label='95% CI')\n",
        "  ax2.scatter(i[0], Wj0[tgraph_index, 0, ind_max_wj0[i[0]]], color='red', label='true')\n",
        "  ax2.title.set_text('Lower Nodes')\n",
        "  ax2.set_xlabel('index of nodes sorted by decreasing wj0')\n",
        "  ax2.set_ylabel('wj0')\n",
        "\n",
        "  fig.tight_layout()\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "skcKiuEP9g9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Posterior_distribution_Log(sample_vals, original_val, num_samples, thinning, num_chains, L, L_prime):\n",
        "\n",
        "  tgraph_index = 1\n",
        "  m = 50\n",
        "  # Wi0 estimates\n",
        "  Wi0 = original_val['wi0']\n",
        "  wi0_est = sample_vals['wi0']\n",
        "  wi0_est = wi0_est.reshape(int(num_samples/thinning*num_chains), L)\n",
        "  wi0_est = hpdi(wi0_est, prob=0.95)\n",
        "\n",
        "  # Wj0 estimates\n",
        "  Wj0 = original_val['wj0']\n",
        "  wj0_est = sample_vals['wj0']\n",
        "  wj0_est = wj0_est.reshape(int(num_samples/thinning*num_chains), L_prime)\n",
        "  wj0_est = hpdi(wj0_est, prob=0.95)\n",
        "  print(f'Wi0 Shape:{wi0_est.shape}\\nWj0 Shape:{wj0_est.shape}')\n",
        "\n",
        "  ind_min_wi0 = np.argsort(Wi0[tgraph_index, 0, :])[:m]\n",
        "  w0i_ci_min = wi0_est[:, ind_min_wi0]\n",
        "\n",
        "  ind_min_wj0 = np.argsort(Wj0[tgraph_index, 0, :])[:m]\n",
        "  w0j_ci_min = wj0_est[:, ind_min_wj0]\n",
        "\n",
        "  x = [(i, i) for i in range(m)]\n",
        "  y = [(w0i_ci_min[0, i], w0i_ci_min[1, i]) for i in range(m)]\n",
        "  y_prime = [(w0j_ci_min[0, i], w0j_ci_min[1, i]) for i in range(m)]\n",
        "\n",
        "  #Plots\n",
        "  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\n",
        "  # Upper Nodes\n",
        "  for i, j in zip(x, y):\n",
        "      ax1.plot((i[0], i[1]), (np.log(j[0]), np.log(j[1])), color='blue')\n",
        "      ax1.scatter(i[0], np.log(Wi0[tgraph_index, 0, ind_min_wi0[i[0]]]), color='red')\n",
        "  ax1.plot((i[0], i[1]), (j[0], j[1]), color='blue', label='95% CI')\n",
        "  ax1.scatter(i[0], np.log(Wi0[tgraph_index, 0, ind_min_wi0[i[0]]]), color='red', label='true')\n",
        "  ax1.title.set_text('Upper Nodes')\n",
        "  ax1.set_xlabel('index of nodes sorted by decreasing wi0')\n",
        "  ax1.set_ylabel('Log_wi0')\n",
        "\n",
        "  # Lower Nodes\n",
        "  for i, j in zip(x, y_prime):\n",
        "      ax2.plot((i[0], i[1]), (np.log(j[0]), np.log(j[1])), color='blue')\n",
        "      ax2.scatter(i[0], np.log(Wj0[tgraph_index, 0, ind_min_wj0[i[0]]]), color='red')\n",
        "  ax2.plot((i[0], i[1]), (j[0], j[1]), color='blue', label='95% CI')\n",
        "  ax2.scatter(i[0], np.log(Wj0[tgraph_index, 0, ind_min_wj0[i[0]]]), color='red', label='true')\n",
        "  ax2.title.set_text('Lower Nodes')\n",
        "  ax2.set_xlabel('index of nodes sorted by decreasing wj0')\n",
        "  ax2.set_ylabel('Log_wj0')\n",
        "\n",
        "  fig.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "b7usjRepmUeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PREDICTED POST DEGREE DISTRIBUTION**"
      ],
      "metadata": {
        "id": "b8D9kBo0m4pT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def network_weights_w_wprime(samples_dict, iters, iters_prime, p):\n",
        "  # Calculate the estimated network weights for upper and lower nodes.\n",
        "\n",
        "  # Upper Nodes\n",
        "  W_upper_est = np.zeros((iters, L_mcmc, p))\n",
        "  for i in range(iters):\n",
        "    #Multiply each score in `scores` by the corresponding wi0 weight\n",
        "      W_upper_est[i, :, :] = np.transpose(np.multiply(np.transpose(samples_dict['scores'][i, :, :]), samples_dict['wi0'][i, :]))\n",
        "\n",
        "  # Lower Nodes\n",
        "  W_lower_est = np.zeros((iters_prime, L_prime_mcmc, p))\n",
        "  for j in range(iters_prime):\n",
        "    #Multiply each score in `scores` by the corresponding wj0 weight\n",
        "      W_lower_est[j, :, :] = np.transpose(np.multiply(np.transpose(samples_dict['scores_prime'][j, :, :]), samples_dict['wj0'][j, :]))\n",
        "\n",
        "  return W_upper_est, W_lower_est"
      ],
      "metadata": {
        "id": "sOXnqzxsgu4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def exptiltBFRY_jax(rng_key, alpha, sigma, tau, L):\n",
        "    #Generate samples from an exponentially tilted-(BFRY) distribution\n",
        "    t = ((L * sigma / alpha) ** (1.0/ sigma)).astype(float)\n",
        "    g=jax.random.gamma(rng_key, 1-sigma, shape=(L,1))\n",
        "    unif = jax.random.uniform(rng_key,shape=(L,1))\n",
        "    s = jnp.multiply(g, jnp.power(((t + tau) ** sigma) * (1 - unif) + (tau ** sigma) * unif, -1 / sigma))\n",
        "    return s"
      ],
      "metadata": {
        "id": "oR13zSjAn55c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_etbfry_todeschini_jax(rng_key, args_dict,  alpha, alpha_prime,\n",
        "                                sigma, sigma_prime, tau, tau_prime,\n",
        "                                a, a_prime, b, b_prime,\n",
        "                                scores=None, scores_prime=None,\n",
        "                                wi0=None, wj0=None, weights=None, weights_prime=None):\n",
        "\n",
        "    ##### A JAX-based implementation of the Exponentially Tilted Beta-Fractional Random Graph Model by Todeschini\n",
        "    ## Extract node counts and feature dimension from args_dict\n",
        "    L, L_prime = args_dict['L'], args_dict['L_prime']\n",
        "    p = args_dict['p']\n",
        "\n",
        "    # Initialize weights wi0 and wj0 for upper nodes if not provided\n",
        "    if wi0 is None:\n",
        "        wi0 = jnp.zeros((L, 1), dtype='float')\n",
        "        wi0 = exptiltBFRY_jax(rng_key, alpha, sigma, tau, L).reshape(L)\n",
        "        print(wi0.shape)\n",
        "    if wj0 is None:\n",
        "        wj0 = jnp.zeros((L_prime, 1), dtype='float32')\n",
        "        wj0 = exptiltBFRY_jax(rng_key, alpha_prime, sigma_prime, tau_prime, L_prime).reshape(L_prime)\n",
        "        print(wj0.shape)\n",
        "\n",
        "    # Generate latent scores for upper and lower nodes if not provided\n",
        "    if scores is None:\n",
        "        scores = jnp.zeros((L, p), dtype='float32')\n",
        "        scores = jax.random.gamma(rng_key, a, shape=(L, p))/b\n",
        "        print(scores.shape)\n",
        "    if scores_prime is None:\n",
        "        scores_prime = jnp.zeros((L_prime, p), dtype='float32')\n",
        "        scores_prime = jax.random.gamma(rng_key, a_prime, shape=(L_prime, p))/b_prime\n",
        "        print(scores_prime.shape)\n",
        "\n",
        "    # Compute weights for lower and upper nodes by scaling scores_prime with wi0 and wj0\n",
        "    if weights is None:\n",
        "        weights = jnp.zeros((L, p), dtype='float32')\n",
        "        weights=jnp.transpose(jnp.multiply(jnp.transpose(scores), wi0))\n",
        "        print(weights.shape)\n",
        "\n",
        "    if weights_prime is None:\n",
        "        weights_prime = jnp.zeros((L_prime, p), dtype='float32')\n",
        "        weights_prime = jnp.transpose(jnp.multiply(jnp.transpose(scores_prime), wj0))\n",
        "        print(weights_prime.shape)\n",
        "\n",
        "\n",
        "    print(f'Wi0:{wi0.shape}, Wj0:{wj0.shape}, scores:{scores.shape}, scores_prime:{scores_prime.shape}, weights_prime:{weights_prime.shape}, weights:{weights.shape}')\n",
        "\n",
        "    # # Calculate the weight matrix 'ww' for the network connections\n",
        "    ww = jnp.matmul( weights, jnp.transpose(weights_prime))## (L,T,p) x (L_prime,T,p)= (L, L_prime)\n",
        "    # Generate observed connections (Poisson-distributed) based on 'ww'\n",
        "    N_obs=jax.random.poisson(rng_key, ww, shape=((L, L_prime)))\n",
        "    print(f'N_obs:{N_obs.shape}')\n",
        "    ## Generate binary connection indicator matrix\n",
        "    Z_obs = (N_obs) >0\n",
        "    print(f'Z_obs:{Z_obs.shape}')\n",
        "\n",
        "    return N_obs, Z_obs, weights, weights_prime, scores, scores_prime, wi0, wj0"
      ],
      "metadata": {
        "id": "bXjVXaOAn5y1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def plot_degree_sparse(G, step=1, color='b'):\n",
        "  ### Plots the degree distribution of a sparse graph represented by an adjacency matrix `G` using logarithmic binning.\n",
        "\n",
        "    pd.plotting.register_matplotlib_converters()\n",
        "    # Calculate Degrees\n",
        "    deg_upper = np.squeeze(np.sum(G,0))\n",
        "    deg_lower = np.squeeze(np.sum(G,1))\n",
        "    # Remove nodes with no connections\n",
        "    any_upper = np.squeeze(np.asarray(deg_upper > 0))\n",
        "    any_lower = np.squeeze(np.asarray(deg_lower > 0))\n",
        "    ##  Filter rows and columns\n",
        "    G = G[any_lower, :]\n",
        "    G = G[:, any_upper]\n",
        "    ## Recalculate degrees after filtering\n",
        "    deg_upper = np.squeeze(np.sum(G,0))\n",
        "    deg_lower = np.squeeze(np.sum(G,1))\n",
        "\n",
        "    # Uses logarithmic binning to get a less noisy estimate of the pdf of the degree distribution\n",
        "    edgebins = 2**np.arange(0, 17, step)\n",
        "    sizebins = edgebins[1:] - edgebins[:-1]\n",
        "    sizebins = np.append(sizebins, 1)\n",
        "    centerbins = edgebins\n",
        "\n",
        "    ## Calculate and plot the degree distribution for upper nodes\n",
        "    counts_upper = np.histogram(deg_upper, np.append(edgebins, np.inf))\n",
        "    freq_upper = np.divide(counts_upper[0],sizebins)/G.shape[0]\n",
        "    h2_upper = plt.loglog(centerbins, freq_upper,'o',color=color)\n",
        "\n",
        "    ## Calculate and plot the degree distribution for lower nodes\n",
        "    counts_lower = np.histogram(deg_lower, np.append(edgebins, np.inf))\n",
        "    freq_lower = np.divide(counts_lower[0], sizebins)/G.shape[1]\n",
        "    h2_lower = plt.loglog(centerbins, freq_lower,'o',color=color)\n",
        "\n",
        "    plt.xlabel('Degree', fontsize=16)\n",
        "    plt.ylabel('Distribution', fontsize=16)\n",
        "    plt.gca().set_xlim(left=1)\n",
        "    # h2, centerbins, freq\n",
        "    return [h2_upper, h2_lower, centerbins, freq_upper, freq_lower ]"
      ],
      "metadata": {
        "id": "KihEkMJDn5sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def plot_figure(freq, centerbins, freq_true):\n",
        "    ##### Plots a degree distribution with posterior predictive intervals along with the true frequency distribution.\n",
        "\n",
        "    #Calculate the 2.5% and 97.5% quantiles of the posterior predictive distribution\n",
        "    quantile_freq = np.quantile(freq, [.025, .975],0)\n",
        "    ## Avoid division by zero for nodes with zero degree\n",
        "    ind1 = quantile_freq[0,:]==0\n",
        "    quantile_freq[0, ind1] = quantile_freq[1, ind1] / 100000\n",
        "\n",
        "    plt.figure()\n",
        "    ## Plot the quantile range for the posterior predictive distribution\n",
        "    plt.plot(centerbins, np.transpose(quantile_freq), color='b', alpha=0.2, label='_nolegend_')\n",
        "    ind = quantile_freq[0,:]>0\n",
        "\n",
        "    ## Fill the area between the quantile ranges with transparency\n",
        "    plt.fill_between(centerbins[ind], np.transpose(quantile_freq[0,ind]), np.transpose(quantile_freq[1,ind]), alpha=0.2)\n",
        "    plt.xscale('log')\n",
        "    plt.yscale('log')\n",
        "    # Plot the true frequency data\n",
        "    ind = freq_true>0\n",
        "    plt.loglog(centerbins[ind], freq_true[ind], 'o', color='b')\n",
        "\n",
        "    plt.xlabel('Degree')\n",
        "    plt.ylabel('Distribution')\n",
        "    plt.legend(labels=('Data', '95% posterior predictive'),frameon=False, loc='lower left')\n",
        "    plt.xlim([.8, 1e4])\n",
        "    plt.tight_layout()\n",
        "\n",
        "    return quantile_freq"
      ],
      "metadata": {
        "id": "85rKZi86oe37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_post_degree_distribution(num_draws, samples, original_adj_matrix, args_dict, use_weights=1):\n",
        "    ### Main function to plot post degree distributions\n",
        "\n",
        "    rng_key, rng_key_predict = random.split(random.PRNGKey(0)) # random key\n",
        "    # Inters- Calculate the number of iterations (draws) for each chain\n",
        "    iters = int(samples['wi0'].shape[0] / num_chains)\n",
        "    iters_prime = int(samples['wj0'].shape[0] / num_chains)\n",
        "    ## samples used to parallelize the sampling process\n",
        "    vmap_args = (\n",
        "            random.split(rng_key_predict,  num_draws),\n",
        "            samples[\"alpha\"][iters-num_draws:iters],\n",
        "            samples[\"sigma\"][iters-num_draws:iters],\n",
        "            samples[\"tau\"][iters-num_draws:iters],\n",
        "            samples[\"a\"][iters-num_draws:iters],\n",
        "            samples[\"b\"][iters-num_draws:iters],\n",
        "\n",
        "            samples[\"alpha_prime\"][iters_prime-num_draws:iters_prime],\n",
        "            samples[\"sigma_prime\"][iters_prime-num_draws:iters_prime],\n",
        "            samples[\"tau_prime\"][iters_prime-num_draws:iters_prime],\n",
        "            samples[\"a_prime\"][iters_prime-num_draws:iters_prime],\n",
        "            samples[\"b_prime\"][iters_prime-num_draws:iters_prime] )\n",
        "\n",
        "    if use_weights:\n",
        "      # If weights are being used, calculate them and add to the samples\n",
        "        W_upper_est, W_lower_est = network_weights_w_wprime(samples, iters, iters_prime, p =3) # helper function\n",
        "        samples['weights'] = W_upper_est\n",
        "        samples['weights_prime'] = W_lower_est\n",
        "        print(f'Weights Type A:{W_upper_est.shape}, Weights Type B:{W_lower_est.shape}')\n",
        "\n",
        "        vmap_args=vmap_args+ (\n",
        "        samples[\"wi0\"][iters-num_draws:iters],\n",
        "        samples[\"scores\"][iters-num_draws:iters],\n",
        "        samples[\"weights\"][iters-num_draws:iters],\n",
        "        samples[\"wj0\"][iters-num_draws:iters],\n",
        "        samples[\"scores_prime\"][iters-num_draws:iters],\n",
        "        samples[\"weights_prime\"][iters-num_draws:iters])\n",
        "        print('Use Weights')\n",
        "        ## # Use vmap to sample from the model with the weights\n",
        "        N_obs_pred, Z_obs_pred, weights_pred, weights_prime_pred, scores_pred, scores_prime_pred, wi0_pred, wj0_pred = vmap(\n",
        "            lambda rng_key, alpha, sigma, tau, a, b, alpha_prime, sigma_prime, tau_prime, a_prime, b_prime, wi0, scores, weights, wj0, scores_prime, weights_prime : model_etbfry_todeschini_jax(\n",
        "                rng_key=rng_key, args_dict=args_dict, alpha=alpha, alpha_prime=alpha_prime,\n",
        "                sigma=sigma, sigma_prime=sigma_prime, tau=tau, tau_prime=tau_prime,\n",
        "                 a=a, a_prime=a_prime, b=b, b_prime=b_prime, wi0=wi0, wj0=wj0,\n",
        "                scores=scores, scores_prime=scores_prime, weights=weights, weights_prime=weights_prime))(*vmap_args)\n",
        "    else:\n",
        "         ## # Use vmap to sample from the model with no weights\n",
        "        print('No Weights ')\n",
        "        N_obs_pred, Z_obs_pred, weights_pred, weights_prime_pred, scores_pred, scores_prime_pred, wi0_pred, wj0_pred  = vmap(\n",
        "                lambda rng_key, alpha, sigma, tau, a, b, alpha_prime, sigma_prime, tau_prime, a_prime, b_prime : model_etbfry_todeschini_jax(\n",
        "                    rng_key=rng_key, args_dict=args_dict, alpha=alpha, alpha_prime=alpha_prime,\n",
        "                sigma=sigma, sigma_prime=sigma_prime, tau=tau, tau_prime=tau_prime,\n",
        "                 a=a, a_prime=a_prime, b=b, b_prime=b_prime))(*vmap_args)\n",
        "\n",
        "    #  Initialize arrays to store frequency distributions for degree distribution plotting\n",
        "    freq_samp, freq_samp_prime  = np.zeros((num_draws, 17)),  np.zeros((num_draws, 17))\n",
        "    centerbins1 = np.zeros((17))\n",
        "    freq_true, freq_true_prime = np.zeros((17)), np.zeros((17))\n",
        "    ## Arrays to store quantile frequency distributions\n",
        "    quantile_freq_upper, quantile_freq_lower = np.zeros((2,17)),  np.zeros((2,17))\n",
        "\n",
        "    ##each draw to compute the degree distribution for each sample\n",
        "    for i in range(num_draws):\n",
        "        Gsamp = Z_obs_pred[i]\n",
        "        [_, _, _, freq_samp[i, :], freq_samp_prime[i, :] ] = plot_degree_sparse(Gsamp) ## Calculate the degree distribution for the current sample\n",
        "        [_, _, centerbins1[:], freq_true[:], freq_true_prime[:] ] = plot_degree_sparse(jnp.array(original_adj_matrix)) ## Calculate the degree distribution for the true sample\n",
        "        plt.close()\n",
        "\n",
        "    ## Plot the upper  and lower part of the degree distribution\n",
        "    print('plot_figure Upper')\n",
        "    quantile_freq_upper[:,:] = plot_figure(freq_samp[:,:], centerbins1[:], freq_true[:])\n",
        "    print('plot_figure Lower')\n",
        "    quantile_freq_lower[:,:] = plot_figure(freq_samp_prime[:,:], centerbins1[:], freq_true_prime[:])\n",
        "\n",
        "    return quantile_freq_upper, quantile_freq_lower, N_obs_pred, Z_obs_pred, weights_pred, weights_prime_pred, scores_pred, scores_prime_pred, wi0_pred, wj0_pred"
      ],
      "metadata": {
        "id": "jfX5BeWYoic7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Simulations"
      ],
      "metadata": {
        "id": "l9otzcpn0rh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class import -> dicotnary with the nessecary information\n",
        "# parameters, (alpha, sigma, alpha ptime, sigma_prime, L, L_prime, p , a, b, a_prime, b_prime)"
      ],
      "metadata": {
        "id": "i907btrI0tJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p = 3 # number of comunities (not inferred)\n",
        "L, L_prime = 180, 120 # number of upper & bottom nodes (not inferred)\n",
        "\n",
        "# BFRY parameters\n",
        "talpha, talpha_prime = 15, 10\n",
        "tsigma, tsigma_prime = 0.2, 0.2\n",
        "ttau, ttau_prime = 1.0, 1.0\n",
        "\n",
        "# Gamma parameters (affiliation)\n",
        "ta, ta_prime = jnp.array([1.5, 1.2, 1.0]), jnp.array( [1.1, 1.0, 1.0]) # vector len == community len\n",
        "tb, tb_prime = jnp.array([2.0, 1.9, 2.0]), jnp.array([1.5, 1.9, 2.0])\n",
        "args={}\n",
        "args['p'] = p\n",
        "\n",
        "# Upper Nodes\n",
        "args['L'] =  L\n",
        "args['alpha'] = talpha\n",
        "args['sigma'] = tsigma\n",
        "args['tau'] = ttau\n",
        "args['a'] = ta #*np.ones(args['p']) ## 1xp;\n",
        "print('True a for all the upper communities is', args['a'])\n",
        "args['b'] = tb #*np.ones(args['p']) ## 1xp\n",
        "args['scores']=None\n",
        "args['wi0']=None\n",
        "\n",
        "# Bottom Nodes\n",
        "args['L_prime'] = L_prime\n",
        "args['alpha_prime'] = talpha_prime\n",
        "args['sigma_prime'] = tsigma_prime # None\n",
        "args['tau_prime'] = ttau_prime\n",
        "args['a_prime'] = ta_prime #*np.ones(args['p']) ## 1xp;\n",
        "print('True a_prime for all the bottom communities is', args['a_prime'])\n",
        "args['b_prime'] = tb_prime #*np.ones(args['p']) ## 1xp\n",
        "args['scores_prime']=None\n",
        "args['wj0']=None\n",
        "\n",
        "args['batch_size'] = 1 # number of graphs to simulate\n",
        "args['Z_obs']=None # given"
      ],
      "metadata": {
        "id": "sDdWgv1i0tGK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83533ce0-d48c-42ed-f38d-ebe7b216ed3b",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True a for all the upper communities is [1.5 1.2 1. ]\n",
            "True a_prime for all the bottom communities is [1.1 1.  1. ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bp_network = Bipartite_Network(args=args)\n",
        "bp_network.bipartite_network_parameters()\n",
        "bp_network.simulate_bipartite_network()\n",
        "bp_network.update_args_dict()\n",
        "new_args = bp_network.get_update_args()"
      ],
      "metadata": {
        "id": "oTZrtfkd0s2M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f96b8d4-16c2-4225-dde8-d52f4f273e7b",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arguments Parameters:\n",
            "dict_items([('p', 3), ('L', 180), ('alpha', 15), ('sigma', 0.2), ('tau', 1.0), ('a', Array([1.5, 1.2, 1. ], dtype=float32)), ('b', Array([2. , 1.9, 2. ], dtype=float32)), ('scores', None), ('wi0', None), ('L_prime', 120), ('alpha_prime', 10), ('sigma_prime', 0.2), ('tau_prime', 1.0), ('a_prime', Array([1.1, 1. , 1. ], dtype=float32)), ('b_prime', Array([1.5, 1.9, 2. ], dtype=float32)), ('scores_prime', None), ('wj0', None), ('batch_size', 1), ('Z_obs', None)])\n",
            "Upper hyperparameters:\n",
            " tau:1.0, sigma:0.2, alpha:15, a:[1.5 1.2 1. ], b:[2.  1.9 2. ]\n",
            "Bottom hyperparameters:\n",
            " tau:1.0, sigma:0.2, alpha:10, a:[1.1 1.  1. ], b:[1.5 1.9 2. ]\n",
            "Scores: (180, 3) Wi0:(180,)\n",
            "Scores_prieme:(120, 3) Wj0:(120,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adj_matrix = new_args['adj_matrix']\n",
        "deg_typeA = np.sum(adj_matrix, axis=0)\n",
        "deg_typeB = np.sum(adj_matrix, axis=1)"
      ],
      "metadata": {
        "id": "qKBwoDbmbO7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adj_matrix_zeros, L_new, L_prime_new = Network_details.add_zeros(adj_matrix=adj_matrix, ratio=1.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rivv_Oega6v-",
        "outputId": "950f27d6-700a-418a-9c2d-2bfe089dbbdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adj matrix: (180, 120) \n",
            "New Adj matrix: (270, 180) \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Network_details.network_statistics(typeA_nodes=L,\n",
        "                                   typeB_nodes=L_prime,\n",
        "                                   adj_matrix=adj_matrix)"
      ],
      "metadata": {
        "id": "BdQ7Z-8jcWYO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8563f3d3-d825-4f13-c38e-5ff602e98339",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------+----------+\n",
            "| Metric                |    Value |\n",
            "+=======================+==========+\n",
            "| # Top Nodes           | 120      |\n",
            "+-----------------------+----------+\n",
            "| # Bottom Nodes        | 180      |\n",
            "+-----------------------+----------+\n",
            "| # Edges               | 569      |\n",
            "+-----------------------+----------+\n",
            "| Top average degree    |   3.1611 |\n",
            "+-----------------------+----------+\n",
            "| Bottom Average degree |   4.7417 |\n",
            "+-----------------------+----------+\n",
            "| Graph Average degree  | 144      |\n",
            "+-----------------------+----------+\n",
            "| Density               |   0.0263 |\n",
            "+-----------------------+----------+\n",
            "| Sparsity              |   0.0063 |\n",
            "+-----------------------+----------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Network_details.plt_deg_distr(deg_typeA, title='Type A', binned=False)\n",
        "Network_details.plt_deg_distr(deg_typeB, title='Type B', binned=False)"
      ],
      "metadata": {
        "id": "xB-Gxc_Fcuqv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        },
        "outputId": "37d3aef5-de3a-4a78-bd23-d26c209fc43e",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NU3zeHLmvQEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_chains = 1 # 1 or 2 chain\n",
        "num_samples = 1000 # 7000\n",
        "num_warmup = 200\n",
        "thinning = 2 # stable at 2\n",
        "\n",
        "# set the arguments\n",
        "args_mcmc={}\n",
        "\n",
        "args_mcmc['p'] = p\n",
        "args_mcmc['Z_obs'] = adj_matrix_zeros\n",
        "\n",
        "# Upper Nodes\n",
        "args_mcmc['L'] = L_new\n",
        "args_mcmc['alpha'] = None\n",
        "args_mcmc['sigma'] = None\n",
        "args_mcmc['tau'] = ttau\n",
        "args_mcmc['scores'] = None\n",
        "args_mcmc['a'] = None\n",
        "args_mcmc['b'] = None\n",
        "args_mcmc['wi0'] = None\n",
        "\n",
        "# Bottom Nodes\n",
        "args_mcmc['L_prime'] = L_prime_new\n",
        "args_mcmc['alpha_prime'] = None\n",
        "args_mcmc['sigma_prime'] = None\n",
        "args_mcmc['tau_prime'] = ttau_prime\n",
        "args_mcmc['scores_prime'] = None\n",
        "args_mcmc['a_prime'] = None\n",
        "args_mcmc['b_prime'] = None\n",
        "args_mcmc['wj0'] = None\n",
        "\n",
        "args_mcmc['samples']={}\n",
        "\n",
        "args_mcmc['num_chains'] = num_chains\n",
        "args_mcmc['num_samples'] = num_samples\n",
        "args_mcmc['thinning'] = thinning\n"
      ],
      "metadata": {
        "id": "3ErJ2NLkvk78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "infer = Infer_Bipartite_Network(infer_args=args_mcmc)\n",
        "infer.infer_network()"
      ],
      "metadata": {
        "id": "e7tiun0Bvk5H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b6e1802-3c5d-41f1-9de4-35e7bea4b208",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('p', 3), ('Z_obs', array([[0., 0., 0., ..., 0., 0., 0.],\n",
            "       [0., 0., 0., ..., 0., 0., 0.],\n",
            "       [0., 0., 0., ..., 0., 0., 0.],\n",
            "       ...,\n",
            "       [0., 0., 0., ..., 0., 0., 0.],\n",
            "       [0., 0., 0., ..., 0., 0., 0.],\n",
            "       [0., 0., 0., ..., 0., 0., 0.]])), ('L', 270), ('alpha', None), ('sigma', None), ('tau', 1.0), ('scores', None), ('a', None), ('b', None), ('wi0', None), ('L_prime', 180), ('alpha_prime', None), ('sigma_prime', None), ('tau_prime', 1.0), ('scores_prime', None), ('a_prime', None), ('b_prime', None), ('wj0', None), ('samples', {}), ('num_chains', 1), ('num_samples', 1000), ('thinning', 2)]\n",
            "Upper hyperparameters:\n",
            " tau:1.0, sigma:0.4340122640132904, alpha:0.7668227553367615, a:[0.31073564 0.6023828  1.7937478 ], b:[0.31073564 0.6023828  1.7937478 ]\n",
            "Bottom hyperparameters:\n",
            " tau:1.0, sigma:0.4340122640132904, alpha:0.7668227553367615, a:[0.31073564 0.6023828  1.7937478 ], b:[0.31073564 0.6023828  1.7937478 ]\n",
            "Scores: (270, 3) Wi0:(270,)\n",
            "Scores_prieme:(180, 3) Wj0:(180,)\n",
            "Upper hyperparameters:\n",
            " tau:1.0, sigma:Traced<ConcreteArray(0.6149214506149292, dtype=float32)>with<JVPTrace(level=2/0)> with\n",
            "  primal = Array(0.61492145, dtype=float32)\n",
            "  tangent = Traced<ShapedArray(float32[])>with<JaxprTrace(level=1/0)> with\n",
            "    pval = (ShapedArray(float32[]), None)\n",
            "    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7bbe45624a60>, in_tracers=(Traced<ShapedArray(float32[]):JaxprTrace(level=1/0)>, Traced<ConcreteArray(1.0, dtype=float32):JaxprTrace(level=1/0)>, Traced<ConcreteArray(1.0, dtype=float32):JaxprTrace(level=1/0)>), out_tracer_refs=[<weakref at 0x7bbe42fc94e0; to 'JaxprTracer' at 0x7bbe42fc9490>], out_avals=[ShapedArray(float32[])], primitive=pjit, params={'jaxpr': { lambda ; a:f32[] b:f32[] c:f32[]. let\n",
            "    d:f32[] = mul a b\n",
            "    e:f32[] = mul d c\n",
            "  in (e,) }, 'in_shardings': (UnspecifiedValue, UnspecifiedValue, UnspecifiedValue), 'out_shardings': (UnspecifiedValue,), 'in_layouts': (None, None, None), 'out_layouts': (None,), 'resource_env': None, 'donated_invars': (False, False, False), 'name': 'clip', 'keep_unused': False, 'inline': False}, effects=set(), source_info=<jax._src.source_info_util.SourceInfo object at 0x7bbe42e083d0>, ctx=JaxprEqnContext(compute_type=None,threefry_partitionable=False),xla_metadata={}), alpha:Traced<ConcreteArray(0.7961199283599854, dtype=float32)>with<JVPTrace(level=2/0)> with\n",
            "  primal = Array(0.7961199, dtype=float32)\n",
            "  tangent = Traced<ShapedArray(float32[])>with<JaxprTrace(level=1/0)> with\n",
            "    pval = (ShapedArray(float32[]), None)\n",
            "    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7bbe45624040>, in_tracers=(Traced<ShapedArray(float32[]):JaxprTrace(level=1/0)>, Traced<ConcreteArray(0.7961199283599854, dtype=float32):JaxprTrace(level=1/0)>), out_tracer_refs=[<weakref at 0x7bbe42fcb600; to 'JaxprTracer' at 0x7bbe42fcb5b0>], out_avals=[ShapedArray(float32[])], primitive=pjit, params={'jaxpr': { lambda ; a:f32[] b:f32[]. let c:f32[] = mul a b in (c,) }, 'in_shardings': (UnspecifiedValue, UnspecifiedValue), 'out_shardings': (UnspecifiedValue,), 'in_layouts': (None, None), 'out_layouts': (None,), 'resource_env': None, 'donated_invars': (False, False), 'name': 'exp', 'keep_unused': False, 'inline': True}, effects=set(), source_info=<jax._src.source_info_util.SourceInfo object at 0x7bbe42e183d0>, ctx=JaxprEqnContext(compute_type=None,threefry_partitionable=False),xla_metadata={}), a:Traced<ConcreteArray([3.234739  5.3660693 1.3808708], dtype=float32)>with<JVPTrace(level=2/0)> with\n",
            "  primal = Array([3.234739 , 5.3660693, 1.3808708], dtype=float32)\n",
            "  tangent = Traced<ShapedArray(float32[3])>with<JaxprTrace(level=1/0)> with\n",
            "    pval = (ShapedArray(float32[3]), None)\n",
            "    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7bbe45624dd0>, in_tracers=(Traced<ShapedArray(float32[3]):JaxprTrace(level=1/0)>, Traced<ConcreteArray([3.234739  5.3660693 1.3808708], dtype=float32):JaxprTrace(level=1/0)>), out_tracer_refs=[<weakref at 0x7bbe42e101d0; to 'JaxprTracer' at 0x7bbe42e10180>], out_avals=[ShapedArray(float32[3])], primitive=pjit, params={'jaxpr': { lambda ; a:f32[3] b:f32[3]. let c:f32[3] = mul a b in (c,) }, 'in_shardings': (UnspecifiedValue, UnspecifiedValue), 'out_shardings': (UnspecifiedValue,), 'in_layouts': (None, None), 'out_layouts': (None,), 'resource_env': None, 'donated_invars': (False, False), 'name': 'exp', 'keep_unused': False, 'inline': True}, effects=set(), source_info=<jax._src.source_info_util.SourceInfo object at 0x7bbe42e19e70>, ctx=JaxprEqnContext(compute_type=None,threefry_partitionable=False),xla_metadata={}), b:Traced<ConcreteArray([0.2929836  2.9180434  0.69654936], dtype=float32)>with<JVPTrace(level=2/0)> with\n",
            "  primal = Array([0.2929836 , 2.9180434 , 0.69654936], dtype=float32)\n",
            "  tangent = Traced<ShapedArray(float32[3])>with<JaxprTrace(level=1/0)> with\n",
            "    pval = (ShapedArray(float32[3]), None)\n",
            "    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7bbe45624ed0>, in_tracers=(Traced<ShapedArray(float32[3]):JaxprTrace(level=1/0)>, Traced<ConcreteArray([0.2929836  2.9180434  0.69654936], dtype=float32):JaxprTrace(level=1/0)>), out_tracer_refs=[<weakref at 0x7bbe42e10db0; to 'JaxprTracer' at 0x7bbe42e10d60>], out_avals=[ShapedArray(float32[3])], primitive=pjit, params={'jaxpr': { lambda ; a:f32[3] b:f32[3]. let c:f32[3] = mul a b in (c,) }, 'in_shardings': (UnspecifiedValue, UnspecifiedValue), 'out_shardings': (UnspecifiedValue,), 'in_layouts': (None, None), 'out_layouts': (None,), 'resource_env': None, 'donated_invars': (False, False), 'name': 'exp', 'keep_unused': False, 'inline': True}, effects=set(), source_info=<jax._src.source_info_util.SourceInfo object at 0x7bbe42e1b670>, ctx=JaxprEqnContext(compute_type=None,threefry_partitionable=False),xla_metadata={})\n",
            "Bottom hyperparameters:\n",
            " tau:1.0, sigma:Traced<ConcreteArray(0.1865561455488205, dtype=float32)>with<JVPTrace(level=2/0)> with\n",
            "  primal = Array(0.18655615, dtype=float32)\n",
            "  tangent = Traced<ShapedArray(float32[])>with<JaxprTrace(level=1/0)> with\n",
            "    pval = (ShapedArray(float32[]), None)\n",
            "    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7bbe45624c70>, in_tracers=(Traced<ShapedArray(float32[]):JaxprTrace(level=1/0)>, Traced<ConcreteArray(1.0, dtype=float32):JaxprTrace(level=1/0)>, Traced<ConcreteArray(1.0, dtype=float32):JaxprTrace(level=1/0)>), out_tracer_refs=[<weakref at 0x7bbe42fca7f0; to 'JaxprTracer' at 0x7bbe42fca7a0>], out_avals=[ShapedArray(float32[])], primitive=pjit, params={'jaxpr': { lambda ; a:f32[] b:f32[] c:f32[]. let\n",
            "    d:f32[] = mul a b\n",
            "    e:f32[] = mul d c\n",
            "  in (e,) }, 'in_shardings': (UnspecifiedValue, UnspecifiedValue, UnspecifiedValue), 'out_shardings': (UnspecifiedValue,), 'in_layouts': (None, None, None), 'out_layouts': (None,), 'resource_env': None, 'donated_invars': (False, False, False), 'name': 'clip', 'keep_unused': False, 'inline': False}, effects=set(), source_info=<jax._src.source_info_util.SourceInfo object at 0x7bbe42e0a500>, ctx=JaxprEqnContext(compute_type=None,threefry_partitionable=False),xla_metadata={}), alpha:Traced<ConcreteArray(0.3394417464733124, dtype=float32)>with<JVPTrace(level=2/0)> with\n",
            "  primal = Array(0.33944175, dtype=float32)\n",
            "  tangent = Traced<ShapedArray(float32[])>with<JaxprTrace(level=1/0)> with\n",
            "    pval = (ShapedArray(float32[]), None)\n",
            "    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7bbe45624dc0>, in_tracers=(Traced<ShapedArray(float32[]):JaxprTrace(level=1/0)>, Traced<ConcreteArray(0.3394417464733124, dtype=float32):JaxprTrace(level=1/0)>), out_tracer_refs=[<weakref at 0x7bbe42fcbba0; to 'JaxprTracer' at 0x7bbe42fcbb50>], out_avals=[ShapedArray(float32[])], primitive=pjit, params={'jaxpr': { lambda ; a:f32[] b:f32[]. let c:f32[] = mul a b in (c,) }, 'in_shardings': (UnspecifiedValue, UnspecifiedValue), 'out_shardings': (UnspecifiedValue,), 'in_layouts': (None, None), 'out_layouts': (None,), 'resource_env': None, 'donated_invars': (False, False), 'name': 'exp', 'keep_unused': False, 'inline': True}, effects=set(), source_info=<jax._src.source_info_util.SourceInfo object at 0x7bbe42e190f0>, ctx=JaxprEqnContext(compute_type=None,threefry_partitionable=False),xla_metadata={}), a:Traced<ConcreteArray([1.9157474 1.7675457 1.4203868], dtype=float32)>with<JVPTrace(level=2/0)> with\n",
            "  primal = Array([1.9157474, 1.7675457, 1.4203868], dtype=float32)\n",
            "  tangent = Traced<ShapedArray(float32[3])>with<JaxprTrace(level=1/0)> with\n",
            "    pval = (ShapedArray(float32[3]), None)\n",
            "    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7bbe45624e90>, in_tracers=(Traced<ShapedArray(float32[3]):JaxprTrace(level=1/0)>, Traced<ConcreteArray([1.9157474 1.7675457 1.4203868], dtype=float32):JaxprTrace(level=1/0)>), out_tracer_refs=[<weakref at 0x7bbe42e107c0; to 'JaxprTracer' at 0x7bbe42e10770>], out_avals=[ShapedArray(float32[3])], primitive=pjit, params={'jaxpr': { lambda ; a:f32[3] b:f32[3]. let c:f32[3] = mul a b in (c,) }, 'in_shardings': (UnspecifiedValue, UnspecifiedValue), 'out_shardings': (UnspecifiedValue,), 'in_layouts': (None, None), 'out_layouts': (None,), 'resource_env': None, 'donated_invars': (False, False), 'name': 'exp', 'keep_unused': False, 'inline': True}, effects=set(), source_info=<jax._src.source_info_util.SourceInfo object at 0x7bbe42e1aa70>, ctx=JaxprEqnContext(compute_type=None,threefry_partitionable=False),xla_metadata={}), b:Traced<ConcreteArray([0.23784973 3.5325556  3.748091  ], dtype=float32)>with<JVPTrace(level=2/0)> with\n",
            "  primal = Array([0.23784973, 3.5325556 , 3.748091  ], dtype=float32)\n",
            "  tangent = Traced<ShapedArray(float32[3])>with<JaxprTrace(level=1/0)> with\n",
            "    pval = (ShapedArray(float32[3]), None)\n",
            "    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7bbe45624f20>, in_tracers=(Traced<ShapedArray(float32[3]):JaxprTrace(level=1/0)>, Traced<ConcreteArray([0.23784973 3.5325556  3.748091  ], dtype=float32):JaxprTrace(level=1/0)>), out_tracer_refs=[<weakref at 0x7bbe42e113a0; to 'JaxprTracer' at 0x7bbe42e11350>], out_avals=[ShapedArray(float32[3])], primitive=pjit, params={'jaxpr': { lambda ; a:f32[3] b:f32[3]. let c:f32[3] = mul a b in (c,) }, 'in_shardings': (UnspecifiedValue, UnspecifiedValue), 'out_shardings': (UnspecifiedValue,), 'in_layouts': (None, None), 'out_layouts': (None,), 'resource_env': None, 'donated_invars': (False, False), 'name': 'exp', 'keep_unused': False, 'inline': True}, effects=set(), source_info=<jax._src.source_info_util.SourceInfo object at 0x7bbe42e1c2b0>, ctx=JaxprEqnContext(compute_type=None,threefry_partitionable=False),xla_metadata={})\n",
            "Scores: (270, 3) Wi0:(270,)\n",
            "Scores_prieme:(180, 3) Wj0:(180,)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1100 [00:00<?, ?it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Upper hyperparameters:\n",
            " tau:1.0, sigma:Traced<ShapedArray(float32[])>with<JVPTrace(level=5/0)> with\n",
            "  primal = Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=3/0)>\n",
            "  tangent = Traced<ShapedArray(float32[])>with<JaxprTrace(level=4/0)> with\n",
            "    pval = (ShapedArray(float32[]), None)\n",
            "    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7bbe45626000>, in_tracers=(Traced<ShapedArray(float32[]):JaxprTrace(level=4/0)>, Traced<ShapedArray(float32[]):JaxprTrace(level=4/0)>, Traced<ShapedArray(float32[]):JaxprTrace(level=4/0)>), out_tracer_refs=[<weakref at 0x7bbe42e13650; to 'JaxprTracer' at 0x7bbe42e13790>], out_avals=[ShapedArray(float32[])], primitive=pjit, params={'jaxpr': { lambda ; a:f32[] b:f32[] c:f32[]. let\n",
            "    d:f32[] = mul a b\n",
            "    e:f32[] = mul d c\n",
            "  in (e,) }, 'in_shardings': (UnspecifiedValue, UnspecifiedValue, UnspecifiedValue), 'out_shardings': (UnspecifiedValue,), 'in_layouts': (None, None, None), 'out_layouts': (None,), 'resource_env': None, 'donated_invars': (False, False, False), 'name': 'clip', 'keep_unused': False, 'inline': False}, effects=set(), source_info=<jax._src.source_info_util.SourceInfo object at 0x7bbe452daaa0>, ctx=JaxprEqnContext(compute_type=None,threefry_partitionable=False),xla_metadata={}), alpha:Traced<ShapedArray(float32[])>with<JVPTrace(level=5/0)> with\n",
            "  primal = Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=3/0)>\n",
            "  tangent = Traced<ShapedArray(float32[])>with<JaxprTrace(level=4/0)> with\n",
            "    pval = (ShapedArray(float32[]), None)\n",
            "    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7bbe45624c10>, in_tracers=(Traced<ShapedArray(float32[]):JaxprTrace(level=4/0)>, Traced<ShapedArray(float32[]):JaxprTrace(level=4/0)>), out_tracer_refs=[<weakref at 0x7bbe4537dee0; to 'JaxprTracer' at 0x7bbe4537dc10>], out_avals=[ShapedArray(float32[])], primitive=pjit, params={'jaxpr': { lambda ; a:f32[] b:f32[]. let c:f32[] = mul a b in (c,) }, 'in_shardings': (UnspecifiedValue, UnspecifiedValue), 'out_shardings': (UnspecifiedValue,), 'in_layouts': (None, None), 'out_layouts': (None,), 'resource_env': None, 'donated_invars': (False, False), 'name': 'exp', 'keep_unused': False, 'inline': True}, effects=set(), source_info=<jax._src.source_info_util.SourceInfo object at 0x7bbe42e76ce0>, ctx=JaxprEqnContext(compute_type=None,threefry_partitionable=False),xla_metadata={}), a:Traced<ShapedArray(float32[3])>with<JVPTrace(level=5/0)> with\n",
            "  primal = Traced<ShapedArray(float32[3])>with<DynamicJaxprTrace(level=3/0)>\n",
            "  tangent = Traced<ShapedArray(float32[3])>with<JaxprTrace(level=4/0)> with\n",
            "    pval = (ShapedArray(float32[3]), None)\n",
            "    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7bbe45624b40>, in_tracers=(Traced<ShapedArray(float32[3]):JaxprTrace(level=4/0)>, Traced<ShapedArray(float32[3]):JaxprTrace(level=4/0)>), out_tracer_refs=[<weakref at 0x7bbe4537ec00; to 'JaxprTracer' at 0x7bbe4537db20>], out_avals=[ShapedArray(float32[3])], primitive=pjit, params={'jaxpr': { lambda ; a:f32[3] b:f32[3]. let c:f32[3] = mul a b in (c,) }, 'in_shardings': (UnspecifiedValue, UnspecifiedValue), 'out_shardings': (UnspecifiedValue,), 'in_layouts': (None, None), 'out_layouts': (None,), 'resource_env': None, 'donated_invars': (False, False), 'name': 'exp', 'keep_unused': False, 'inline': True}, effects=set(), source_info=<jax._src.source_info_util.SourceInfo object at 0x7bbe42e74d30>, ctx=JaxprEqnContext(compute_type=None,threefry_partitionable=False),xla_metadata={}), b:Traced<ShapedArray(float32[3])>with<JVPTrace(level=5/0)> with\n",
            "  primal = Traced<ShapedArray(float32[3])>with<DynamicJaxprTrace(level=3/0)>\n",
            "  tangent = Traced<ShapedArray(float32[3])>with<JaxprTrace(level=4/0)> with\n",
            "    pval = (ShapedArray(float32[3]), None)\n",
            "    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7bbe45624c00>, in_tracers=(Traced<ShapedArray(float32[3]):JaxprTrace(level=4/0)>, Traced<ShapedArray(float32[3]):JaxprTrace(level=4/0)>), out_tracer_refs=[<weakref at 0x7bbe42fc8130; to 'JaxprTracer' at 0x7bbe42fc8220>], out_avals=[ShapedArray(float32[3])], primitive=pjit, params={'jaxpr': { lambda ; a:f32[3] b:f32[3]. let c:f32[3] = mul a b in (c,) }, 'in_shardings': (UnspecifiedValue, UnspecifiedValue), 'out_shardings': (UnspecifiedValue,), 'in_layouts': (None, None), 'out_layouts': (None,), 'resource_env': None, 'donated_invars': (False, False), 'name': 'exp', 'keep_unused': False, 'inline': True}, effects=set(), source_info=<jax._src.source_info_util.SourceInfo object at 0x7bbe42e88bb0>, ctx=JaxprEqnContext(compute_type=None,threefry_partitionable=False),xla_metadata={})\n",
            "Bottom hyperparameters:\n",
            " tau:1.0, sigma:Traced<ShapedArray(float32[])>with<JVPTrace(level=5/0)> with\n",
            "  primal = Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=3/0)>\n",
            "  tangent = Traced<ShapedArray(float32[])>with<JaxprTrace(level=4/0)> with\n",
            "    pval = (ShapedArray(float32[]), None)\n",
            "    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7bbe45625550>, in_tracers=(Traced<ShapedArray(float32[]):JaxprTrace(level=4/0)>, Traced<ShapedArray(float32[]):JaxprTrace(level=4/0)>, Traced<ShapedArray(float32[]):JaxprTrace(level=4/0)>), out_tracer_refs=[<weakref at 0x7bbe455e4b30; to 'JaxprTracer' at 0x7bbe455e6520>], out_avals=[ShapedArray(float32[])], primitive=pjit, params={'jaxpr': { lambda ; a:f32[] b:f32[] c:f32[]. let\n",
            "    d:f32[] = mul a b\n",
            "    e:f32[] = mul d c\n",
            "  in (e,) }, 'in_shardings': (UnspecifiedValue, UnspecifiedValue, UnspecifiedValue), 'out_shardings': (UnspecifiedValue,), 'in_layouts': (None, None, None), 'out_layouts': (None,), 'resource_env': None, 'donated_invars': (False, False, False), 'name': 'clip', 'keep_unused': False, 'inline': False}, effects=set(), source_info=<jax._src.source_info_util.SourceInfo object at 0x7bbe42e0a230>, ctx=JaxprEqnContext(compute_type=None,threefry_partitionable=False),xla_metadata={}), alpha:Traced<ShapedArray(float32[])>with<JVPTrace(level=5/0)> with\n",
            "  primal = Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=3/0)>\n",
            "  tangent = Traced<ShapedArray(float32[])>with<JaxprTrace(level=4/0)> with\n",
            "    pval = (ShapedArray(float32[]), None)\n",
            "    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7bbe456255b0>, in_tracers=(Traced<ShapedArray(float32[]):JaxprTrace(level=4/0)>, Traced<ShapedArray(float32[]):JaxprTrace(level=4/0)>), out_tracer_refs=[<weakref at 0x7bbe4537cef0; to 'JaxprTracer' at 0x7bbe4537fec0>], out_avals=[ShapedArray(float32[])], primitive=pjit, params={'jaxpr': { lambda ; a:f32[] b:f32[]. let c:f32[] = mul a b in (c,) }, 'in_shardings': (UnspecifiedValue, UnspecifiedValue), 'out_shardings': (UnspecifiedValue,), 'in_layouts': (None, None), 'out_layouts': (None,), 'resource_env': None, 'donated_invars': (False, False), 'name': 'exp', 'keep_unused': False, 'inline': True}, effects=set(), source_info=<jax._src.source_info_util.SourceInfo object at 0x7bbe42e77b20>, ctx=JaxprEqnContext(compute_type=None,threefry_partitionable=False),xla_metadata={}), a:Traced<ShapedArray(float32[3])>with<JVPTrace(level=5/0)> with\n",
            "  primal = Traced<ShapedArray(float32[3])>with<DynamicJaxprTrace(level=3/0)>\n",
            "  tangent = Traced<ShapedArray(float32[3])>with<JaxprTrace(level=4/0)> with\n",
            "    pval = (ShapedArray(float32[3]), None)\n",
            "    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7bbe45624050>, in_tracers=(Traced<ShapedArray(float32[3]):JaxprTrace(level=4/0)>, Traced<ShapedArray(float32[3]):JaxprTrace(level=4/0)>), out_tracer_refs=[<weakref at 0x7bbe42fc8d60; to 'JaxprTracer' at 0x7bbe42fc8b80>], out_avals=[ShapedArray(float32[3])], primitive=pjit, params={'jaxpr': { lambda ; a:f32[3] b:f32[3]. let c:f32[3] = mul a b in (c,) }, 'in_shardings': (UnspecifiedValue, UnspecifiedValue), 'out_shardings': (UnspecifiedValue,), 'in_layouts': (None, None), 'out_layouts': (None,), 'resource_env': None, 'donated_invars': (False, False), 'name': 'exp', 'keep_unused': False, 'inline': True}, effects=set(), source_info=<jax._src.source_info_util.SourceInfo object at 0x7bbe42e884c0>, ctx=JaxprEqnContext(compute_type=None,threefry_partitionable=False),xla_metadata={}), b:Traced<ShapedArray(float32[3])>with<JVPTrace(level=5/0)> with\n",
            "  primal = Traced<ShapedArray(float32[3])>with<DynamicJaxprTrace(level=3/0)>\n",
            "  tangent = Traced<ShapedArray(float32[3])>with<JaxprTrace(level=4/0)> with\n",
            "    pval = (ShapedArray(float32[3]), None)\n",
            "    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7bbe45625820>, in_tracers=(Traced<ShapedArray(float32[3]):JaxprTrace(level=4/0)>, Traced<ShapedArray(float32[3]):JaxprTrace(level=4/0)>), out_tracer_refs=[<weakref at 0x7bbe42fc9850; to 'JaxprTracer' at 0x7bbe42fc9620>], out_avals=[ShapedArray(float32[3])], primitive=pjit, params={'jaxpr': { lambda ; a:f32[3] b:f32[3]. let c:f32[3] = mul a b in (c,) }, 'in_shardings': (UnspecifiedValue, UnspecifiedValue), 'out_shardings': (UnspecifiedValue,), 'in_layouts': (None, None), 'out_layouts': (None,), 'resource_env': None, 'donated_invars': (False, False), 'name': 'exp', 'keep_unused': False, 'inline': True}, effects=set(), source_info=<jax._src.source_info_util.SourceInfo object at 0x7bbe42e89e10>, ctx=JaxprEqnContext(compute_type=None,threefry_partitionable=False),xla_metadata={})\n",
            "Scores: (270, 3) Wi0:(270,)\n",
            "Scores_prieme:(180, 3) Wj0:(180,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "sample: 100%|██████████| 1100/1100 [11:01<00:00,  1.66it/s, 127 steps of size 5.34e-02. acc. prob=0.88]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upper hyperparameters:\n",
            " tau:1.0, sigma:Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=2/0)>, alpha:Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=2/0)>, a:Traced<ShapedArray(float32[3])>with<DynamicJaxprTrace(level=2/0)>, b:Traced<ShapedArray(float32[3])>with<DynamicJaxprTrace(level=2/0)>\n",
            "Bottom hyperparameters:\n",
            " tau:1.0, sigma:Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=2/0)>, alpha:Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=2/0)>, a:Traced<ShapedArray(float32[3])>with<DynamicJaxprTrace(level=2/0)>, b:Traced<ShapedArray(float32[3])>with<DynamicJaxprTrace(level=2/0)>\n",
            "Scores: (270, 3) Wi0:(270,)\n",
            "Scores_prieme:(180, 3) Wj0:(180,)\n",
            "\n",
            "                         mean       std    median      5.0%     95.0%     n_eff     r_hat\n",
            "               a[0]      1.79      0.75      1.64      0.76      2.84     32.22      1.00\n",
            "               a[1]      1.09      0.77      0.99      0.17      2.22      5.70      1.20\n",
            "               a[2]      0.38      0.18      0.36      0.11      0.62      6.43      1.05\n",
            "         a_prime[0]      1.72      1.05      1.38      0.46      3.32     25.51      1.05\n",
            "         a_prime[1]      1.74      1.12      1.58      0.19      3.22      9.93      1.23\n",
            "         a_prime[2]      1.17      1.04      0.87      0.07      2.70      5.38      1.46\n",
            "              alpha     12.65      2.23     12.78      9.71     16.85    103.91      1.00\n",
            "        alpha_prime      8.88      1.67      8.82      6.53     11.98    165.39      1.00\n",
            "               b[0]      2.04      1.19      1.98      0.26      3.56     51.37      1.00\n",
            "               b[1]      1.66      1.22      1.48      0.04      3.43     22.97      1.05\n",
            "               b[2]      1.82      1.15      1.64      0.06      3.26     90.74      1.04\n",
            "         b_prime[0]      1.91      1.23      1.67      0.23      3.65     52.13      1.03\n",
            "         b_prime[1]      1.72      1.18      1.57      0.10      3.38     23.81      1.12\n",
            "         b_prime[2]      1.68      1.15      1.45      0.05      3.11    115.56      1.00\n",
            "        scores[0,0]      1.37      1.75      0.80      0.00      3.46     80.51      1.00\n",
            "        scores[0,1]      1.53      4.09      0.42      0.00      3.68     31.82      1.08\n",
            "        scores[0,2]      0.33      1.02      0.07      0.00      0.70    338.17      1.00\n",
            "        scores[1,0]      1.26      1.67      0.75      0.01      2.81     84.19      1.00\n",
            "        scores[1,1]      1.68      4.85      0.40      0.00      3.78     29.15      1.07\n",
            "        scores[1,2]      0.31      0.71      0.07      0.00      0.83    353.52      1.00\n",
            "        scores[2,0]      1.50      2.43      0.89      0.01      3.24    191.23      1.00\n",
            "        scores[2,1]      2.04      6.12      0.48      0.00      4.16     34.80      1.07\n",
            "        scores[2,2]      0.36      0.68      0.09      0.00      1.02    240.69      1.00\n",
            "        scores[3,0]      1.19      1.37      0.72      0.00      2.78     67.76      1.00\n",
            "        scores[3,1]      2.13      6.18      0.44      0.00      4.81     29.39      1.07\n",
            "        scores[3,2]      0.37      1.30      0.09      0.00      0.82    325.87      1.00\n",
            "        scores[4,0]      1.62      2.51      0.89      0.01      3.47    138.37      1.00\n",
            "        scores[4,1]      2.34      7.53      0.49      0.00      4.91     49.92      1.05\n",
            "        scores[4,2]      0.55      1.58      0.11      0.00      1.38    246.91      1.00\n",
            "        scores[5,0]      1.31      1.71      0.72      0.00      3.38     73.46      1.00\n",
            "        scores[5,1]      2.07      7.94      0.38      0.00      4.11     59.03      1.04\n",
            "        scores[5,2]      0.37      0.93      0.07      0.00      0.93    233.59      1.00\n",
            "        scores[6,0]      1.54      2.36      0.86      0.01      3.44    107.79      1.00\n",
            "        scores[6,1]      1.94      6.61      0.44      0.00      3.81     55.80      1.05\n",
            "        scores[6,2]      0.46      1.30      0.09      0.00      1.24    441.81      1.00\n",
            "        scores[7,0]      1.83      2.86      1.11      0.00      3.50     84.12      1.00\n",
            "        scores[7,1]      3.44      9.70      0.78      0.00      7.55     31.26      1.08\n",
            "        scores[7,2]      0.45      1.13      0.09      0.00      0.97    275.25      1.00\n",
            "        scores[8,0]      1.26      1.71      0.79      0.00      2.76     82.69      1.00\n",
            "        scores[8,1]      1.53      3.82      0.42      0.00      3.61     28.48      1.08\n",
            "        scores[8,2]      0.37      0.87      0.08      0.00      0.90    294.66      1.01\n",
            "        scores[9,0]      1.56      2.06      0.86      0.00      3.66     69.60      1.01\n",
            "        scores[9,1]      2.35      5.70      0.56      0.00      5.37     24.67      1.10\n",
            "        scores[9,2]      0.32      0.81      0.07      0.00      0.80    300.70      1.00\n",
            "       scores[10,0]      1.26      1.76      0.76      0.01      2.71     87.76      1.00\n",
            "       scores[10,1]      1.76      4.92      0.38      0.00      3.66     32.78      1.07\n",
            "       scores[10,2]      0.28      0.63      0.05      0.00      0.87    315.83      1.00\n",
            "       scores[11,0]      1.46      1.95      0.85      0.01      3.20     66.25      1.00\n",
            "       scores[11,1]      2.35      8.39      0.60      0.00      4.30     46.63      1.04\n",
            "       scores[11,2]      0.33      0.73      0.07      0.00      0.88    238.70      1.00\n",
            "       scores[12,0]      1.41      1.74      0.88      0.00      3.13     66.62      1.00\n",
            "       scores[12,1]      2.67      6.35      0.66      0.00      6.30     20.46      1.12\n",
            "       scores[12,2]      0.38      1.09      0.08      0.00      0.88    247.28      1.00\n",
            "       scores[13,0]      1.17      1.47      0.66      0.00      2.75     76.78      1.00\n",
            "       scores[13,1]      2.02      6.78      0.44      0.00      3.28     46.47      1.05\n",
            "       scores[13,2]      0.29      0.71      0.08      0.00      0.73    259.60      1.01\n",
            "       scores[14,0]      1.24      2.04      0.69      0.00      2.80     84.93      1.00\n",
            "       scores[14,1]      1.74      5.87      0.46      0.00      3.74     50.01      1.05\n",
            "       scores[14,2]      0.34      1.17      0.05      0.00      0.71    375.68      1.00\n",
            "       scores[15,0]      1.43      1.78      0.87      0.00      3.15     86.26      1.00\n",
            "       scores[15,1]      2.10      7.95      0.44      0.00      4.34     65.97      1.04\n",
            "       scores[15,2]      0.52      1.30      0.09      0.00      1.25    230.59      1.00\n",
            "       scores[16,0]      1.37      1.78      0.80      0.00      3.23     68.87      1.00\n",
            "       scores[16,1]      2.42      6.04      0.57      0.00      4.74     22.93      1.09\n",
            "       scores[16,2]      0.42      1.06      0.06      0.00      1.09    388.43      1.01\n",
            "       scores[17,0]      1.28      1.77      0.74      0.01      3.01     64.69      1.00\n",
            "       scores[17,1]      1.68      5.14      0.41      0.00      3.34     40.02      1.06\n",
            "       scores[17,2]      0.33      0.79      0.07      0.00      0.81    258.17      1.00\n",
            "       scores[18,0]      1.17      1.60      0.73      0.00      2.66    107.32      1.00\n",
            "       scores[18,1]      1.65      4.94      0.42      0.00      2.94     39.09      1.06\n",
            "       scores[18,2]      0.31      0.70      0.07      0.00      0.85    242.95      1.00\n",
            "       scores[19,0]      1.31      1.69      0.70      0.01      3.17     62.28      1.00\n",
            "       scores[19,1]      1.97      5.92      0.43      0.00      3.39     36.11      1.06\n",
            "       scores[19,2]      0.39      1.43      0.07      0.00      0.98    289.78      1.00\n",
            "       scores[20,0]      1.38      2.00      0.77      0.01      3.04     77.37      1.00\n",
            "       scores[20,1]      2.29      6.97      0.54      0.00      5.11     37.31      1.07\n",
            "       scores[20,2]      0.36      0.99      0.07      0.00      0.98    348.84      1.00\n",
            "       scores[21,0]      1.39      1.68      0.85      0.03      3.27     97.45      1.00\n",
            "       scores[21,1]      2.06      5.44      0.50      0.00      4.30     27.65      1.08\n",
            "       scores[21,2]      0.49      1.39      0.09      0.00      1.21    239.97      1.00\n",
            "       scores[22,0]      1.52      1.78      0.93      0.01      3.28     75.67      1.00\n",
            "       scores[22,1]      1.89      4.51      0.44      0.00      5.21     23.60      1.08\n",
            "       scores[22,2]      0.43      1.12      0.12      0.00      0.92    376.95      1.00\n",
            "       scores[23,0]      1.40      1.68      0.81      0.00      3.38     34.30      1.01\n",
            "       scores[23,1]      2.75      7.02      0.69      0.00      6.13     25.14      1.08\n",
            "       scores[23,2]      0.44      2.75      0.06      0.00      0.89    442.21      1.00\n",
            "       scores[24,0]      1.46      1.94      0.87      0.01      3.20     95.04      1.00\n",
            "       scores[24,1]      2.11      5.77      0.48      0.00      4.38     41.90      1.07\n",
            "       scores[24,2]      0.46      1.17      0.09      0.00      1.17    377.19      1.00\n",
            "       scores[25,0]      1.47      2.46      0.85      0.01      3.01    115.52      1.00\n",
            "       scores[25,1]      2.28      6.40      0.53      0.00      5.71     30.09      1.07\n",
            "       scores[25,2]      0.37      0.93      0.07      0.00      0.81    213.98      1.01\n",
            "       scores[26,0]      1.33      1.91      0.75      0.00      3.08     63.79      1.01\n",
            "       scores[26,1]      2.30      8.22      0.54      0.00      4.15     53.70      1.04\n",
            "       scores[26,2]      0.35      0.87      0.08      0.00      0.89    349.16      1.00\n",
            "       scores[27,0]      1.41      1.92      0.76      0.01      3.43     49.15      1.00\n",
            "       scores[27,1]      2.57      8.76      0.59      0.00      4.61     50.92      1.05\n",
            "       scores[27,2]      0.38      1.20      0.06      0.00      0.89    301.49      1.01\n",
            "       scores[28,0]      1.56      2.25      0.90      0.00      3.52    106.30      1.00\n",
            "       scores[28,1]      1.73      4.78      0.51      0.00      3.56     36.96      1.06\n",
            "       scores[28,2]      0.52      1.49      0.10      0.00      1.08    290.08      1.00\n",
            "       scores[29,0]      1.27      1.84      0.69      0.00      2.93     79.59      1.00\n",
            "       scores[29,1]      1.84      6.89      0.35      0.00      3.92     51.42      1.04\n",
            "       scores[29,2]      0.36      0.93      0.07      0.00      0.96    298.54      1.00\n",
            "       scores[30,0]      1.47      1.80      0.87      0.02      3.48     71.22      1.00\n",
            "       scores[30,1]      1.59      4.05      0.50      0.00      2.97     35.13      1.07\n",
            "       scores[30,2]      0.44      1.56      0.09      0.00      0.91    321.86      1.00\n",
            "       scores[31,0]      1.52      1.92      0.90      0.00      3.34     46.70      1.01\n",
            "       scores[31,1]      2.97      8.89      0.71      0.00      6.40     42.08      1.06\n",
            "       scores[31,2]      0.46      1.37      0.07      0.00      0.96    225.71      1.00\n",
            "       scores[32,0]      1.24      1.66      0.72      0.00      2.74    107.44      1.00\n",
            "       scores[32,1]      1.72      5.02      0.39      0.00      3.65     31.55      1.07\n",
            "       scores[32,2]      0.34      0.88      0.08      0.00      0.79    321.77      1.01\n",
            "       scores[33,0]      1.26      2.07      0.72      0.00      2.68     92.06      1.00\n",
            "       scores[33,1]      1.67      5.14      0.40      0.00      3.38     41.97      1.05\n",
            "       scores[33,2]      0.34      0.85      0.07      0.00      0.82    261.73      1.01\n",
            "       scores[34,0]      1.94      2.41      1.22      0.00      4.45     66.14      1.00\n",
            "       scores[34,1]      3.32      8.87      0.88      0.00      7.02     31.03      1.08\n",
            "       scores[34,2]      0.56      1.34      0.11      0.00      1.54    211.79      1.00\n",
            "       scores[35,0]      1.29      1.88      0.69      0.02      3.00     75.48      1.00\n",
            "       scores[35,1]      1.90      6.07      0.39      0.00      4.34     40.91      1.06\n",
            "       scores[35,2]      0.34      0.96      0.07      0.00      0.74    288.53      1.00\n",
            "       scores[36,0]      1.20      1.37      0.79      0.00      2.83     75.30      1.00\n",
            "       scores[36,1]      1.97      8.58      0.39      0.00      3.71     73.56      1.03\n",
            "       scores[36,2]      0.33      0.94      0.08      0.00      0.79    314.31      1.00\n",
            "       scores[37,0]      1.54      2.11      0.83      0.01      3.48    101.18      1.00\n",
            "       scores[37,1]      2.26      6.24      0.52      0.00      4.68     34.90      1.08\n",
            "       scores[37,2]      0.40      1.02      0.07      0.00      0.98    290.91      1.00\n",
            "       scores[38,0]      1.43      2.31      0.81      0.01      3.01     94.93      1.00\n",
            "       scores[38,1]      1.97      4.74      0.50      0.00      5.47     27.08      1.08\n",
            "       scores[38,2]      0.44      1.12      0.08      0.00      1.21    324.10      1.00\n",
            "       scores[39,0]      1.52      2.23      0.87      0.00      3.21     83.90      1.00\n",
            "       scores[39,1]      2.18      5.76      0.56      0.00      4.83     26.35      1.08\n",
            "       scores[39,2]      0.35      0.77      0.09      0.00      0.91    349.32      1.01\n",
            "       scores[40,0]      1.33      1.63      0.80      0.00      3.15     49.92      1.01\n",
            "       scores[40,1]      2.60      8.08      0.53      0.00      5.18     38.79      1.06\n",
            "       scores[40,2]      0.45      1.84      0.07      0.00      0.77    341.64      1.00\n",
            "       scores[41,0]      1.46      1.88      0.84      0.05      3.58     89.98      1.00\n",
            "       scores[41,1]      2.05      5.65      0.46      0.00      4.79     33.22      1.06\n",
            "       scores[41,2]      0.46      1.15      0.08      0.00      1.16    206.19      1.01\n",
            "       scores[42,0]      1.30      1.85      0.72      0.00      2.82     85.14      1.00\n",
            "       scores[42,1]      1.59      3.94      0.45      0.00      3.73     30.97      1.08\n",
            "       scores[42,2]      0.36      1.03      0.07      0.00      0.87    324.09      1.00\n",
            "       scores[43,0]      1.28      1.64      0.74      0.01      2.89     79.91      1.00\n",
            "       scores[43,1]      2.10      7.80      0.45      0.00      4.08     61.43      1.04\n",
            "       scores[43,2]      0.41      1.05      0.09      0.00      0.86    234.68      1.01\n",
            "       scores[44,0]      1.63      2.40      0.96      0.00      3.60     33.70      1.03\n",
            "       scores[44,1]      3.27      8.08      0.79      0.00      7.99     24.70      1.10\n",
            "       scores[44,2]      0.42      0.97      0.09      0.00      1.03    237.69      1.00\n",
            "       scores[45,0]      1.65      2.43      0.90      0.01      3.93     58.20      1.01\n",
            "       scores[45,1]      2.84      7.49      0.68      0.00      6.30     26.21      1.09\n",
            "       scores[45,2]      0.42      1.97      0.06      0.00      0.87    337.42      1.00\n",
            "       scores[46,0]      1.56      1.91      0.96      0.00      3.88     32.37      1.02\n",
            "       scores[46,1]      2.99      8.16      0.76      0.00      6.63     27.33      1.08\n",
            "       scores[46,2]      0.38      1.38      0.07      0.00      0.85    373.26      1.00\n",
            "       scores[47,0]      1.22      1.80      0.72      0.00      3.02    107.93      1.00\n",
            "       scores[47,1]      1.64      4.76      0.40      0.00      3.90     42.74      1.06\n",
            "       scores[47,2]      0.40      1.20      0.07      0.00      0.91    322.50      1.00\n",
            "       scores[48,0]      1.62      2.42      0.99      0.00      3.40     99.98      1.00\n",
            "       scores[48,1]      2.68      8.61      0.70      0.00      4.33     46.16      1.05\n",
            "       scores[48,2]      0.37      0.97      0.08      0.00      1.05    288.18      1.00\n",
            "       scores[49,0]      1.30      1.57      0.79      0.02      3.26     68.15      1.00\n",
            "       scores[49,1]      2.06      6.17      0.54      0.00      4.86     38.75      1.06\n",
            "       scores[49,2]      0.38      1.20      0.06      0.00      0.80    340.79      1.00\n",
            "       scores[50,0]      1.79      2.75      0.90      0.01      3.78     70.81      1.01\n",
            "       scores[50,1]      3.74     11.13      0.92      0.00      8.24     33.75      1.07\n",
            "       scores[50,2]      0.38      1.21      0.07      0.00      0.90    210.24      1.00\n",
            "       scores[51,0]      1.59      2.00      0.97      0.01      3.55     93.14      1.00\n",
            "       scores[51,1]      2.08      5.73      0.66      0.00      4.34     40.17      1.06\n",
            "       scores[51,2]      0.38      0.67      0.10      0.00      1.06    237.44      1.00\n",
            "       scores[52,0]      1.40      1.89      0.87      0.00      3.07    107.37      1.00\n",
            "       scores[52,1]      1.90      5.09      0.45      0.00      4.03     27.80      1.09\n",
            "       scores[52,2]      0.40      1.00      0.10      0.00      1.01    237.41      1.00\n",
            "       scores[53,0]      1.52      2.18      0.85      0.01      3.25     85.05      1.00\n",
            "       scores[53,1]      2.00      6.48      0.54      0.00      3.58     50.14      1.05\n",
            "       scores[53,2]      0.47      1.04      0.10      0.00      1.25    243.87      1.00\n",
            "       scores[54,0]      1.16      1.37      0.73      0.01      2.59     78.88      1.00\n",
            "       scores[54,1]      1.93      5.75      0.40      0.00      4.15     32.44      1.07\n",
            "       scores[54,2]      0.34      0.73      0.09      0.00      0.90    344.31      1.00\n",
            "       scores[55,0]      1.37      1.67      0.81      0.00      3.13     63.46      1.01\n",
            "       scores[55,1]      1.94      4.36      0.56      0.00      4.72     24.86      1.11\n",
            "       scores[55,2]      0.35      1.36      0.07      0.00      0.82    334.32      1.00\n",
            "       scores[56,0]      1.32      1.92      0.73      0.00      2.98    104.38      1.00\n",
            "       scores[56,1]      1.48      4.19      0.35      0.00      3.47     33.88      1.07\n",
            "       scores[56,2]      0.30      0.70      0.07      0.00      0.77    311.42      1.00\n",
            "       scores[57,0]      1.25      1.85      0.70      0.00      2.92     85.17      1.00\n",
            "       scores[57,1]      1.85      6.77      0.39      0.00      3.30     56.01      1.04\n",
            "       scores[57,2]      0.27      0.62      0.06      0.00      0.69    360.09      1.00\n",
            "       scores[58,0]      1.19      1.48      0.75      0.00      2.69     80.08      1.00\n",
            "       scores[58,1]      1.65      4.84      0.43      0.00      3.64     36.38      1.06\n",
            "       scores[58,2]      0.29      0.60      0.08      0.00      0.72    305.54      1.00\n",
            "       scores[59,0]      1.39      2.27      0.81      0.01      2.94     95.98      1.00\n",
            "       scores[59,1]      2.18      5.95      0.58      0.00      5.05     34.95      1.08\n",
            "       scores[59,2]      0.39      1.11      0.07      0.00      0.90    331.58      1.00\n",
            "       scores[60,0]      1.26      1.77      0.73      0.00      2.95     63.18      1.01\n",
            "       scores[60,1]      1.85      5.38      0.40      0.00      3.95     30.41      1.07\n",
            "       scores[60,2]      0.34      0.72      0.09      0.00      0.89    333.40      1.00\n",
            "       scores[61,0]      1.54      2.46      0.88      0.00      3.62     72.59      1.01\n",
            "       scores[61,1]      2.65      7.62      0.57      0.00      6.20     33.56      1.08\n",
            "       scores[61,2]      0.38      1.13      0.06      0.00      0.89    335.12      1.00\n",
            "       scores[62,0]      1.23      1.85      0.70      0.01      2.58     77.95      1.00\n",
            "       scores[62,1]      1.74      5.41      0.43      0.00      3.92     35.80      1.06\n",
            "       scores[62,2]      0.41      1.27      0.08      0.00      0.92    268.72      1.00\n",
            "       scores[63,0]      1.23      1.53      0.73      0.00      2.87     66.05      1.00\n",
            "       scores[63,1]      1.83      5.33      0.39      0.00      3.54     39.50      1.06\n",
            "       scores[63,2]      0.31      0.69      0.07      0.00      0.74    116.51      1.00\n",
            "       scores[64,0]      1.35      1.64      0.80      0.01      3.28     77.70      1.00\n",
            "       scores[64,1]      2.07      6.29      0.45      0.00      3.58     39.65      1.06\n",
            "       scores[64,2]      0.42      0.82      0.11      0.00      1.30    299.52      1.00\n",
            "       scores[65,0]      1.45      1.83      0.90      0.01      3.20     90.97      1.00\n",
            "       scores[65,1]      2.57      8.35      0.59      0.00      5.67     51.75      1.05\n",
            "       scores[65,2]      0.31      0.59      0.09      0.00      0.82    195.15      1.00\n",
            "       scores[66,0]      1.47      2.27      0.76      0.01      3.38     73.49      1.01\n",
            "       scores[66,1]      2.53      7.06      0.51      0.00      5.69     30.77      1.07\n",
            "       scores[66,2]      0.37      1.37      0.08      0.00      0.85    431.91      1.00\n",
            "       scores[67,0]      1.54      1.96      0.89      0.01      3.54     82.78      1.00\n",
            "       scores[67,1]      1.84      4.82      0.61      0.00      3.85     32.82      1.06\n",
            "       scores[67,2]      0.56      1.36      0.12      0.00      1.28    249.42      1.00\n",
            "       scores[68,0]      1.51      2.19      0.89      0.02      3.58     79.73      1.00\n",
            "       scores[68,1]      2.83      9.28      0.61      0.00      5.72     40.14      1.06\n",
            "       scores[68,2]      0.55      2.43      0.07      0.00      0.99    371.51      1.00\n",
            "       scores[69,0]      1.32      1.77      0.73      0.01      2.85    122.64      1.00\n",
            "       scores[69,1]      2.08      5.44      0.47      0.00      4.61     27.60      1.08\n",
            "       scores[69,2]      0.33      0.86      0.06      0.00      0.82    282.84      1.01\n",
            "       scores[70,0]      1.48      2.20      0.95      0.00      2.99    100.80      1.00\n",
            "       scores[70,1]      2.17      7.63      0.55      0.00      3.77     62.37      1.04\n",
            "       scores[70,2]      0.39      0.90      0.06      0.00      1.03    305.73      1.00\n",
            "       scores[71,0]      1.25      1.76      0.72      0.01      2.73    111.57      1.00\n",
            "       scores[71,1]      1.93      5.84      0.49      0.00      3.87     50.84      1.06\n",
            "       scores[71,2]      0.36      1.15      0.06      0.00      0.89    317.81      1.00\n",
            "       scores[72,0]      1.58      2.36      0.89      0.02      3.37     91.14      1.01\n",
            "       scores[72,1]      2.03      4.22      0.58      0.00      5.38     20.07      1.11\n",
            "       scores[72,2]      0.44      1.18      0.09      0.00      1.06    198.20      1.00\n",
            "       scores[73,0]      1.29      1.93      0.67      0.00      3.21     82.06      1.00\n",
            "       scores[73,1]      1.98      6.56      0.45      0.00      4.37     44.60      1.05\n",
            "       scores[73,2]      0.34      0.97      0.07      0.00      0.83    291.58      1.00\n",
            "       scores[74,0]      1.27      2.15      0.68      0.00      3.10    120.04      1.00\n",
            "       scores[74,1]      1.76      5.84      0.46      0.00      3.60     53.62      1.05\n",
            "       scores[74,2]      0.36      0.93      0.07      0.00      0.83    191.14      1.01\n",
            "       scores[75,0]      1.30      1.64      0.77      0.01      3.05     77.87      1.00\n",
            "       scores[75,1]      1.74      5.60      0.37      0.00      3.27     39.41      1.05\n",
            "       scores[75,2]      0.37      1.04      0.06      0.00      0.93    312.43      1.00\n",
            "       scores[76,0]      1.43      1.95      0.79      0.01      3.79    116.55      1.00\n",
            "       scores[76,1]      2.10      7.50      0.51      0.00      3.79     55.55      1.04\n",
            "       scores[76,2]      0.41      1.07      0.08      0.00      0.95    258.27      1.00\n",
            "       scores[77,0]      1.41      2.02      0.77      0.01      3.08     76.04      1.00\n",
            "       scores[77,1]      2.29      6.62      0.53      0.00      4.74     39.31      1.06\n",
            "       scores[77,2]      0.36      1.12      0.07      0.00      0.86    337.01      1.00\n",
            "       scores[78,0]      1.20      1.64      0.68      0.00      2.76    100.65      1.00\n",
            "       scores[78,1]      1.81      5.51      0.38      0.00      3.03     32.76      1.07\n",
            "       scores[78,2]      0.32      0.76      0.04      0.00      0.87    276.40      1.00\n",
            "       scores[79,0]      1.32      1.83      0.78      0.02      3.01    108.80      1.00\n",
            "       scores[79,1]      1.65      5.15      0.43      0.00      3.28     36.41      1.06\n",
            "       scores[79,2]      0.37      1.19      0.06      0.00      0.80    225.60      1.00\n",
            "       scores[80,0]      1.24      1.70      0.69      0.00      2.59     94.05      1.00\n",
            "       scores[80,1]      1.74      4.50      0.39      0.00      4.29     28.39      1.08\n",
            "       scores[80,2]      0.45      2.38      0.07      0.00      0.84    368.71      1.00\n",
            "       scores[81,0]      1.32      1.85      0.74      0.02      2.91     83.01      1.00\n",
            "       scores[81,1]      1.85      5.98      0.44      0.00      3.77     39.80      1.06\n",
            "       scores[81,2]      0.34      0.79      0.07      0.00      0.87    282.75      1.00\n",
            "       scores[82,0]      1.44      2.04      0.88      0.02      3.13     97.10      1.00\n",
            "       scores[82,1]      1.92      5.06      0.42      0.00      4.31     25.68      1.08\n",
            "       scores[82,2]      0.49      1.68      0.08      0.00      1.24    299.14      1.00\n",
            "       scores[83,0]      1.49      2.53      0.75      0.00      3.49    138.98      1.00\n",
            "       scores[83,1]      2.41      6.96      0.51      0.00      5.02     47.60      1.06\n",
            "       scores[83,2]      0.30      0.73      0.06      0.00      0.81    232.72      1.00\n",
            "       scores[84,0]      1.32      2.25      0.67      0.00      3.22     81.10      1.00\n",
            "       scores[84,1]      1.88      5.06      0.41      0.00      3.98     31.23      1.08\n",
            "       scores[84,2]      0.27      0.66      0.06      0.00      0.71    249.89      1.00\n",
            "       scores[85,0]      1.70      2.31      0.93      0.01      3.92     66.95      1.00\n",
            "       scores[85,1]      2.73      6.90      0.74      0.00      7.16     23.11      1.09\n",
            "       scores[85,2]      0.45      1.50      0.08      0.00      0.95    345.79      1.00\n",
            "       scores[86,0]      1.39      2.07      0.73      0.01      3.31     95.77      1.00\n",
            "       scores[86,1]      2.51      7.60      0.53      0.00      4.98     37.33      1.07\n",
            "       scores[86,2]      0.30      0.72      0.06      0.00      0.75    259.74      1.00\n",
            "       scores[87,0]      1.40      2.12      0.76      0.00      3.25     76.59      1.00\n",
            "       scores[87,1]      1.75      5.13      0.52      0.00      3.47     35.36      1.06\n",
            "       scores[87,2]      0.42      1.10      0.09      0.00      0.95    234.97      1.02\n",
            "       scores[88,0]      1.35      1.70      0.77      0.00      3.63     68.46      1.00\n",
            "       scores[88,1]      1.49      5.10      0.41      0.00      3.09     55.35      1.04\n",
            "       scores[88,2]      0.37      1.23      0.06      0.00      0.79    350.65      1.00\n",
            "       scores[89,0]      1.21      1.50      0.74      0.01      2.74    158.72      1.00\n",
            "       scores[89,1]      2.12      6.93      0.47      0.00      3.46     42.89      1.06\n",
            "       scores[89,2]      0.35      1.11      0.06      0.00      0.80    302.62      1.00\n",
            "       scores[90,0]      1.30      1.66      0.73      0.00      2.90     80.15      1.00\n",
            "       scores[90,1]      2.05      7.34      0.40      0.00      3.84     62.61      1.04\n",
            "       scores[90,2]      0.47      2.40      0.06      0.00      0.79    321.30      1.00\n",
            "       scores[91,0]      1.74      2.16      1.04      0.01      3.83     81.31      1.00\n",
            "       scores[91,1]      2.64      7.26      0.75      0.00      5.59     30.44      1.07\n",
            "       scores[91,2]      0.48      1.24      0.11      0.00      1.19    243.68      1.00\n",
            "       scores[92,0]      1.25      1.64      0.75      0.02      3.07     74.55      1.00\n",
            "       scores[92,1]      1.82      5.08      0.46      0.00      3.38     33.19      1.07\n",
            "       scores[92,2]      0.37      1.15      0.07      0.00      0.84    426.43      1.00\n",
            "       scores[93,0]      1.61      2.46      0.95      0.01      3.70     95.09      1.00\n",
            "       scores[93,1]      2.19      7.05      0.51      0.00      4.73     43.25      1.05\n",
            "       scores[93,2]      0.62      1.78      0.14      0.00      1.39    278.85      1.00\n",
            "       scores[94,0]      1.30      2.42      0.67      0.01      3.12     85.08      1.01\n",
            "       scores[94,1]      1.59      4.80      0.40      0.00      3.15     44.40      1.05\n",
            "       scores[94,2]      0.28      0.53      0.06      0.00      0.81    182.79      1.01\n",
            "       scores[95,0]      1.46      2.37      0.82      0.00      3.29    129.78      1.00\n",
            "       scores[95,1]      2.21      6.30      0.57      0.00      5.38     36.59      1.06\n",
            "       scores[95,2]      0.48      1.47      0.12      0.00      1.04    398.06      1.00\n",
            "       scores[96,0]      1.45      1.84      0.85      0.00      3.42     84.72      1.00\n",
            "       scores[96,1]      2.42      6.69      0.58      0.00      4.56     30.22      1.08\n",
            "       scores[96,2]      0.30      0.76      0.06      0.00      0.75    347.55      1.00\n",
            "       scores[97,0]      1.42      1.75      0.84      0.00      3.34     69.14      1.00\n",
            "       scores[97,1]      1.83      5.21      0.47      0.00      3.92     40.54      1.06\n",
            "       scores[97,2]      0.40      0.85      0.08      0.00      1.19    289.28      1.00\n",
            "       scores[98,0]      1.26      1.75      0.73      0.00      2.81     91.63      1.00\n",
            "       scores[98,1]      1.67      5.59      0.32      0.00      3.02     53.92      1.05\n",
            "       scores[98,2]      0.33      0.90      0.07      0.00      0.76    409.05      1.00\n",
            "       scores[99,0]      1.23      1.82      0.71      0.01      2.63     76.27      1.00\n",
            "       scores[99,1]      1.81      5.60      0.46      0.00      3.68     39.96      1.06\n",
            "       scores[99,2]      0.35      1.61      0.06      0.00      0.71    387.62      1.00\n",
            "      scores[100,0]      1.39      2.32      0.82      0.01      2.86    132.67      1.00\n",
            "      scores[100,1]      2.02      5.21      0.49      0.00      4.50     27.06      1.08\n",
            "      scores[100,2]      0.34      0.98      0.09      0.00      0.73    433.11      1.00\n",
            "      scores[101,0]      1.46      2.14      0.84      0.01      3.33    102.27      1.00\n",
            "      scores[101,1]      1.60      3.44      0.43      0.00      4.42     19.22      1.10\n",
            "      scores[101,2]      0.49      1.37      0.10      0.00      1.14    245.49      1.00\n",
            "      scores[102,0]      1.35      1.84      0.79      0.00      2.92     83.04      1.00\n",
            "      scores[102,1]      2.40      7.87      0.53      0.00      5.04     39.21      1.06\n",
            "      scores[102,2]      0.38      1.59      0.07      0.00      0.83    448.23      1.00\n",
            "      scores[103,0]      1.35      2.03      0.82      0.00      3.25     65.17      1.02\n",
            "      scores[103,1]      2.99      7.87      0.75      0.00      7.38     25.03      1.09\n",
            "      scores[103,2]      0.35      1.00      0.06      0.00      0.81    319.07      1.00\n",
            "      scores[104,0]      1.44      1.87      0.93      0.00      3.03     91.09      1.00\n",
            "      scores[104,1]      2.28      8.24      0.50      0.00      5.06     61.53      1.04\n",
            "      scores[104,2]      0.39      1.02      0.08      0.00      1.02    209.55      1.00\n",
            "      scores[105,0]      1.48      1.94      0.87      0.00      3.05     73.43      1.00\n",
            "      scores[105,1]      1.67      3.99      0.50      0.00      3.80     29.50      1.07\n",
            "      scores[105,2]      0.41      0.87      0.09      0.00      1.04    342.64      1.00\n",
            "      scores[106,0]      1.34      1.84      0.79      0.03      2.96     75.74      1.00\n",
            "      scores[106,1]      2.20      6.34      0.47      0.00      4.13     34.17      1.07\n",
            "      scores[106,2]      0.39      0.91      0.07      0.00      1.01    227.99      1.00\n",
            "      scores[107,0]      1.46      1.96      0.92      0.01      3.17     65.77      1.00\n",
            "      scores[107,1]      2.07      5.50      0.53      0.00      3.77     37.08      1.07\n",
            "      scores[107,2]      0.39      0.99      0.07      0.00      1.01    362.21      1.00\n",
            "      scores[108,0]      1.40      2.73      0.81      0.03      3.02    116.16      1.01\n",
            "      scores[108,1]      2.63      6.49      0.64      0.00      6.18     21.53      1.09\n",
            "      scores[108,2]      0.36      0.95      0.07      0.00      0.79    208.09      1.00\n",
            "      scores[109,0]      1.48      2.08      0.84      0.02      3.38     56.64      1.00\n",
            "      scores[109,1]      2.18      5.20      0.56      0.00      5.60     23.77      1.09\n",
            "      scores[109,2]      0.38      0.99      0.07      0.00      0.92    299.60      1.01\n",
            "      scores[110,0]      1.39      1.90      0.86      0.02      3.02     85.88      1.00\n",
            "      scores[110,1]      2.40      7.37      0.49      0.00      4.05     39.43      1.06\n",
            "      scores[110,2]      0.32      0.79      0.07      0.00      0.84    185.01      1.01\n",
            "      scores[111,0]      1.27      1.77      0.74      0.00      2.71    100.82      1.00\n",
            "      scores[111,1]      1.70      4.76      0.45      0.00      3.93     29.29      1.07\n",
            "      scores[111,2]      0.28      0.63      0.06      0.00      0.73    411.50      1.00\n",
            "      scores[112,0]      1.26      1.87      0.76      0.00      2.73     73.14      1.00\n",
            "      scores[112,1]      1.97      6.65      0.42      0.00      3.44     48.27      1.05\n",
            "      scores[112,2]      0.34      0.79      0.08      0.00      0.90    303.61      1.00\n",
            "      scores[113,0]      1.75      2.46      1.02      0.00      4.11     37.88      1.02\n",
            "      scores[113,1]      3.73      9.62      0.88      0.00      7.92     26.36      1.09\n",
            "      scores[113,2]      0.48      1.53      0.08      0.00      1.08    290.74      1.00\n",
            "      scores[114,0]      1.30      1.96      0.62      0.00      2.93     76.81      1.00\n",
            "      scores[114,1]      1.83      6.83      0.40      0.00      3.08     54.27      1.04\n",
            "      scores[114,2]      0.31      0.86      0.07      0.00      0.74    298.50      1.00\n",
            "      scores[115,0]      1.43      2.00      0.80      0.00      3.00     55.88      1.00\n",
            "      scores[115,1]      2.89      8.31      0.54      0.00      5.76     28.45      1.08\n",
            "      scores[115,2]      0.42      1.02      0.08      0.00      1.13    162.08      1.02\n",
            "      scores[116,0]      1.44      1.89      0.85      0.01      3.39     96.18      1.00\n",
            "      scores[116,1]      2.13      6.28      0.50      0.00      4.09     37.52      1.06\n",
            "      scores[116,2]      0.45      1.23      0.10      0.00      1.07    296.81      1.00\n",
            "      scores[117,0]      1.49      2.07      0.82      0.01      3.53     82.73      1.00\n",
            "      scores[117,1]      2.70      7.94      0.57      0.00      5.17     37.27      1.07\n",
            "      scores[117,2]      0.41      1.17      0.06      0.00      1.06    198.47      1.01\n",
            "      scores[118,0]      1.44      2.18      0.85      0.01      3.10     70.32      1.01\n",
            "      scores[118,1]      2.54      6.21      0.68      0.00      6.11     29.89      1.09\n",
            "      scores[118,2]      0.34      0.88      0.08      0.00      0.76    390.26      1.01\n",
            "      scores[119,0]      1.25      1.71      0.66      0.00      3.07    107.50      1.00\n",
            "      scores[119,1]      1.75      4.72      0.41      0.00      3.39     37.64      1.06\n",
            "      scores[119,2]      0.32      0.70      0.09      0.00      0.80    228.65      1.00\n",
            "      scores[120,0]      1.32      1.85      0.70      0.00      3.33     93.16      1.00\n",
            "      scores[120,1]      1.72      4.67      0.44      0.00      4.11     35.06      1.07\n",
            "      scores[120,2]      0.32      1.01      0.05      0.00      0.81    351.20      1.01\n",
            "      scores[121,0]      1.23      1.65      0.70      0.01      2.75    112.00      1.00\n",
            "      scores[121,1]      1.73      5.22      0.38      0.00      3.56     38.52      1.06\n",
            "      scores[121,2]      0.32      0.85      0.07      0.00      0.82    410.80      1.00\n",
            "      scores[122,0]      1.43      2.13      0.79      0.02      3.38     80.82      1.00\n",
            "      scores[122,1]      1.95      7.69      0.42      0.00      3.37     67.80      1.03\n",
            "      scores[122,2]      0.38      1.49      0.07      0.00      0.82    404.80      1.00\n",
            "      scores[123,0]      1.68      2.08      1.05      0.02      3.84     72.67      1.02\n",
            "      scores[123,1]      1.71      4.59      0.49      0.00      3.76     49.72      1.05\n",
            "      scores[123,2]      0.68      1.74      0.16      0.00      1.69    205.03      1.00\n",
            "      scores[124,0]      1.31      1.82      0.68      0.01      3.11     73.88      1.00\n",
            "      scores[124,1]      1.93      6.53      0.35      0.00      3.61     38.79      1.05\n",
            "      scores[124,2]      0.34      0.98      0.07      0.00      0.79    275.11      1.00\n",
            "      scores[125,0]      1.23      2.15      0.66      0.01      2.63     73.85      1.00\n",
            "      scores[125,1]      1.92      5.67      0.46      0.00      3.68     37.29      1.06\n",
            "      scores[125,2]      0.43      1.98      0.06      0.00      0.89    280.35      1.00\n",
            "      scores[126,0]      1.18      1.48      0.72      0.00      2.48     70.46      1.00\n",
            "      scores[126,1]      1.82      6.35      0.40      0.00      3.39     45.04      1.05\n",
            "      scores[126,2]      0.36      1.04      0.06      0.00      0.77    288.56      1.00\n",
            "      scores[127,0]      1.23      1.83      0.64      0.01      2.75    125.65      1.00\n",
            "      scores[127,1]      1.72      5.65      0.34      0.00      3.32     39.16      1.05\n",
            "      scores[127,2]      0.33      0.64      0.09      0.00      0.86    226.68      1.00\n",
            "      scores[128,0]      1.36      1.74      0.82      0.00      3.17     67.61      1.01\n",
            "      scores[128,1]      2.27      7.48      0.53      0.00      4.55     53.20      1.05\n",
            "      scores[128,2]      0.37      0.99      0.07      0.00      0.95    267.50      1.00\n",
            "      scores[129,0]      1.45      1.97      0.78      0.01      3.09    120.41      1.00\n",
            "      scores[129,1]      2.34      8.01      0.55      0.00      4.16     51.62      1.05\n",
            "      scores[129,2]      0.47      1.31      0.10      0.00      1.11    293.03      1.00\n",
            "      scores[130,0]      1.40      2.04      0.78      0.00      2.97    101.15      1.00\n",
            "      scores[130,1]      1.62      4.38      0.41      0.00      3.78     37.65      1.07\n",
            "      scores[130,2]      0.33      0.78      0.06      0.00      0.96    284.10      1.00\n",
            "      scores[131,0]      1.62      2.30      0.97      0.03      3.37     96.80      1.00\n",
            "      scores[131,1]      1.78      4.40      0.51      0.00      4.40     29.71      1.08\n",
            "      scores[131,2]      0.64      2.55      0.13      0.00      1.18    282.40      1.00\n",
            "      scores[132,0]      1.28      1.73      0.70      0.01      3.08     73.80      1.00\n",
            "      scores[132,1]      1.95      7.10      0.40      0.00      3.87     49.46      1.05\n",
            "      scores[132,2]      0.34      1.48      0.06      0.00      0.70    450.46      1.00\n",
            "      scores[133,0]      1.23      1.76      0.73      0.01      2.75     77.01      1.00\n",
            "      scores[133,1]      1.70      6.27      0.41      0.00      3.89     53.67      1.04\n",
            "      scores[133,2]      0.32      0.74      0.06      0.00      0.82    272.97      1.01\n",
            "      scores[134,0]      1.15      1.46      0.72      0.01      2.46     94.79      1.00\n",
            "      scores[134,1]      1.81      6.01      0.44      0.00      3.63     40.29      1.05\n",
            "      scores[134,2]      0.29      0.74      0.07      0.00      0.72    401.69      1.00\n",
            "      scores[135,0]      1.13      1.48      0.68      0.00      2.47    136.60      1.00\n",
            "      scores[135,1]      1.60      4.62      0.42      0.00      3.16     38.73      1.05\n",
            "      scores[135,2]      0.33      0.80      0.08      0.00      0.81    405.48      1.01\n",
            "      scores[136,0]      1.35      2.00      0.70      0.00      3.16     99.14      1.00\n",
            "      scores[136,1]      1.72      5.01      0.36      0.00      4.09     31.62      1.06\n",
            "      scores[136,2]      0.32      0.78      0.07      0.00      0.83    238.11      1.01\n",
            "      scores[137,0]      1.28      1.77      0.71      0.00      3.05     73.46      1.00\n",
            "      scores[137,1]      1.72      5.08      0.40      0.00      3.86     34.19      1.07\n",
            "      scores[137,2]      0.36      1.08      0.08      0.00      0.85    340.73      1.00\n",
            "      scores[138,0]      1.45      2.00      0.88      0.00      3.01     88.73      1.00\n",
            "      scores[138,1]      2.02      6.29      0.49      0.00      4.21     61.01      1.05\n",
            "      scores[138,2]      0.32      0.73      0.06      0.00      0.80    452.53      1.00\n",
            "      scores[139,0]      1.27      1.81      0.72      0.00      3.16     73.22      1.00\n",
            "      scores[139,1]      1.53      4.17      0.39      0.00      3.28     29.96      1.07\n",
            "      scores[139,2]      0.34      1.16      0.07      0.00      0.80    497.32      1.00\n",
            "      scores[140,0]      1.40      1.96      0.81      0.01      3.23     76.14      1.00\n",
            "      scores[140,1]      2.34      7.06      0.51      0.00      4.58     35.49      1.07\n",
            "      scores[140,2]      0.36      1.18      0.06      0.00      1.06    444.79      1.00\n",
            "      scores[141,0]      1.30      1.85      0.76      0.00      2.79     93.15      1.00\n",
            "      scores[141,1]      1.73      4.73      0.46      0.00      3.75     42.36      1.06\n",
            "      scores[141,2]      0.33      0.91      0.05      0.00      0.81    233.02      1.01\n",
            "      scores[142,0]      1.29      1.59      0.74      0.00      3.20     43.10      1.00\n",
            "      scores[142,1]      2.31      5.75      0.53      0.00      5.20     23.21      1.09\n",
            "      scores[142,2]      0.37      1.33      0.05      0.00      0.88    297.18      1.00\n",
            "      scores[143,0]      1.34      1.68      0.85      0.00      2.96     75.19      1.00\n",
            "      scores[143,1]      1.79      4.49      0.48      0.00      4.55     30.83      1.07\n",
            "      scores[143,2]      0.38      0.86      0.09      0.00      1.06    266.97      1.00\n",
            "      scores[144,0]      1.44      1.89      0.80      0.02      3.39    105.16      1.00\n",
            "      scores[144,1]      2.03      5.82      0.47      0.00      3.81     44.08      1.07\n",
            "      scores[144,2]      0.35      0.71      0.10      0.00      0.96    325.20      1.00\n",
            "      scores[145,0]      1.24      1.59      0.71      0.01      2.66     40.99      1.01\n",
            "      scores[145,1]      2.57      8.04      0.55      0.00      5.31     45.37      1.06\n",
            "      scores[145,2]      0.39      1.18      0.06      0.00      0.90    292.38      1.00\n",
            "      scores[146,0]      1.46      2.02      0.80      0.02      3.33     80.39      1.00\n",
            "      scores[146,1]      2.08      6.36      0.56      0.00      4.02     41.08      1.06\n",
            "      scores[146,2]      0.31      0.92      0.06      0.00      0.78    321.47      1.00\n",
            "      scores[147,0]      1.25      1.68      0.69      0.00      2.90     82.48      1.00\n",
            "      scores[147,1]      1.84      5.99      0.42      0.00      3.56     42.78      1.05\n",
            "      scores[147,2]      0.32      0.70      0.08      0.00      0.87    235.89      1.01\n",
            "      scores[148,0]      1.37      1.95      0.76      0.00      3.03     75.64      1.00\n",
            "      scores[148,1]      2.03      5.53      0.52      0.00      3.97     34.88      1.07\n",
            "      scores[148,2]      0.40      1.16      0.08      0.00      0.96    475.59      1.00\n",
            "      scores[149,0]      1.21      1.50      0.68      0.01      2.88     57.10      1.00\n",
            "      scores[149,1]      1.72      6.36      0.39      0.00      3.22     57.07      1.04\n",
            "      scores[149,2]      0.30      0.68      0.07      0.00      0.83    131.46      1.01\n",
            "      scores[150,0]      1.63      2.38      0.86      0.00      3.95     80.40      1.01\n",
            "      scores[150,1]      1.58      4.06      0.54      0.00      3.48     42.64      1.06\n",
            "      scores[150,2]      0.51      1.22      0.12      0.00      1.21    241.83      1.01\n",
            "      scores[151,0]      1.25      1.58      0.71      0.00      2.92     64.75      1.00\n",
            "      scores[151,1]      1.60      4.45      0.40      0.00      3.68     28.95      1.07\n",
            "      scores[151,2]      0.36      0.76      0.08      0.00      0.99    349.46      1.00\n",
            "      scores[152,0]      1.38      2.65      0.70      0.02      3.06    108.18      1.00\n",
            "      scores[152,1]      1.75      6.08      0.41      0.00      3.52     44.92      1.04\n",
            "      scores[152,2]      0.37      1.71      0.06      0.00      0.70    422.83      1.00\n",
            "      scores[153,0]      1.54      2.27      0.82      0.01      3.57     79.67      1.00\n",
            "      scores[153,1]      2.20      7.12      0.48      0.00      4.67     45.22      1.05\n",
            "      scores[153,2]      0.43      1.09      0.09      0.00      1.11    255.93      1.00\n",
            "      scores[154,0]      1.51      2.44      0.74      0.01      3.48     97.86      1.00\n",
            "      scores[154,1]      2.06      6.63      0.45      0.00      3.68     43.27      1.06\n",
            "      scores[154,2]      0.45      1.31      0.10      0.00      1.13    342.24      1.00\n",
            "      scores[155,0]      1.20      1.58      0.71      0.00      2.69    128.02      1.00\n",
            "      scores[155,1]      1.49      3.64      0.44      0.00      3.16     27.83      1.08\n",
            "      scores[155,2]      0.32      0.89      0.05      0.00      0.79    285.12      1.00\n",
            "      scores[156,0]      1.26      1.68      0.79      0.00      2.50     92.30      1.00\n",
            "      scores[156,1]      1.93      6.02      0.42      0.00      3.90     37.31      1.06\n",
            "      scores[156,2]      0.31      0.84      0.06      0.00      0.74    353.97      1.00\n",
            "      scores[157,0]      1.33      1.89      0.77      0.00      3.12     79.11      1.00\n",
            "      scores[157,1]      2.17      5.90      0.47      0.00      4.97     31.09      1.07\n",
            "      scores[157,2]      0.34      0.97      0.06      0.00      0.79    433.74      1.00\n",
            "      scores[158,0]      1.30      1.83      0.71      0.00      2.87     93.81      1.00\n",
            "      scores[158,1]      2.05      7.21      0.35      0.00      4.28     54.42      1.05\n",
            "      scores[158,2]      0.27      0.59      0.07      0.00      0.70    319.49      1.00\n",
            "      scores[159,0]      1.20      1.72      0.69      0.00      2.72     80.99      1.00\n",
            "      scores[159,1]      1.82      5.73      0.42      0.00      3.39     51.58      1.05\n",
            "      scores[159,2]      0.39      1.04      0.08      0.00      0.88    225.55      1.00\n",
            "      scores[160,0]      1.42      1.78      0.83      0.00      3.33     90.16      1.00\n",
            "      scores[160,1]      2.09      7.55      0.46      0.00      4.11     59.63      1.04\n",
            "      scores[160,2]      0.39      0.76      0.09      0.00      1.11    232.14      1.00\n",
            "      scores[161,0]      1.48      2.07      0.85      0.01      3.31    105.32      1.00\n",
            "      scores[161,1]      1.67      3.95      0.44      0.00      3.69     33.98      1.07\n",
            "      scores[161,2]      0.44      1.27      0.10      0.00      0.96    464.26      1.00\n",
            "      scores[162,0]      1.33      1.87      0.82      0.00      3.00     81.50      1.00\n",
            "      scores[162,1]      1.98      7.50      0.39      0.00      3.81     56.23      1.04\n",
            "      scores[162,2]      0.36      1.37      0.06      0.00      0.84    390.26      1.00\n",
            "      scores[163,0]      1.34      2.16      0.69      0.00      3.01     88.11      1.00\n",
            "      scores[163,1]      1.65      4.74      0.33      0.00      3.72     44.24      1.06\n",
            "      scores[163,2]      0.31      0.62      0.10      0.00      0.80    276.11      1.00\n",
            "      scores[164,0]      1.26      1.66      0.73      0.00      2.88     68.30      1.00\n",
            "      scores[164,1]      1.98      5.50      0.42      0.00      4.15     30.89      1.08\n",
            "      scores[164,2]      0.32      0.95      0.07      0.00      0.83    307.92      1.01\n",
            "      scores[165,0]      1.28      1.88      0.71      0.00      2.87     79.99      1.00\n",
            "      scores[165,1]      1.65      4.28      0.39      0.00      4.11     33.42      1.08\n",
            "      scores[165,2]      0.35      1.05      0.08      0.00      0.80    331.05      1.01\n",
            "      scores[166,0]      1.46      1.92      0.84      0.00      3.36     62.09      1.00\n",
            "      scores[166,1]      2.80      6.79      0.67      0.00      6.85     24.48      1.10\n",
            "      scores[166,2]      0.40      1.48      0.07      0.00      0.97    352.22      1.00\n",
            "      scores[167,0]      1.96      2.85      1.24      0.04      4.35    104.36      1.01\n",
            "      scores[167,1]      1.95      4.34      0.66      0.00      4.70     26.83      1.09\n",
            "      scores[167,2]      0.73      1.64      0.20      0.00      1.82    222.57      1.00\n",
            "      scores[168,0]      1.28      2.11      0.67      0.00      2.68     65.88      1.00\n",
            "      scores[168,1]      1.65      4.12      0.36      0.00      3.42     35.30      1.09\n",
            "      scores[168,2]      0.29      0.72      0.07      0.00      0.75    364.72      1.00\n",
            "      scores[169,0]      1.48      1.88      0.85      0.00      3.30     60.62      1.01\n",
            "      scores[169,1]      2.84      9.55      0.57      0.00      6.03     41.73      1.05\n",
            "      scores[169,2]      0.33      0.90      0.07      0.00      0.85    265.48      1.00\n",
            "      scores[170,0]      1.58      2.18      0.92      0.01      3.37     86.56      1.00\n",
            "      scores[170,1]      1.97      5.22      0.50      0.00      4.27     36.20      1.07\n",
            "      scores[170,2]      0.56      1.47      0.10      0.00      1.23    241.76      1.00\n",
            "      scores[171,0]      1.26      1.85      0.71      0.01      2.93     98.56      1.00\n",
            "      scores[171,1]      1.81      4.55      0.42      0.00      4.25     31.29      1.09\n",
            "      scores[171,2]      0.34      1.02      0.07      0.00      0.82    413.78      1.00\n",
            "      scores[172,0]      1.48      2.11      0.83      0.01      3.27    101.13      1.00\n",
            "      scores[172,1]      1.97      5.21      0.49      0.00      3.62     33.47      1.08\n",
            "      scores[172,2]      0.45      2.04      0.08      0.00      0.98    403.05      1.00\n",
            "      scores[173,0]      1.41      1.77      0.86      0.00      3.19     65.01      1.00\n",
            "      scores[173,1]      2.60      8.14      0.57      0.00      5.53     47.30      1.05\n",
            "      scores[173,2]      0.39      1.78      0.07      0.00      0.77    421.29      1.00\n",
            "      scores[174,0]      1.23      1.98      0.68      0.00      2.70    174.69      1.00\n",
            "      scores[174,1]      1.48      4.23      0.40      0.00      3.16     38.17      1.06\n",
            "      scores[174,2]      0.32      0.64      0.06      0.00      0.92    253.34      1.00\n",
            "      scores[175,0]      1.25      1.66      0.80      0.00      2.58    114.46      1.00\n",
            "      scores[175,1]      1.68      4.69      0.41      0.00      3.27     34.48      1.07\n",
            "      scores[175,2]      0.35      1.52      0.07      0.00      0.78    381.03      1.00\n",
            "      scores[176,0]      1.54      2.07      0.87      0.01      3.48    113.50      1.00\n",
            "      scores[176,1]      1.63      3.62      0.44      0.00      3.97     23.84      1.09\n",
            "      scores[176,2]      0.48      1.10      0.10      0.00      1.16    247.54      1.00\n",
            "      scores[177,0]      1.39      1.84      0.78      0.00      3.45     69.80      1.00\n",
            "      scores[177,1]      1.98      4.48      0.49      0.00      4.74     21.81      1.10\n",
            "      scores[177,2]      0.33      1.09      0.06      0.00      0.78    233.69      1.01\n",
            "      scores[178,0]      1.22      1.77      0.75      0.01      2.65     92.93      1.00\n",
            "      scores[178,1]      1.80      4.79      0.42      0.00      3.97     39.09      1.07\n",
            "      scores[178,2]      0.39      1.18      0.07      0.00      0.86    379.36      1.00\n",
            "      scores[179,0]      1.38      1.80      0.85      0.00      3.13    106.15      1.00\n",
            "      scores[179,1]      2.42      6.68      0.54      0.00      5.62     28.26      1.07\n",
            "      scores[179,2]      0.35      0.93      0.07      0.00      0.82    354.53      1.00\n",
            "      scores[180,0]      1.32      1.81      0.76      0.00      2.90     77.28      1.00\n",
            "      scores[180,1]      1.83      4.80      0.44      0.00      4.06     29.66      1.08\n",
            "      scores[180,2]      0.37      0.86      0.06      0.00      1.01    180.24      1.01\n",
            "      scores[181,0]      1.38      2.12      0.71      0.00      3.34     83.61      1.00\n",
            "      scores[181,1]      1.77      6.56      0.37      0.00      3.71     57.02      1.04\n",
            "      scores[181,2]      0.40      1.00      0.07      0.00      0.95    329.07      1.01\n",
            "      scores[182,0]      1.32      2.02      0.72      0.01      2.89    134.81      1.00\n",
            "      scores[182,1]      1.62      3.83      0.46      0.00      3.78     22.36      1.10\n",
            "      scores[182,2]      0.28      0.62      0.10      0.00      0.75    400.77      1.00\n",
            "      scores[183,0]      1.16      1.48      0.69      0.02      2.79     92.39      1.00\n",
            "      scores[183,1]      1.60      4.57      0.37      0.00      3.53     37.75      1.07\n",
            "      scores[183,2]      0.36      1.21      0.06      0.00      0.85    343.02      1.00\n",
            "      scores[184,0]      1.35      1.85      0.74      0.00      3.19     98.04      1.00\n",
            "      scores[184,1]      1.67      5.56      0.41      0.00      3.85     47.87      1.05\n",
            "      scores[184,2]      0.41      1.75      0.09      0.00      0.89    461.45      1.00\n",
            "      scores[185,0]      1.30      1.88      0.74      0.01      2.98     73.96      1.00\n",
            "      scores[185,1]      1.78      5.38      0.37      0.00      3.63     38.26      1.06\n",
            "      scores[185,2]      0.38      0.96      0.07      0.00      0.91    344.60      1.00\n",
            "      scores[186,0]      1.24      1.68      0.73      0.01      2.86    111.01      1.00\n",
            "      scores[186,1]      1.68      4.33      0.42      0.00      3.60     28.07      1.09\n",
            "      scores[186,2]      0.29      0.51      0.10      0.00      0.82    229.44      1.00\n",
            "      scores[187,0]      1.18      1.51      0.74      0.00      2.83    122.01      1.00\n",
            "      scores[187,1]      1.61      4.63      0.37      0.00      3.61     37.76      1.06\n",
            "      scores[187,2]      0.35      1.27      0.07      0.00      0.74    254.71      1.00\n",
            "      scores[188,0]      1.23      1.92      0.66      0.01      2.76     94.34      1.00\n",
            "      scores[188,1]      1.62      4.51      0.36      0.00      3.60     36.84      1.07\n",
            "      scores[188,2]      0.35      1.07      0.08      0.00      0.75    474.75      1.00\n",
            "      scores[189,0]      1.28      1.81      0.74      0.00      2.90     66.71      1.00\n",
            "      scores[189,1]      1.76      5.40      0.34      0.00      3.67     38.72      1.06\n",
            "      scores[189,2]      0.30      0.65      0.07      0.00      0.78    208.61      1.01\n",
            "      scores[190,0]      1.18      1.33      0.76      0.00      2.66     85.39      1.00\n",
            "      scores[190,1]      1.75      5.33      0.37      0.00      3.20     36.38      1.06\n",
            "      scores[190,2]      0.33      0.86      0.07      0.00      0.99    319.07      1.00\n",
            "      scores[191,0]      1.27      1.67      0.69      0.01      3.13     72.58      1.00\n",
            "      scores[191,1]      1.64      5.60      0.36      0.00      3.89     44.61      1.05\n",
            "      scores[191,2]      0.39      1.29      0.08      0.00      0.97    271.41      1.00\n",
            "      scores[192,0]      1.30      2.05      0.66      0.00      3.12     91.38      1.00\n",
            "      scores[192,1]      1.64      3.94      0.43      0.00      3.98     29.42      1.08\n",
            "      scores[192,2]      0.33      1.14      0.05      0.00      0.71    431.54      1.00\n",
            "      scores[193,0]      1.22      1.57      0.71      0.00      2.90     74.57      1.00\n",
            "      scores[193,1]      1.62      4.11      0.45      0.00      3.88     28.98      1.07\n",
            "      scores[193,2]      0.39      1.00      0.09      0.00      0.91    325.90      1.00\n",
            "      scores[194,0]      1.27      1.63      0.72      0.00      2.90     73.46      1.00\n",
            "      scores[194,1]      1.80      4.93      0.44      0.00      3.95     34.24      1.07\n",
            "      scores[194,2]      0.42      1.56      0.08      0.00      0.81    294.25      1.00\n",
            "      scores[195,0]      1.25      2.18      0.68      0.00      2.75    121.56      1.00\n",
            "      scores[195,1]      1.92      6.71      0.42      0.00      3.89     50.42      1.05\n",
            "      scores[195,2]      0.34      0.75      0.08      0.00      0.89    180.21      1.00\n",
            "      scores[196,0]      1.26      1.77      0.68      0.00      2.79     76.69      1.00\n",
            "      scores[196,1]      1.66      4.79      0.45      0.00      3.14     49.18      1.06\n",
            "      scores[196,2]      0.33      0.69      0.08      0.00      0.87    424.15      1.01\n",
            "      scores[197,0]      1.22      1.62      0.71      0.00      2.73     75.61      1.00\n",
            "      scores[197,1]      1.96      7.73      0.44      0.00      3.81     55.43      1.03\n",
            "      scores[197,2]      0.33      1.66      0.07      0.00      0.76    503.98      1.00\n",
            "      scores[198,0]      1.27      1.75      0.73      0.00      2.81    106.67      1.00\n",
            "      scores[198,1]      1.72      5.40      0.49      0.00      3.53     45.16      1.05\n",
            "      scores[198,2]      0.36      1.07      0.07      0.00      0.83    288.80      1.00\n",
            "      scores[199,0]      1.27      1.85      0.64      0.00      2.81     58.85      1.00\n",
            "      scores[199,1]      1.89      5.31      0.40      0.00      3.24     28.42      1.07\n",
            "      scores[199,2]      0.32      0.85      0.05      0.00      0.81    258.46      1.00\n",
            "      scores[200,0]      1.31      1.79      0.77      0.01      2.94     68.49      1.00\n",
            "      scores[200,1]      1.85      6.39      0.41      0.00      3.35     56.33      1.05\n",
            "      scores[200,2]      0.38      1.10      0.08      0.00      0.84    361.02      1.00\n",
            "      scores[201,0]      1.25      2.17      0.71      0.00      2.78     94.49      1.00\n",
            "      scores[201,1]      1.72      4.62      0.42      0.00      3.84     33.63      1.07\n",
            "      scores[201,2]      0.36      1.29      0.06      0.00      0.81    372.18      1.00\n",
            "      scores[202,0]      1.22      1.53      0.69      0.00      3.05     81.49      1.00\n",
            "      scores[202,1]      1.87      5.72      0.37      0.00      3.89     41.33      1.06\n",
            "      scores[202,2]      0.39      1.30      0.06      0.00      0.85    472.22      1.01\n",
            "      scores[203,0]      1.23      1.71      0.67      0.01      2.87     88.02      1.00\n",
            "      scores[203,1]      1.70      4.84      0.45      0.00      3.94     35.70      1.06\n",
            "      scores[203,2]      0.34      0.73      0.08      0.00      0.81    228.15      1.01\n",
            "      scores[204,0]      1.29      1.95      0.74      0.00      2.61     80.52      1.00\n",
            "      scores[204,1]      1.78      7.00      0.36      0.00      3.11     58.98      1.04\n",
            "      scores[204,2]      0.31      0.76      0.06      0.00      0.88    433.66      1.00\n",
            "      scores[205,0]      1.23      1.64      0.68      0.01      2.86     80.89      1.00\n",
            "      scores[205,1]      1.97      6.87      0.38      0.00      3.96     51.28      1.05\n",
            "      scores[205,2]      0.33      0.91      0.06      0.00      0.76    502.66      1.00\n",
            "      scores[206,0]      1.20      1.52      0.74      0.01      2.66     89.60      1.01\n",
            "      scores[206,1]      1.72      5.63      0.39      0.00      3.42     59.86      1.05\n",
            "      scores[206,2]      0.34      0.79      0.07      0.00      0.87    345.00      1.00\n",
            "      scores[207,0]      1.23      1.57      0.67      0.01      2.87     96.29      1.00\n",
            "      scores[207,1]      1.86      5.17      0.40      0.00      4.22     37.20      1.07\n",
            "      scores[207,2]      0.30      0.75      0.07      0.00      0.75    361.35      1.00\n",
            "      scores[208,0]      1.33      1.88      0.73      0.00      3.23     71.50      1.00\n",
            "      scores[208,1]      1.92      5.18      0.49      0.00      3.78     36.91      1.07\n",
            "      scores[208,2]      0.34      1.24      0.08      0.00      0.75    334.32      1.00\n",
            "      scores[209,0]      1.30      1.98      0.72      0.01      2.77     89.48      1.00\n",
            "      scores[209,1]      2.02      8.00      0.39      0.00      3.98     66.92      1.04\n",
            "      scores[209,2]      0.40      1.31      0.08      0.00      0.92    340.32      1.01\n",
            "      scores[210,0]      1.24      1.51      0.71      0.00      3.03     81.46      1.00\n",
            "      scores[210,1]      1.94      6.07      0.38      0.00      3.95     34.75      1.06\n",
            "      scores[210,2]      0.28      0.98      0.05      0.00      0.76    348.12      1.01\n",
            "      scores[211,0]      1.22      1.77      0.74      0.02      2.72    103.60      1.00\n",
            "      scores[211,1]      1.89      5.54      0.44      0.00      4.04     33.54      1.07\n",
            "      scores[211,2]      0.35      0.94      0.07      0.00      0.85    239.71      1.00\n",
            "      scores[212,0]      1.24      1.57      0.64      0.01      2.88     57.74      1.00\n",
            "      scores[212,1]      2.22      8.15      0.41      0.00      4.07     53.01      1.04\n",
            "      scores[212,2]      0.33      0.94      0.06      0.00      0.85    268.22      1.00\n",
            "      scores[213,0]      1.16      1.53      0.66      0.01      2.67     66.16      1.00\n",
            "      scores[213,1]      1.76      6.00      0.39      0.00      3.50     43.47      1.05\n",
            "      scores[213,2]      0.29      0.66      0.06      0.00      0.69    228.16      1.00\n",
            "      scores[214,0]      1.31      1.64      0.70      0.00      3.06     63.67      1.00\n",
            "      scores[214,1]      1.78      4.98      0.42      0.00      3.85     36.38      1.07\n",
            "      scores[214,2]      0.36      1.54      0.07      0.00      0.77    393.81      1.00\n",
            "      scores[215,0]      1.22      1.85      0.62      0.00      2.69     71.64      1.00\n",
            "      scores[215,1]      1.71      5.63      0.42      0.00      3.40     45.01      1.05\n",
            "      scores[215,2]      0.34      1.34      0.06      0.00      0.79    375.93      1.00\n",
            "      scores[216,0]      1.26      1.79      0.69      0.00      3.03    132.13      1.00\n",
            "      scores[216,1]      1.88      5.41      0.45      0.00      3.83     31.35      1.07\n",
            "      scores[216,2]      0.34      0.91      0.06      0.00      0.78    362.12      1.00\n",
            "      scores[217,0]      1.22      1.60      0.76      0.00      2.98     81.08      1.00\n",
            "      scores[217,1]      1.84      6.39      0.42      0.00      3.92     52.13      1.05\n",
            "      scores[217,2]      0.36      0.96      0.06      0.00      0.96    291.87      1.01\n",
            "      scores[218,0]      1.26      1.74      0.65      0.00      3.03     94.01      1.00\n",
            "      scores[218,1]      1.94      7.30      0.40      0.00      3.16     55.42      1.04\n",
            "      scores[218,2]      0.29      0.69      0.07      0.00      0.76    385.16      1.01\n",
            "      scores[219,0]      1.25      1.96      0.64      0.01      2.62     65.96      1.00\n",
            "      scores[219,1]      2.06      8.30      0.43      0.00      3.75     67.78      1.04\n",
            "      scores[219,2]      0.37      1.14      0.06      0.00      0.83    300.27      1.00\n",
            "      scores[220,0]      1.24      1.55      0.67      0.01      3.22     53.12      1.01\n",
            "      scores[220,1]      2.00      6.55      0.38      0.00      3.91     48.86      1.05\n",
            "      scores[220,2]      0.29      0.65      0.06      0.00      0.77    225.44      1.00\n",
            "      scores[221,0]      1.29      1.95      0.70      0.00      2.87     86.93      1.00\n",
            "      scores[221,1]      1.72      5.13      0.39      0.00      3.67     36.71      1.06\n",
            "      scores[221,2]      0.37      1.14      0.07      0.00      0.81    314.14      1.00\n",
            "      scores[222,0]      1.23      1.72      0.73      0.00      2.58     88.68      1.00\n",
            "      scores[222,1]      2.07      7.34      0.43      0.00      3.50     46.95      1.05\n",
            "      scores[222,2]      0.44      1.91      0.06      0.00      0.84    315.53      1.00\n",
            "      scores[223,0]      1.32      1.81      0.69      0.00      3.41    121.39      1.00\n",
            "      scores[223,1]      1.79      5.43      0.41      0.00      3.72     37.08      1.07\n",
            "      scores[223,2]      0.34      0.88      0.08      0.00      0.87    162.85      1.00\n",
            "      scores[224,0]      1.23      1.50      0.70      0.00      3.01     82.09      1.00\n",
            "      scores[224,1]      1.65      5.63      0.40      0.00      3.37     49.46      1.04\n",
            "      scores[224,2]      0.33      0.78      0.08      0.00      0.83    257.06      1.01\n",
            "      scores[225,0]      1.22      1.49      0.75      0.00      2.89     66.66      1.00\n",
            "      scores[225,1]      1.66      5.07      0.42      0.00      3.45     32.73      1.06\n",
            "      scores[225,2]      0.27      0.61      0.04      0.00      0.67    166.77      1.00\n",
            "      scores[226,0]      1.25      1.93      0.68      0.01      2.66     89.65      1.00\n",
            "      scores[226,1]      1.84      5.85      0.45      0.00      3.69     42.46      1.06\n",
            "      scores[226,2]      0.30      0.61      0.08      0.00      0.72    293.23      1.00\n",
            "      scores[227,0]      1.32      2.49      0.71      0.00      2.99    155.30      1.00\n",
            "      scores[227,1]      1.92      5.95      0.48      0.00      3.76     42.47      1.06\n",
            "      scores[227,2]      0.30      0.71      0.06      0.00      0.90    194.67      1.01\n",
            "      scores[228,0]      1.29      1.97      0.76      0.00      2.69     94.24      1.00\n",
            "      scores[228,1]      1.74      4.70      0.46      0.00      3.52     35.01      1.07\n",
            "      scores[228,2]      0.31      0.79      0.08      0.00      0.82    289.73      1.00\n",
            "      scores[229,0]      1.21      1.48      0.75      0.00      2.80     68.24      1.00\n",
            "      scores[229,1]      1.60      4.97      0.40      0.00      3.06     48.58      1.06\n",
            "      scores[229,2]      0.42      1.36      0.07      0.00      0.81    330.55      1.00\n",
            "      scores[230,0]      1.22      1.57      0.73      0.00      2.62     72.95      1.00\n",
            "      scores[230,1]      1.56      4.14      0.45      0.00      3.43     34.87      1.06\n",
            "      scores[230,2]      0.37      0.97      0.08      0.00      0.85    410.68      1.00\n",
            "      scores[231,0]      1.19      1.44      0.73      0.00      2.77     86.03      1.00\n",
            "      scores[231,1]      1.80      4.67      0.39      0.00      3.66     26.09      1.09\n",
            "      scores[231,2]      0.34      1.16      0.07      0.00      0.77    396.77      1.00\n",
            "      scores[232,0]      1.20      1.73      0.62      0.00      2.73    100.52      1.00\n",
            "      scores[232,1]      1.68      4.47      0.37      0.00      3.86     28.83      1.07\n",
            "      scores[232,2]      0.30      0.58      0.07      0.00      0.86    143.55      1.00\n",
            "      scores[233,0]      1.30      1.87      0.73      0.01      2.81     98.15      1.00\n",
            "      scores[233,1]      1.75      5.91      0.43      0.00      3.25     52.20      1.05\n",
            "      scores[233,2]      0.33      0.91      0.07      0.00      0.72    246.52      1.00\n",
            "      scores[234,0]      1.28      2.07      0.75      0.00      2.80    112.84      1.00\n",
            "      scores[234,1]      1.55      3.92      0.41      0.00      3.79     27.57      1.08\n",
            "      scores[234,2]      0.32      0.70      0.06      0.00      0.90    230.49      1.00\n",
            "      scores[235,0]      1.32      2.98      0.71      0.01      2.92    193.70      1.00\n",
            "      scores[235,1]      1.72      4.94      0.39      0.00      3.69     34.26      1.06\n",
            "      scores[235,2]      0.35      0.79      0.08      0.00      1.00    217.76      1.01\n",
            "      scores[236,0]      1.31      2.12      0.70      0.01      3.20    123.99      1.00\n",
            "      scores[236,1]      1.75      4.98      0.40      0.00      4.25     32.25      1.06\n",
            "      scores[236,2]      0.35      0.93      0.06      0.00      0.78    350.53      1.00\n",
            "      scores[237,0]      1.34      2.25      0.79      0.00      2.93    130.96      1.00\n",
            "      scores[237,1]      2.11      6.29      0.40      0.00      4.31     35.75      1.07\n",
            "      scores[237,2]      0.40      1.39      0.07      0.00      0.91    396.01      1.00\n",
            "      scores[238,0]      1.33      1.69      0.77      0.00      3.14     70.91      1.00\n",
            "      scores[238,1]      1.63      4.68      0.37      0.00      3.40     31.59      1.07\n",
            "      scores[238,2]      0.33      0.71      0.05      0.00      0.81    218.69      1.00\n",
            "      scores[239,0]      1.17      1.44      0.66      0.02      2.88     86.05      1.00\n",
            "      scores[239,1]      1.80      5.17      0.46      0.00      3.99     45.82      1.06\n",
            "      scores[239,2]      0.36      1.10      0.08      0.00      1.01    318.71      1.00\n",
            "      scores[240,0]      1.34      1.99      0.75      0.01      3.13     76.32      1.00\n",
            "      scores[240,1]      1.62      4.35      0.37      0.00      3.61     33.27      1.07\n",
            "      scores[240,2]      0.31      0.81      0.07      0.00      0.79    373.52      1.00\n",
            "      scores[241,0]      1.15      1.62      0.66      0.00      2.69     70.66      1.00\n",
            "      scores[241,1]      1.73      4.87      0.49      0.00      3.81     33.85      1.06\n",
            "      scores[241,2]      0.34      0.77      0.08      0.00      0.81    237.23      1.00\n",
            "      scores[242,0]      1.32      1.82      0.70      0.00      3.06     95.14      1.00\n",
            "      scores[242,1]      1.74      4.67      0.42      0.00      3.90     30.01      1.08\n",
            "      scores[242,2]      0.30      0.80      0.05      0.00      0.70    227.06      1.00\n",
            "      scores[243,0]      1.20      1.57      0.70      0.00      2.70     52.18      1.00\n",
            "      scores[243,1]      2.07      7.03      0.46      0.00      4.01     50.61      1.05\n",
            "      scores[243,2]      0.33      1.29      0.04      0.00      0.66    378.46      1.00\n",
            "      scores[244,0]      1.37      1.94      0.79      0.01      3.36     71.10      1.00\n",
            "      scores[244,1]      1.90      6.02      0.44      0.00      3.67     42.71      1.06\n",
            "      scores[244,2]      0.34      1.05      0.06      0.00      0.72    392.59      1.00\n",
            "      scores[245,0]      1.32      2.00      0.70      0.00      2.96     75.33      1.00\n",
            "      scores[245,1]      1.81      7.05      0.38      0.00      3.63     50.99      1.04\n",
            "      scores[245,2]      0.36      1.65      0.08      0.00      0.77    463.11      1.00\n",
            "      scores[246,0]      1.22      1.87      0.62      0.02      2.99    150.53      1.00\n",
            "      scores[246,1]      1.40      3.88      0.41      0.00      2.96     37.15      1.06\n",
            "      scores[246,2]      0.24      0.64      0.06      0.00      0.56    475.31      1.01\n",
            "      scores[247,0]      1.32      1.67      0.74      0.01      3.12     96.55      1.00\n",
            "      scores[247,1]      1.53      4.17      0.36      0.00      3.28     33.53      1.07\n",
            "      scores[247,2]      0.33      0.70      0.08      0.00      0.95    173.52      1.01\n",
            "      scores[248,0]      1.45      2.12      0.73      0.01      3.64     78.43      1.00\n",
            "      scores[248,1]      1.76      6.64      0.38      0.00      3.15     71.82      1.04\n",
            "      scores[248,2]      0.30      0.68      0.06      0.00      0.76    149.98      1.01\n",
            "      scores[249,0]      1.29      1.85      0.70      0.02      3.02     77.82      1.00\n",
            "      scores[249,1]      1.95      6.44      0.39      0.00      3.74     47.38      1.05\n",
            "      scores[249,2]      0.34      0.97      0.05      0.00      0.91    484.60      1.00\n",
            "      scores[250,0]      1.23      1.59      0.69      0.01      2.93     85.49      1.00\n",
            "      scores[250,1]      1.95      5.63      0.45      0.00      4.28     39.04      1.07\n",
            "      scores[250,2]      0.28      0.55      0.07      0.00      0.74    241.78      1.01\n",
            "      scores[251,0]      1.20      1.40      0.80      0.01      2.69    113.75      1.00\n",
            "      scores[251,1]      1.69      5.37      0.40      0.00      3.14     41.99      1.06\n",
            "      scores[251,2]      0.30      0.83      0.06      0.00      0.79    239.10      1.01\n",
            "      scores[252,0]      1.22      1.61      0.66      0.01      2.75     71.58      1.00\n",
            "      scores[252,1]      1.66      4.01      0.37      0.00      4.07     20.43      1.10\n",
            "      scores[252,2]      0.31      0.69      0.07      0.00      0.86    325.12      1.00\n",
            "      scores[253,0]      1.29      1.92      0.69      0.01      2.90     91.43      1.00\n",
            "      scores[253,1]      1.81      5.24      0.44      0.00      3.61     36.21      1.05\n",
            "      scores[253,2]      0.27      0.53      0.06      0.00      0.79    164.06      1.01\n",
            "      scores[254,0]      1.32      1.83      0.73      0.00      3.13     83.67      1.00\n",
            "      scores[254,1]      2.01      7.13      0.45      0.00      4.21     60.24      1.04\n",
            "      scores[254,2]      0.38      1.27      0.06      0.00      0.85    424.17      1.00\n",
            "      scores[255,0]      1.31      1.76      0.73      0.00      3.12     78.59      1.00\n",
            "      scores[255,1]      1.92      6.39      0.40      0.00      4.60     47.10      1.05\n",
            "      scores[255,2]      0.36      1.09      0.06      0.00      0.86    308.22      1.00\n",
            "      scores[256,0]      1.27      1.68      0.76      0.00      2.65     74.59      1.00\n",
            "      scores[256,1]      1.67      3.76      0.45      0.00      4.54     21.08      1.11\n",
            "      scores[256,2]      0.34      1.06      0.08      0.00      0.79    363.41      1.00\n",
            "      scores[257,0]      1.23      1.79      0.67      0.00      2.72     86.50      1.00\n",
            "      scores[257,1]      1.54      3.79      0.32      0.00      3.54     24.81      1.09\n",
            "      scores[257,2]      0.31      0.76      0.06      0.00      0.77    209.81      1.01\n",
            "      scores[258,0]      1.36      1.77      0.78      0.00      3.20     71.34      1.00\n",
            "      scores[258,1]      2.02      5.70      0.39      0.00      4.69     30.19      1.07\n",
            "      scores[258,2]      0.34      0.83      0.04      0.00      1.11    251.59      1.00\n",
            "      scores[259,0]      1.29      2.11      0.71      0.01      2.76    119.10      1.00\n",
            "      scores[259,1]      1.87      5.41      0.41      0.00      3.84     34.72      1.07\n",
            "      scores[259,2]      0.48      2.60      0.06      0.00      0.90    391.89      1.00\n",
            "      scores[260,0]      1.23      1.80      0.69      0.00      2.64     87.24      1.00\n",
            "      scores[260,1]      1.88      7.63      0.40      0.00      3.67     59.75      1.04\n",
            "      scores[260,2]      0.33      1.02      0.06      0.00      0.87    370.07      1.00\n",
            "      scores[261,0]      1.38      2.50      0.72      0.00      2.80    120.86      1.00\n",
            "      scores[261,1]      1.90      6.22      0.43      0.00      3.77     45.72      1.05\n",
            "      scores[261,2]      0.38      1.15      0.07      0.00      0.83    317.89      1.00\n",
            "      scores[262,0]      1.26      1.78      0.68      0.00      3.15     85.88      1.00\n",
            "      scores[262,1]      1.75      4.81      0.41      0.00      3.39     41.24      1.07\n",
            "      scores[262,2]      0.30      0.94      0.05      0.00      0.64    281.62      1.00\n",
            "      scores[263,0]      1.25      1.54      0.79      0.00      2.76     88.95      1.00\n",
            "      scores[263,1]      1.70      5.19      0.43      0.00      3.23     48.02      1.06\n",
            "      scores[263,2]      0.40      0.95      0.08      0.00      1.04    267.32      1.00\n",
            "      scores[264,0]      1.35      2.19      0.70      0.01      2.95     73.75      1.00\n",
            "      scores[264,1]      1.73      4.97      0.38      0.00      3.50     31.22      1.07\n",
            "      scores[264,2]      0.35      1.48      0.07      0.00      0.77    502.94      1.00\n",
            "      scores[265,0]      1.28      1.82      0.67      0.00      3.14    119.45      1.00\n",
            "      scores[265,1]      1.71      4.74      0.44      0.00      3.60     33.56      1.07\n",
            "      scores[265,2]      0.30      0.75      0.07      0.00      0.72    424.27      1.00\n",
            "      scores[266,0]      1.35      1.96      0.74      0.00      2.85     67.08      1.00\n",
            "      scores[266,1]      1.73      4.68      0.44      0.00      3.51     46.16      1.07\n",
            "      scores[266,2]      0.36      0.85      0.07      0.00      0.89    183.91      1.01\n",
            "      scores[267,0]      1.17      1.63      0.64      0.01      2.77     89.82      1.00\n",
            "      scores[267,1]      1.61      4.11      0.41      0.00      4.04     29.63      1.08\n",
            "      scores[267,2]      0.28      0.62      0.06      0.00      0.83    222.95      1.00\n",
            "      scores[268,0]      1.40      2.62      0.76      0.01      3.20    155.78      1.00\n",
            "      scores[268,1]      1.88      6.69      0.43      0.00      3.80     65.65      1.04\n",
            "      scores[268,2]      0.26      0.75      0.05      0.00      0.64    298.74      1.01\n",
            "      scores[269,0]      1.26      1.54      0.71      0.00      3.04     74.98      1.00\n",
            "      scores[269,1]      1.92      6.55      0.45      0.00      3.55     48.14      1.04\n",
            "      scores[269,2]      0.34      0.85      0.07      0.00      0.88    260.41      1.00\n",
            "  scores_prime[0,0]      1.32      1.73      0.63      0.00      3.45     49.44      1.03\n",
            "  scores_prime[0,1]      1.38      1.91      0.76      0.00      3.13     77.85      1.00\n",
            "  scores_prime[0,2]      1.18      2.62      0.44      0.00      3.19     49.92      1.04\n",
            "  scores_prime[1,0]      1.90      2.50      0.92      0.00      4.92     32.67      1.08\n",
            "  scores_prime[1,1]      2.67      3.95      1.34      0.01      6.54     62.30      1.03\n",
            "  scores_prime[1,2]      1.08      2.63      0.42      0.00      2.54     73.04      1.03\n",
            "  scores_prime[2,0]      1.73      2.26      1.01      0.01      3.86     37.46      1.07\n",
            "  scores_prime[2,1]      2.64      5.25      1.31      0.01      5.07    109.28      1.00\n",
            "  scores_prime[2,2]      1.16      2.18      0.41      0.00      3.21     48.93      1.06\n",
            "  scores_prime[3,0]      1.20      1.48      0.65      0.00      3.22     41.37      1.03\n",
            "  scores_prime[3,1]      1.34      1.80      0.79      0.00      3.07     99.32      1.00\n",
            "  scores_prime[3,2]      1.19      2.37      0.44      0.00      3.07     56.62      1.09\n",
            "  scores_prime[4,0]      1.35      1.79      0.72      0.00      3.21     32.24      1.06\n",
            "  scores_prime[4,1]      1.46      2.31      0.73      0.00      3.33     86.81      1.00\n",
            "  scores_prime[4,2]      1.27      2.52      0.40      0.00      3.37     43.67      1.08\n",
            "  scores_prime[5,0]      1.54      2.17      0.83      0.00      3.87     45.17      1.05\n",
            "  scores_prime[5,1]      1.81      2.36      1.01      0.00      4.86     67.18      1.00\n",
            "  scores_prime[5,2]      1.43      2.90      0.48      0.00      3.51     98.24      1.03\n",
            "  scores_prime[6,0]      1.42      1.96      0.72      0.00      3.51     47.92      1.03\n",
            "  scores_prime[6,1]      1.54      2.24      0.78      0.00      3.66     69.11      1.00\n",
            "  scores_prime[6,2]      1.12      2.25      0.41      0.00      3.01     37.10      1.07\n",
            "  scores_prime[7,0]      1.28      1.61      0.71      0.00      3.30     41.98      1.04\n",
            "  scores_prime[7,1]      1.40      1.77      0.84      0.00      3.49     55.02      1.00\n",
            "  scores_prime[7,2]      1.24      2.43      0.46      0.00      3.34     42.31      1.08\n",
            "  scores_prime[8,0]      1.47      1.71      0.97      0.00      3.32     53.04      1.01\n",
            "  scores_prime[8,1]      1.43      2.09      0.79      0.00      3.49     76.24      1.00\n",
            "  scores_prime[8,2]      1.85      4.51      0.50      0.00      4.15     93.13      1.07\n",
            "  scores_prime[9,0]      1.31      1.74      0.75      0.00      3.13     46.51      1.03\n",
            "  scores_prime[9,1]      1.52      2.31      0.80      0.00      3.92    105.42      1.00\n",
            "  scores_prime[9,2]      1.23      2.20      0.41      0.00      3.27     34.61      1.08\n",
            " scores_prime[10,0]      1.44      1.97      0.70      0.00      3.85     49.78      1.03\n",
            " scores_prime[10,1]      1.48      1.89      0.80      0.00      3.58     45.20      1.00\n",
            " scores_prime[10,2]      1.18      2.58      0.43      0.00      2.76     51.24      1.05\n",
            " scores_prime[11,0]      1.37      1.86      0.75      0.00      3.28     37.95      1.04\n",
            " scores_prime[11,1]      1.63      2.79      0.89      0.00      3.76     98.91      1.00\n",
            " scores_prime[11,2]      1.17      2.35      0.38      0.00      3.31     40.41      1.08\n",
            " scores_prime[12,0]      1.55      2.04      0.88      0.00      3.71     51.94      1.04\n",
            " scores_prime[12,1]      1.64      2.73      0.85      0.00      3.85    101.00      1.00\n",
            " scores_prime[12,2]      1.37      3.83      0.43      0.00      3.24    110.29      1.05\n",
            " scores_prime[13,0]      1.21      1.66      0.69      0.00      3.20     43.02      1.03\n",
            " scores_prime[13,1]      1.39      2.36      0.73      0.00      3.15    112.26      1.00\n",
            " scores_prime[13,2]      1.24      2.36      0.45      0.00      2.90     57.37      1.10\n",
            " scores_prime[14,0]      1.42      2.04      0.67      0.00      3.71     37.76      1.03\n",
            " scores_prime[14,1]      1.48      2.17      0.79      0.00      3.54     76.79      1.00\n",
            " scores_prime[14,2]      1.44      3.59      0.55      0.00      3.09    111.28      1.05\n",
            " scores_prime[15,0]      1.24      1.66      0.69      0.00      2.95     51.11      1.04\n",
            " scores_prime[15,1]      1.50      3.05      0.73      0.00      3.19    177.51      1.00\n",
            " scores_prime[15,2]      1.24      2.54      0.38      0.00      3.22     65.36      1.08\n",
            " scores_prime[16,0]      1.38      2.48      0.64      0.00      3.60    103.60      1.03\n",
            " scores_prime[16,1]      1.54      2.53      0.73      0.00      3.57     70.87      1.00\n",
            " scores_prime[16,2]      1.15      2.47      0.37      0.00      2.86     50.70      1.07\n",
            " scores_prime[17,0]      1.53      2.09      0.85      0.00      3.87     36.12      1.07\n",
            " scores_prime[17,1]      1.85      2.84      1.04      0.00      4.15     80.02      1.00\n",
            " scores_prime[17,2]      1.22      2.49      0.42      0.00      3.01     60.75      1.05\n",
            " scores_prime[18,0]      1.40      2.06      0.66      0.00      3.33     55.37      1.03\n",
            " scores_prime[18,1]      1.48      2.16      0.80      0.00      3.42    102.98      1.00\n",
            " scores_prime[18,2]      1.24      2.35      0.42      0.00      3.39     32.57      1.09\n",
            " scores_prime[19,0]      1.43      1.82      0.81      0.00      3.37     49.71      1.04\n",
            " scores_prime[19,1]      1.72      2.63      0.90      0.00      3.96     62.33      1.01\n",
            " scores_prime[19,2]      1.59      4.60      0.50      0.00      3.18    111.14      1.05\n",
            " scores_prime[20,0]      1.69      2.31      0.96      0.00      3.98     36.12      1.08\n",
            " scores_prime[20,1]      2.26      3.16      1.20      0.00      5.01     62.17      1.02\n",
            " scores_prime[20,2]      1.09      1.95      0.38      0.00      2.78     28.19      1.10\n",
            " scores_prime[21,0]      1.44      1.96      0.70      0.00      3.50     67.19      1.02\n",
            " scores_prime[21,1]      1.54      2.15      0.89      0.00      3.59     90.82      1.00\n",
            " scores_prime[21,2]      1.39      4.31      0.36      0.00      3.11    147.23      1.04\n",
            " scores_prime[22,0]      1.62      2.16      0.85      0.00      3.91     35.99      1.09\n",
            " scores_prime[22,1]      2.28      3.18      1.31      0.00      5.42     77.37      1.01\n",
            " scores_prime[22,2]      1.14      2.16      0.37      0.00      2.95     57.79      1.03\n",
            " scores_prime[23,0]      1.48      1.89      0.87      0.00      3.70     56.69      1.07\n",
            " scores_prime[23,1]      1.85      3.48      0.84      0.00      4.29    115.42      1.00\n",
            " scores_prime[23,2]      1.28      2.83      0.38      0.00      3.00     71.31      1.06\n",
            " scores_prime[24,0]      1.40      2.12      0.67      0.00      3.45     59.91      1.05\n",
            " scores_prime[24,1]      1.45      1.94      0.89      0.00      3.16     82.44      1.00\n",
            " scores_prime[24,2]      1.32      3.93      0.42      0.00      2.97    107.15      1.03\n",
            " scores_prime[25,0]      1.56      1.88      0.85      0.00      3.96     29.74      1.09\n",
            " scores_prime[25,1]      2.17      3.23      1.08      0.00      5.12     56.54      1.01\n",
            " scores_prime[25,2]      1.21      2.43      0.38      0.00      2.94     97.13      1.05\n",
            " scores_prime[26,0]      1.48      2.04      0.81      0.00      3.68     91.16      1.02\n",
            " scores_prime[26,1]      1.76      3.07      0.87      0.00      3.78     74.28      1.00\n",
            " scores_prime[26,2]      1.70      4.39      0.51      0.00      3.48     89.26      1.05\n",
            " scores_prime[27,0]      1.43      1.89      0.79      0.00      3.42     76.38      1.02\n",
            " scores_prime[27,1]      1.66      3.52      0.91      0.00      3.54    158.44      1.00\n",
            " scores_prime[27,2]      1.31      2.84      0.49      0.00      3.29     81.96      1.04\n",
            " scores_prime[28,0]      1.36      2.19      0.65      0.00      3.10     94.77      1.02\n",
            " scores_prime[28,1]      1.53      2.63      0.72      0.00      3.50    103.65      1.00\n",
            " scores_prime[28,2]      1.11      2.08      0.45      0.00      2.63     38.38      1.08\n",
            " scores_prime[29,0]      1.35      1.80      0.74      0.00      3.39     51.09      1.04\n",
            " scores_prime[29,1]      1.37      1.97      0.76      0.00      3.08     68.44      1.00\n",
            " scores_prime[29,2]      1.07      2.12      0.38      0.00      2.82     36.12      1.06\n",
            " scores_prime[30,0]      1.86      2.38      1.21      0.01      4.36     80.04      1.03\n",
            " scores_prime[30,1]      2.02      2.80      1.09      0.00      4.67     33.35      1.02\n",
            " scores_prime[30,2]      1.89      4.66      0.50      0.00      4.26    122.34      1.05\n",
            " scores_prime[31,0]      2.07      2.88      1.19      0.00      4.78     54.79      1.08\n",
            " scores_prime[31,1]      2.98      4.44      1.48      0.00      7.30     43.44      1.04\n",
            " scores_prime[31,2]      1.14      2.35      0.41      0.00      2.84     89.18      1.05\n",
            " scores_prime[32,0]      1.35      1.77      0.74      0.00      3.34     36.38      1.05\n",
            " scores_prime[32,1]      1.47      2.21      0.80      0.00      3.59     65.51      1.00\n",
            " scores_prime[32,2]      1.17      2.16      0.42      0.00      3.12     47.13      1.08\n",
            " scores_prime[33,0]      1.57      2.05      0.91      0.00      3.64     48.86      1.04\n",
            " scores_prime[33,1]      1.86      4.26      0.89      0.00      3.99    192.73      1.00\n",
            " scores_prime[33,2]      1.50      3.47      0.46      0.00      3.39     76.35      1.06\n",
            " scores_prime[34,0]      1.32      1.79      0.65      0.00      3.38     50.32      1.04\n",
            " scores_prime[34,1]      1.34      1.93      0.70      0.00      3.31     82.37      1.00\n",
            " scores_prime[34,2]      1.14      2.08      0.38      0.00      3.19     43.66      1.09\n",
            " scores_prime[35,0]      1.40      1.90      0.80      0.00      3.27     58.25      1.03\n",
            " scores_prime[35,1]      1.58      2.54      0.86      0.00      3.21     97.62      1.00\n",
            " scores_prime[35,2]      1.25      2.75      0.42      0.00      2.79     60.45      1.06\n",
            " scores_prime[36,0]      1.40      1.79      0.83      0.00      3.51     38.75      1.02\n",
            " scores_prime[36,1]      1.61      2.35      0.92      0.00      3.75     69.11      1.00\n",
            " scores_prime[36,2]      1.45      3.32      0.43      0.00      3.86     79.90      1.07\n",
            " scores_prime[37,0]      1.36      1.61      0.82      0.00      3.34     38.20      1.02\n",
            " scores_prime[37,1]      1.49      2.32      0.79      0.00      3.30    129.88      1.00\n",
            " scores_prime[37,2]      1.19      2.43      0.41      0.00      2.88     39.43      1.06\n",
            " scores_prime[38,0]      1.34      1.85      0.68      0.00      3.31     49.65      1.02\n",
            " scores_prime[38,1]      1.44      1.90      0.74      0.00      3.39     92.07      1.00\n",
            " scores_prime[38,2]      1.32      2.50      0.45      0.00      3.50     50.88      1.06\n",
            " scores_prime[39,0]      1.66      2.56      0.91      0.00      3.98     43.52      1.04\n",
            " scores_prime[39,1]      1.95      2.67      1.05      0.00      5.37     69.16      1.00\n",
            " scores_prime[39,2]      1.50      4.87      0.38      0.00      3.17    172.76      1.04\n",
            " scores_prime[40,0]      1.61      2.12      0.79      0.00      3.88     39.98      1.06\n",
            " scores_prime[40,1]      1.76      2.79      0.83      0.00      3.83     61.62      1.00\n",
            " scores_prime[40,2]      1.38      3.37      0.40      0.00      3.44     92.02      1.04\n",
            " scores_prime[41,0]      1.44      1.92      0.87      0.00      3.19    100.17      1.00\n",
            " scores_prime[41,1]      1.41      2.24      0.73      0.00      3.24     63.00      1.00\n",
            " scores_prime[41,2]      1.72      4.16      0.50      0.00      3.96     80.08      1.07\n",
            " scores_prime[42,0]      1.46      2.20      0.78      0.00      3.41     54.69      1.04\n",
            " scores_prime[42,1]      1.51      2.05      0.88      0.00      3.63     75.09      1.00\n",
            " scores_prime[42,2]      1.43      3.86      0.45      0.00      3.36    167.92      1.02\n",
            " scores_prime[43,0]      1.54      2.31      0.89      0.00      3.36     59.06      1.04\n",
            " scores_prime[43,1]      1.80      2.78      0.97      0.00      4.22     78.25      1.00\n",
            " scores_prime[43,2]      1.16      2.20      0.43      0.00      3.13     46.70      1.09\n",
            " scores_prime[44,0]      1.49      1.91      0.89      0.00      3.65     34.18      1.07\n",
            " scores_prime[44,1]      1.92      2.71      1.00      0.00      4.27     69.99      1.02\n",
            " scores_prime[44,2]      1.16      2.62      0.40      0.00      2.73     54.87      1.03\n",
            " scores_prime[45,0]      1.55      2.13      0.84      0.00      3.74     41.47      1.05\n",
            " scores_prime[45,1]      1.77      2.71      0.97      0.00      3.95     89.46      1.00\n",
            " scores_prime[45,2]      1.51      3.25      0.51      0.00      3.68     73.84      1.06\n",
            " scores_prime[46,0]      1.56      1.93      0.94      0.00      3.61     37.32      1.06\n",
            " scores_prime[46,1]      1.90      2.69      1.02      0.00      4.07     79.37      1.00\n",
            " scores_prime[46,2]      1.29      3.06      0.42      0.00      3.20     83.35      1.04\n",
            " scores_prime[47,0]      1.46      1.86      0.85      0.00      3.53     65.94      1.02\n",
            " scores_prime[47,1]      1.64      2.49      0.91      0.00      3.92     82.41      1.00\n",
            " scores_prime[47,2]      1.82      4.98      0.51      0.00      3.62    115.91      1.05\n",
            " scores_prime[48,0]      1.38      2.09      0.69      0.00      3.11     79.04      1.01\n",
            " scores_prime[48,1]      1.49      2.75      0.76      0.00      3.06    119.79      1.00\n",
            " scores_prime[48,2]      1.29      2.93      0.39      0.00      2.63     75.08      1.05\n",
            " scores_prime[49,0]      1.23      1.70      0.66      0.00      3.06     52.34      1.04\n",
            " scores_prime[49,1]      1.58      2.40      0.80      0.00      3.94     56.64      1.00\n",
            " scores_prime[49,2]      1.28      2.84      0.41      0.00      3.19     79.61      1.08\n",
            " scores_prime[50,0]      1.46      1.92      0.85      0.00      3.46    103.13      1.02\n",
            " scores_prime[50,1]      1.54      2.22      0.79      0.00      3.77    100.13      1.00\n",
            " scores_prime[50,2]      1.40      3.22      0.51      0.00      3.22     77.91      1.05\n",
            " scores_prime[51,0]      1.19      1.48      0.64      0.00      3.12     40.84      1.03\n",
            " scores_prime[51,1]      1.40      1.95      0.82      0.00      3.46     47.74      1.00\n",
            " scores_prime[51,2]      1.23      2.55      0.40      0.00      3.28     46.77      1.09\n",
            " scores_prime[52,0]      1.37      1.94      0.66      0.00      3.70     39.30      1.03\n",
            " scores_prime[52,1]      1.49      2.16      0.79      0.00      3.38     62.21      1.00\n",
            " scores_prime[52,2]      1.29      3.01      0.38      0.00      3.15     71.60      1.07\n",
            " scores_prime[53,0]      1.56      1.83      0.92      0.00      3.93     27.04      1.09\n",
            " scores_prime[53,1]      2.34      3.63      1.19      0.01      5.45     71.00      1.01\n",
            " scores_prime[53,2]      1.11      1.93      0.37      0.00      3.15     32.09      1.11\n",
            " scores_prime[54,0]      1.49      1.92      0.80      0.00      3.83     56.42      1.02\n",
            " scores_prime[54,1]      1.49      2.21      0.80      0.00      3.43     63.48      1.00\n",
            " scores_prime[54,2]      1.38      2.60      0.43      0.00      3.71     55.29      1.09\n",
            " scores_prime[55,0]      1.44      2.45      0.71      0.00      3.35     87.64      1.02\n",
            " scores_prime[55,1]      2.08      8.18      0.98      0.00      3.96    456.91      1.00\n",
            " scores_prime[55,2]      1.38      3.14      0.40      0.00      3.34    116.46      1.04\n",
            " scores_prime[56,0]      1.48      1.91      0.77      0.00      3.88     45.59      1.03\n",
            " scores_prime[56,1]      1.65      2.27      0.83      0.00      4.51     42.70      1.00\n",
            " scores_prime[56,2]      1.25      2.47      0.41      0.00      3.27     54.97      1.06\n",
            " scores_prime[57,0]      1.33      1.72      0.67      0.00      3.57     61.38      1.04\n",
            " scores_prime[57,1]      1.37      2.18      0.75      0.00      3.24    100.20      1.00\n",
            " scores_prime[57,2]      1.28      2.80      0.44      0.00      3.11     74.49      1.07\n",
            " scores_prime[58,0]      1.33      2.12      0.71      0.00      3.24     80.46      1.01\n",
            " scores_prime[58,1]      1.51      2.04      0.89      0.00      3.85     67.39      1.00\n",
            " scores_prime[58,2]      1.28      2.94      0.43      0.00      3.02     72.05      1.05\n",
            " scores_prime[59,0]      1.49      1.87      0.81      0.00      3.66     45.54      1.03\n",
            " scores_prime[59,1]      1.78      2.90      0.92      0.00      3.87     88.46      1.00\n",
            " scores_prime[59,2]      1.35      2.61      0.52      0.00      3.24     50.23      1.08\n",
            " scores_prime[60,0]      1.55      2.11      0.85      0.00      3.83     66.09      1.04\n",
            " scores_prime[60,1]      1.79      2.51      0.97      0.00      4.18     86.64      1.00\n",
            " scores_prime[60,2]      1.34      3.15      0.45      0.00      3.11     78.35      1.06\n",
            " scores_prime[61,0]      1.52      2.07      0.86      0.00      3.77     61.21      1.04\n",
            " scores_prime[61,1]      2.11      3.31      1.02      0.00      4.54     69.16      1.00\n",
            " scores_prime[61,2]      1.38      2.87      0.40      0.00      3.63     86.93      1.06\n",
            " scores_prime[62,0]      1.29      1.61      0.67      0.00      3.33     29.45      1.05\n",
            " scores_prime[62,1]      1.45      2.05      0.81      0.00      3.75    100.68      1.00\n",
            " scores_prime[62,2]      1.20      2.67      0.35      0.00      3.28     68.54      1.07\n",
            " scores_prime[63,0]      1.71      2.34      0.90      0.00      4.17     34.33      1.06\n",
            " scores_prime[63,1]      1.99      3.33      1.08      0.00      3.91     66.97      1.01\n",
            " scores_prime[63,2]      1.32      2.85      0.44      0.00      3.54     69.26      1.05\n",
            " scores_prime[64,0]      1.60      2.14      0.86      0.00      3.91     54.07      1.05\n",
            " scores_prime[64,1]      1.69      2.89      0.94      0.00      4.02    115.08      1.00\n",
            " scores_prime[64,2]      1.58      3.46      0.45      0.00      3.78     78.94      1.07\n",
            " scores_prime[65,0]      1.45      1.88      0.86      0.00      3.33     77.17      1.02\n",
            " scores_prime[65,1]      1.85      2.66      0.94      0.00      4.48     85.02      1.00\n",
            " scores_prime[65,2]      1.22      3.22      0.36      0.00      2.89     99.62      1.03\n",
            " scores_prime[66,0]      1.44      1.74      0.82      0.00      3.55     37.26      1.01\n",
            " scores_prime[66,1]      1.49      2.22      0.77      0.00      3.71     46.60      1.00\n",
            " scores_prime[66,2]      1.90      5.38      0.47      0.00      3.97    117.53      1.04\n",
            " scores_prime[67,0]      1.31      1.82      0.69      0.00      3.46     34.27      1.05\n",
            " scores_prime[67,1]      1.45      1.98      0.80      0.00      3.51     64.66      1.00\n",
            " scores_prime[67,2]      1.31      2.74      0.42      0.00      3.60     54.34      1.06\n",
            " scores_prime[68,0]      1.33      2.02      0.69      0.00      3.07     57.52      1.02\n",
            " scores_prime[68,1]      1.53      2.39      0.81      0.00      3.91     68.51      1.00\n",
            " scores_prime[68,2]      1.35      2.85      0.46      0.00      2.84     81.36      1.06\n",
            " scores_prime[69,0]      1.25      1.75      0.64      0.00      3.03     53.81      1.04\n",
            " scores_prime[69,1]      1.45      2.15      0.74      0.00      3.54     87.41      1.00\n",
            " scores_prime[69,2]      0.97      1.53      0.42      0.00      2.68     24.53      1.10\n",
            " scores_prime[70,0]      1.45      1.93      0.79      0.00      3.13     48.27      1.03\n",
            " scores_prime[70,1]      1.51      2.06      0.88      0.00      3.28     59.39      1.00\n",
            " scores_prime[70,2]      1.18      3.34      0.48      0.00      2.79     95.24      1.02\n",
            " scores_prime[71,0]      1.36      1.83      0.63      0.00      3.54     43.00      1.03\n",
            " scores_prime[71,1]      1.54      2.63      0.76      0.00      3.69     98.52      1.00\n",
            " scores_prime[71,2]      1.28      2.95      0.43      0.00      3.06     79.69      1.03\n",
            " scores_prime[72,0]      1.55      2.07      0.78      0.00      4.08     80.78      1.02\n",
            " scores_prime[72,1]      1.52      2.55      0.77      0.00      3.60     76.04      1.00\n",
            " scores_prime[72,2]      1.70      4.04      0.52      0.00      3.81     85.52      1.07\n",
            " scores_prime[73,0]      1.52      2.27      0.76      0.00      3.60     83.80      1.03\n",
            " scores_prime[73,1]      2.04      2.89      1.14      0.00      5.07     56.45      1.01\n",
            " scores_prime[73,2]      1.17      1.98      0.44      0.00      2.95     28.58      1.12\n",
            " scores_prime[74,0]      1.41      1.80      0.79      0.00      3.45     70.21      1.02\n",
            " scores_prime[74,1]      1.64      2.37      0.83      0.00      3.91     63.73      1.00\n",
            " scores_prime[74,2]      1.48      3.31      0.50      0.00      3.67     67.02      1.07\n",
            " scores_prime[75,0]      1.20      1.60      0.66      0.00      2.95     50.53      1.03\n",
            " scores_prime[75,1]      1.43      2.75      0.74      0.00      3.13    105.29      1.00\n",
            " scores_prime[75,2]      1.33      2.63      0.41      0.00      3.16     48.35      1.08\n",
            " scores_prime[76,0]      1.54      2.41      0.80      0.00      3.70     52.67      1.05\n",
            " scores_prime[76,1]      2.04      3.42      1.04      0.00      4.53     83.59      1.00\n",
            " scores_prime[76,2]      1.11      2.25      0.40      0.00      3.17     85.45      1.04\n",
            " scores_prime[77,0]      1.45      1.94      0.74      0.00      3.91     48.20      1.03\n",
            " scores_prime[77,1]      1.80      3.10      0.86      0.00      4.31    116.05      1.00\n",
            " scores_prime[77,2]      1.37      3.02      0.40      0.00      3.57     83.41      1.06\n",
            " scores_prime[78,0]      1.55      1.99      0.83      0.00      3.94     49.65      1.03\n",
            " scores_prime[78,1]      1.71      2.18      0.94      0.00      4.68     54.97      1.00\n",
            " scores_prime[78,2]      1.44      2.69      0.46      0.00      3.89     53.10      1.10\n",
            " scores_prime[79,0]      1.45      1.83      0.79      0.00      3.81     39.23      1.03\n",
            " scores_prime[79,1]      1.70      2.18      0.92      0.00      4.35     64.99      1.00\n",
            " scores_prime[79,2]      1.28      2.36      0.40      0.00      3.44     43.11      1.12\n",
            " scores_prime[80,0]      1.22      1.81      0.60      0.00      3.31     50.32      1.04\n",
            " scores_prime[80,1]      1.37      1.81      0.78      0.00      3.34     55.10      1.01\n",
            " scores_prime[80,2]      1.23      2.56      0.43      0.00      2.98     70.30      1.05\n",
            " scores_prime[81,0]      1.42      1.65      0.86      0.00      3.84     30.01      1.03\n",
            " scores_prime[81,1]      1.66      2.75      0.83      0.00      3.79     86.80      1.00\n",
            " scores_prime[81,2]      1.44      3.36      0.46      0.00      3.19     69.35      1.05\n",
            " scores_prime[82,0]      1.52      2.01      0.86      0.00      3.88     70.34      1.03\n",
            " scores_prime[82,1]      1.71      2.61      0.98      0.00      4.53    123.13      1.00\n",
            " scores_prime[82,2]      1.45      2.95      0.47      0.00      3.37     64.05      1.08\n",
            " scores_prime[83,0]      1.34      1.91      0.67      0.00      3.48     47.55      1.05\n",
            " scores_prime[83,1]      1.41      2.23      0.70      0.00      3.47     72.10      1.00\n",
            " scores_prime[83,2]      1.31      3.00      0.43      0.00      3.18     75.16      1.06\n",
            " scores_prime[84,0]      1.36      2.13      0.61      0.00      3.45     56.91      1.01\n",
            " scores_prime[84,1]      1.42      1.83      0.87      0.00      3.23     65.49      1.00\n",
            " scores_prime[84,2]      1.35      2.95      0.46      0.00      3.23     70.37      1.06\n",
            " scores_prime[85,0]      1.24      1.62      0.75      0.00      2.99     59.96      1.04\n",
            " scores_prime[85,1]      1.42      2.06      0.77      0.00      3.69     62.96      1.00\n",
            " scores_prime[85,2]      1.25      2.57      0.41      0.00      3.10     56.11      1.07\n",
            " scores_prime[86,0]      1.96      2.47      1.12      0.00      4.65     26.59      1.08\n",
            " scores_prime[86,1]      2.31      3.43      1.27      0.00      5.16     61.74      1.01\n",
            " scores_prime[86,2]      1.36      2.67      0.44      0.00      3.01     66.76      1.07\n",
            " scores_prime[87,0]      1.59      1.89      0.94      0.00      3.89     40.53      1.02\n",
            " scores_prime[87,1]      1.70      2.26      1.02      0.00      3.90     52.57      1.00\n",
            " scores_prime[87,2]      1.59      3.79      0.46      0.00      3.98     90.72      1.03\n",
            " scores_prime[88,0]      1.45      2.15      0.71      0.00      3.68     53.37      1.04\n",
            " scores_prime[88,1]      1.72      2.76      0.87      0.00      4.14     98.48      1.00\n",
            " scores_prime[88,2]      1.36      3.12      0.41      0.00      2.84     61.13      1.05\n",
            " scores_prime[89,0]      1.49      1.99      0.83      0.00      3.52     37.13      1.05\n",
            " scores_prime[89,1]      1.89      3.06      1.02      0.00      4.22    125.84      1.00\n",
            " scores_prime[89,2]      1.31      2.84      0.42      0.00      3.19     63.55      1.06\n",
            " scores_prime[90,0]      1.51      2.09      0.71      0.00      3.92     67.96      1.04\n",
            " scores_prime[90,1]      1.70      3.07      0.83      0.00      3.91    113.62      1.00\n",
            " scores_prime[90,2]      1.22      2.53      0.44      0.00      2.94     70.54      1.07\n",
            " scores_prime[91,0]      1.61      1.93      0.94      0.00      3.93     50.91      1.04\n",
            " scores_prime[91,1]      2.42      4.80      1.17      0.00      4.94     91.88      1.00\n",
            " scores_prime[91,2]      1.42      3.09      0.40      0.00      3.65     79.01      1.08\n",
            " scores_prime[92,0]      1.33      1.76      0.68      0.00      3.64     55.20      1.01\n",
            " scores_prime[92,1]      1.53      2.43      0.76      0.00      3.63     55.65      1.00\n",
            " scores_prime[92,2]      1.24      2.94      0.42      0.00      2.77     88.98      1.05\n",
            " scores_prime[93,0]      1.70      2.13      1.02      0.00      3.82     45.69      1.05\n",
            " scores_prime[93,1]      2.30      3.79      1.08      0.00      5.73     75.31      1.01\n",
            " scores_prime[93,2]      1.89      5.37      0.49      0.00      4.18    139.17      1.03\n",
            " scores_prime[94,0]      1.45      1.94      0.83      0.00      3.62     44.60      1.06\n",
            " scores_prime[94,1]      1.91      2.89      0.98      0.00      4.18     77.01      1.00\n",
            " scores_prime[94,2]      1.17      2.32      0.39      0.00      3.02     58.20      1.09\n",
            " scores_prime[95,0]      1.73      2.44      0.96      0.00      4.40     32.51      1.07\n",
            " scores_prime[95,1]      2.46      3.99      1.15      0.00      5.54     59.05      1.03\n",
            " scores_prime[95,2]      1.00      1.66      0.38      0.00      2.71     39.90      1.08\n",
            " scores_prime[96,0]      1.52      2.13      0.75      0.00      3.41     70.89      1.03\n",
            " scores_prime[96,1]      1.61      2.33      0.89      0.00      3.63     92.33      1.00\n",
            " scores_prime[96,2]      1.33      3.40      0.43      0.00      3.39    143.62      1.02\n",
            " scores_prime[97,0]      1.50      2.08      0.85      0.00      3.59     40.57      1.04\n",
            " scores_prime[97,1]      1.72      3.20      0.83      0.00      3.94    101.50      1.00\n",
            " scores_prime[97,2]      1.41      4.68      0.39      0.00      3.09    121.06      1.02\n",
            " scores_prime[98,0]      1.32      2.18      0.64      0.00      3.09     55.30      1.05\n",
            " scores_prime[98,1]      1.42      2.00      0.72      0.00      3.47     69.59      1.00\n",
            " scores_prime[98,2]      1.11      2.13      0.38      0.00      2.73     41.45      1.07\n",
            " scores_prime[99,0]      1.48      2.04      0.81      0.00      3.65     91.62      1.00\n",
            " scores_prime[99,1]      1.58      2.37      0.81      0.00      3.63     78.50      1.00\n",
            " scores_prime[99,2]      1.46      3.25      0.47      0.00      3.56     69.25      1.07\n",
            "scores_prime[100,0]      1.23      1.63      0.68      0.00      2.91     46.77      1.03\n",
            "scores_prime[100,1]      1.41      2.07      0.67      0.00      3.69     62.54      1.00\n",
            "scores_prime[100,2]      1.33      2.78      0.36      0.00      3.63     66.16      1.08\n",
            "scores_prime[101,0]      1.46      1.83      0.77      0.00      3.68     29.99      1.04\n",
            "scores_prime[101,1]      1.72      2.71      0.88      0.00      4.29     60.25      1.00\n",
            "scores_prime[101,2]      1.41      3.09      0.48      0.00      3.40     77.02      1.07\n",
            "scores_prime[102,0]      1.38      1.99      0.72      0.00      3.35     32.12      1.03\n",
            "scores_prime[102,1]      1.50      2.23      0.78      0.00      3.59     79.53      1.00\n",
            "scores_prime[102,2]      1.23      2.80      0.40      0.00      3.33     49.67      1.04\n",
            "scores_prime[103,0]      1.33      1.87      0.65      0.00      3.23     46.58      1.02\n",
            "scores_prime[103,1]      1.44      2.07      0.75      0.00      3.55     64.28      1.00\n",
            "scores_prime[103,2]      1.30      2.46      0.42      0.00      3.58     51.85      1.10\n",
            "scores_prime[104,0]      1.19      1.48      0.61      0.00      3.09     39.38      1.02\n",
            "scores_prime[104,1]      1.48      2.37      0.81      0.00      3.28     68.50      1.00\n",
            "scores_prime[104,2]      1.33      2.74      0.36      0.00      3.22     51.67      1.10\n",
            "scores_prime[105,0]      1.49      2.11      0.89      0.00      3.30     81.81      1.02\n",
            "scores_prime[105,1]      1.82      2.77      0.92      0.00      4.33     75.07      1.00\n",
            "scores_prime[105,2]      1.21      2.39      0.47      0.00      3.13     47.85      1.07\n",
            "scores_prime[106,0]      1.71      2.61      0.91      0.00      3.92     41.67      1.08\n",
            "scores_prime[106,1]      2.38      3.64      1.15      0.00      5.69     67.77      1.02\n",
            "scores_prime[106,2]      1.21      2.96      0.44      0.00      2.72     83.53      1.06\n",
            "scores_prime[107,0]      1.26      1.60      0.71      0.00      3.11     40.89      1.04\n",
            "scores_prime[107,1]      1.50      2.53      0.71      0.00      3.58     70.05      1.00\n",
            "scores_prime[107,2]      1.34      3.64      0.50      0.00      2.97    145.59      1.04\n",
            "scores_prime[108,0]      1.43      1.85      0.88      0.00      3.33     68.79      1.03\n",
            "scores_prime[108,1]      1.59      2.79      0.86      0.00      3.40    155.06      1.00\n",
            "scores_prime[108,2]      1.22      2.97      0.43      0.00      2.92     70.52      1.05\n",
            "scores_prime[109,0]      1.59      1.92      0.92      0.01      3.86     41.71      1.06\n",
            "scores_prime[109,1]      1.73      2.97      0.94      0.00      3.87    158.05      1.00\n",
            "scores_prime[109,2]      1.40      2.82      0.50      0.00      3.28     87.67      1.05\n",
            "scores_prime[110,0]      1.33      1.66      0.78      0.00      3.09     52.07      1.03\n",
            "scores_prime[110,1]      1.69      2.92      0.94      0.00      3.70    129.86      1.00\n",
            "scores_prime[110,2]      1.58      3.76      0.53      0.00      3.48     85.01      1.05\n",
            "scores_prime[111,0]      1.35      1.66      0.72      0.00      3.28     66.02      1.01\n",
            "scores_prime[111,1]      1.85      3.40      0.83      0.00      4.01     74.77      1.01\n",
            "scores_prime[111,2]      1.27      3.04      0.31      0.00      2.97     69.89      1.07\n",
            "scores_prime[112,0]      1.30      1.74      0.66      0.00      3.40     57.31      1.03\n",
            "scores_prime[112,1]      1.36      1.83      0.86      0.00      3.12     62.44      1.00\n",
            "scores_prime[112,2]      1.42      3.17      0.50      0.00      3.04     80.02      1.05\n",
            "scores_prime[113,0]      1.68      2.12      0.97      0.00      3.83     59.26      1.02\n",
            "scores_prime[113,1]      1.48      2.33      0.74      0.00      3.31     68.55      1.00\n",
            "scores_prime[113,2]      1.56      3.83      0.51      0.00      3.80     76.84      1.04\n",
            "scores_prime[114,0]      1.62      2.51      0.85      0.00      3.94     97.99      1.01\n",
            "scores_prime[114,1]      1.37      1.85      0.77      0.00      3.31     57.16      1.01\n",
            "scores_prime[114,2]      1.87      5.69      0.48      0.00      4.32    152.02      1.03\n",
            "scores_prime[115,0]      1.47      2.00      0.78      0.00      3.50     38.69      1.02\n",
            "scores_prime[115,1]      1.52      2.17      0.83      0.00      3.59     55.35      1.00\n",
            "scores_prime[115,2]      1.31      2.74      0.47      0.00      3.36     94.24      1.08\n",
            "scores_prime[116,0]      1.58      2.04      0.92      0.00      3.72     56.42      1.03\n",
            "scores_prime[116,1]      1.90      3.07      0.98      0.00      4.62     85.15      1.01\n",
            "scores_prime[116,2]      1.18      2.47      0.44      0.00      2.53     69.79      1.07\n",
            "scores_prime[117,0]      1.46      1.71      0.73      0.00      3.60     52.98      1.01\n",
            "scores_prime[117,1]      1.54      2.86      0.78      0.00      3.62     98.13      1.00\n",
            "scores_prime[117,2]      1.41      3.43      0.52      0.00      3.26    104.78      1.04\n",
            "scores_prime[118,0]      1.42      2.21      0.79      0.00      3.39     68.67      1.03\n",
            "scores_prime[118,1]      1.42      2.12      0.70      0.00      3.32     57.16      1.00\n",
            "scores_prime[118,2]      1.33      3.68      0.39      0.00      3.01     86.35      1.02\n",
            "scores_prime[119,0]      1.49      1.90      0.85      0.00      3.85     35.84      1.06\n",
            "scores_prime[119,1]      1.66      2.47      0.86      0.00      3.93     61.72      1.00\n",
            "scores_prime[119,2]      1.47      3.31      0.40      0.00      3.71     58.59      1.06\n",
            "scores_prime[120,0]      1.29      1.68      0.68      0.00      3.29     55.64      1.02\n",
            "scores_prime[120,1]      1.45      2.15      0.76      0.00      3.38     99.05      1.00\n",
            "scores_prime[120,2]      1.29      2.78      0.32      0.00      3.63     57.17      1.04\n",
            "scores_prime[121,0]      1.33      1.86      0.65      0.00      3.14     84.93      1.02\n",
            "scores_prime[121,1]      1.42      2.14      0.76      0.00      3.24     75.59      1.00\n",
            "scores_prime[121,2]      1.34      2.86      0.34      0.00      3.24     57.98      1.07\n",
            "scores_prime[122,0]      1.28      1.98      0.63      0.00      3.16     60.47      1.03\n",
            "scores_prime[122,1]      1.25      1.80      0.70      0.00      3.13     88.01      1.00\n",
            "scores_prime[122,2]      1.20      2.47      0.47      0.00      3.12     74.37      1.06\n",
            "scores_prime[123,0]      1.24      1.56      0.77      0.00      3.03     56.57      1.02\n",
            "scores_prime[123,1]      1.41      1.87      0.81      0.00      3.28     57.88      1.00\n",
            "scores_prime[123,2]      1.27      2.75      0.30      0.00      2.89     62.13      1.07\n",
            "scores_prime[124,0]      1.32      1.75      0.76      0.00      3.20    107.01      1.02\n",
            "scores_prime[124,1]      1.43      1.78      0.83      0.00      3.55     70.64      1.00\n",
            "scores_prime[124,2]      1.30      2.54      0.49      0.00      3.15     47.87      1.09\n",
            "scores_prime[125,0]      1.39      2.03      0.74      0.00      3.47     65.11      1.03\n",
            "scores_prime[125,1]      1.47      2.29      0.74      0.00      3.59     65.78      1.00\n",
            "scores_prime[125,2]      1.45      3.19      0.44      0.00      3.38     90.39      1.06\n",
            "scores_prime[126,0]      1.34      1.73      0.68      0.00      3.41     35.78      1.04\n",
            "scores_prime[126,1]      1.59      2.78      0.81      0.00      3.57     62.56      1.00\n",
            "scores_prime[126,2]      1.26      2.53      0.44      0.00      3.59     48.00      1.06\n",
            "scores_prime[127,0]      1.25      1.63      0.65      0.00      3.12     34.00      1.06\n",
            "scores_prime[127,1]      1.53      2.81      0.78      0.00      3.47    200.95      1.00\n",
            "scores_prime[127,2]      1.24      2.55      0.44      0.00      2.87     52.07      1.05\n",
            "scores_prime[128,0]      1.25      1.45      0.69      0.00      3.18     27.72      1.05\n",
            "scores_prime[128,1]      1.38      1.94      0.77      0.00      3.21     66.71      1.00\n",
            "scores_prime[128,2]      1.25      2.51      0.41      0.00      3.26     43.24      1.06\n",
            "scores_prime[129,0]      1.23      1.50      0.67      0.00      3.03     29.01      1.05\n",
            "scores_prime[129,1]      1.56      2.38      0.83      0.00      3.77     65.48      1.00\n",
            "scores_prime[129,2]      1.22      2.79      0.43      0.00      2.76     71.26      1.05\n",
            "scores_prime[130,0]      1.27      1.96      0.64      0.00      3.06     51.74      1.03\n",
            "scores_prime[130,1]      1.47      2.55      0.72      0.00      3.40    104.89      1.00\n",
            "scores_prime[130,2]      1.13      2.22      0.40      0.00      2.92     51.82      1.05\n",
            "scores_prime[131,0]      1.37      1.91      0.66      0.00      3.59     28.82      1.05\n",
            "scores_prime[131,1]      1.55      2.42      0.75      0.00      3.44     81.93      1.00\n",
            "scores_prime[131,2]      1.19      2.34      0.40      0.00      2.86     50.70      1.10\n",
            "scores_prime[132,0]      1.24      1.57      0.63      0.00      3.48     29.88      1.07\n",
            "scores_prime[132,1]      1.42      1.86      0.83      0.00      3.33     82.65      1.00\n",
            "scores_prime[132,2]      1.16      2.27      0.39      0.00      3.00     40.33      1.07\n",
            "scores_prime[133,0]      1.32      1.74      0.69      0.00      3.38     55.06      1.03\n",
            "scores_prime[133,1]      1.56      2.72      0.81      0.00      3.45     70.07      1.00\n",
            "scores_prime[133,2]      1.19      2.71      0.37      0.00      2.65     63.06      1.06\n",
            "scores_prime[134,0]      1.33      1.70      0.65      0.00      3.45     31.42      1.07\n",
            "scores_prime[134,1]      1.50      2.19      0.86      0.00      3.44     80.34      1.00\n",
            "scores_prime[134,2]      1.36      3.20      0.43      0.00      3.27     77.30      1.08\n",
            "scores_prime[135,0]      1.34      1.80      0.73      0.00      3.35     42.01      1.03\n",
            "scores_prime[135,1]      1.64      2.96      0.77      0.00      3.67     75.60      1.00\n",
            "scores_prime[135,2]      1.14      2.28      0.34      0.00      3.13     51.59      1.05\n",
            "scores_prime[136,0]      1.21      1.67      0.66      0.00      2.77     49.47      1.04\n",
            "scores_prime[136,1]      1.57      3.05      0.77      0.00      3.36    132.15      1.00\n",
            "scores_prime[136,2]      1.33      3.26      0.39      0.00      2.91     88.42      1.04\n",
            "scores_prime[137,0]      1.23      1.62      0.66      0.00      3.30     36.59      1.03\n",
            "scores_prime[137,1]      1.42      2.02      0.73      0.00      3.65     56.95      1.00\n",
            "scores_prime[137,2]      1.40      3.52      0.50      0.00      3.16    100.92      1.05\n",
            "scores_prime[138,0]      1.19      1.52      0.62      0.00      2.94     40.65      1.05\n",
            "scores_prime[138,1]      1.61      3.63      0.77      0.00      3.50    145.26      1.00\n",
            "scores_prime[138,2]      1.15      2.53      0.38      0.00      3.26     51.48      1.04\n",
            "scores_prime[139,0]      1.37      1.95      0.74      0.00      3.26     55.28      1.02\n",
            "scores_prime[139,1]      1.57      3.12      0.75      0.00      3.36    155.31      1.00\n",
            "scores_prime[139,2]      1.30      2.84      0.37      0.00      3.35     56.02      1.08\n",
            "scores_prime[140,0]      1.35      1.80      0.67      0.00      3.57     35.20      1.03\n",
            "scores_prime[140,1]      1.37      2.17      0.70      0.00      3.40     80.87      1.00\n",
            "scores_prime[140,2]      1.22      2.56      0.39      0.00      3.33     48.82      1.07\n",
            "scores_prime[141,0]      1.33      1.93      0.75      0.00      3.21     69.10      1.03\n",
            "scores_prime[141,1]      1.59      2.46      0.87      0.00      3.62    117.52      1.00\n",
            "scores_prime[141,2]      1.22      2.71      0.42      0.00      2.92     61.26      1.06\n",
            "scores_prime[142,0]      1.27      1.63      0.69      0.00      3.33     48.71      1.03\n",
            "scores_prime[142,1]      1.49      2.60      0.84      0.00      3.34     94.16      1.00\n",
            "scores_prime[142,2]      1.47      3.23      0.48      0.00      3.78     82.52      1.08\n",
            "scores_prime[143,0]      1.32      2.15      0.62      0.00      3.09     61.94      1.04\n",
            "scores_prime[143,1]      1.54      2.56      0.77      0.00      3.55    209.41      1.00\n",
            "scores_prime[143,2]      1.32      3.00      0.41      0.00      3.18     98.41      1.07\n",
            "scores_prime[144,0]      1.33      1.68      0.78      0.00      3.36     37.98      1.02\n",
            "scores_prime[144,1]      1.49      2.31      0.91      0.00      3.43    118.66      1.00\n",
            "scores_prime[144,2]      1.41      4.01      0.47      0.00      3.13    160.74      1.05\n",
            "scores_prime[145,0]      1.31      1.72      0.68      0.00      3.37     23.51      1.06\n",
            "scores_prime[145,1]      1.39      1.75      0.88      0.00      3.49     80.05      1.00\n",
            "scores_prime[145,2]      1.26      2.69      0.43      0.00      2.90     74.53      1.06\n",
            "scores_prime[146,0]      1.29      1.91      0.66      0.00      3.22     68.23      1.02\n",
            "scores_prime[146,1]      1.33      1.87      0.75      0.00      3.15     55.55      1.00\n",
            "scores_prime[146,2]      1.29      3.51      0.42      0.00      2.73     81.29      1.03\n",
            "scores_prime[147,0]      1.29      1.68      0.66      0.00      3.08     42.45      1.04\n",
            "scores_prime[147,1]      1.52      2.17      0.71      0.00      3.86     67.40      1.00\n",
            "scores_prime[147,2]      1.15      2.34      0.40      0.00      2.87     48.32      1.06\n",
            "scores_prime[148,0]      1.26      1.95      0.63      0.00      3.12     83.68      1.03\n",
            "scores_prime[148,1]      1.43      2.02      0.77      0.00      3.38     87.85      1.01\n",
            "scores_prime[148,2]      1.37      3.00      0.44      0.00      3.30     64.68      1.07\n",
            "scores_prime[149,0]      1.25      1.55      0.72      0.00      2.98     44.63      1.02\n",
            "scores_prime[149,1]      1.45      2.53      0.76      0.00      3.07    120.29      1.00\n",
            "scores_prime[149,2]      1.16      2.93      0.38      0.00      2.62    110.72      1.04\n",
            "scores_prime[150,0]      1.24      1.63      0.64      0.00      3.26     35.26      1.05\n",
            "scores_prime[150,1]      1.77      3.99      0.76      0.00      3.98    266.19      1.00\n",
            "scores_prime[150,2]      1.27      2.72      0.40      0.00      3.31     44.49      1.08\n",
            "scores_prime[151,0]      1.36      1.82      0.68      0.00      3.77     45.61      1.02\n",
            "scores_prime[151,1]      1.42      2.16      0.70      0.00      3.55     83.50      1.00\n",
            "scores_prime[151,2]      1.24      2.57      0.45      0.00      3.11     62.05      1.05\n",
            "scores_prime[152,0]      1.30      1.74      0.64      0.00      3.54     57.52      1.03\n",
            "scores_prime[152,1]      1.39      1.88      0.78      0.00      3.35     67.18      1.00\n",
            "scores_prime[152,2]      1.27      3.13      0.41      0.00      2.99     57.79      1.04\n",
            "scores_prime[153,0]      1.41      2.11      0.73      0.00      3.66     52.04      1.03\n",
            "scores_prime[153,1]      1.41      2.24      0.80      0.00      3.10    112.42      1.00\n",
            "scores_prime[153,2]      1.19      2.47      0.33      0.00      3.04     64.25      1.06\n",
            "scores_prime[154,0]      1.29      1.74      0.67      0.00      3.19     66.13      1.03\n",
            "scores_prime[154,1]      1.58      2.37      0.87      0.00      3.83     64.95      1.00\n",
            "scores_prime[154,2]      1.23      2.46      0.42      0.00      3.18     64.21      1.06\n",
            "scores_prime[155,0]      1.28      1.61      0.69      0.00      3.29     36.91      1.04\n",
            "scores_prime[155,1]      1.33      1.91      0.74      0.00      2.84     63.95      1.00\n",
            "scores_prime[155,2]      1.15      2.34      0.43      0.00      2.93     53.76      1.07\n",
            "scores_prime[156,0]      1.21      1.54      0.66      0.00      2.94     37.83      1.04\n",
            "scores_prime[156,1]      1.43      2.22      0.69      0.00      3.30     90.02      1.00\n",
            "scores_prime[156,2]      1.20      2.31      0.41      0.00      2.83     43.13      1.08\n",
            "scores_prime[157,0]      1.25      1.55      0.65      0.00      3.24     36.22      1.03\n",
            "scores_prime[157,1]      1.45      2.21      0.71      0.00      3.33     68.04      1.00\n",
            "scores_prime[157,2]      1.35      3.02      0.46      0.00      3.59     93.52      1.06\n",
            "scores_prime[158,0]      1.24      1.75      0.67      0.00      2.97     36.18      1.04\n",
            "scores_prime[158,1]      1.52      2.69      0.70      0.00      3.43    112.89      1.00\n",
            "scores_prime[158,2]      1.20      2.99      0.41      0.00      2.73     89.90      1.05\n",
            "scores_prime[159,0]      1.37      2.34      0.69      0.00      3.31    137.42      1.02\n",
            "scores_prime[159,1]      1.37      2.33      0.74      0.00      3.18    102.44      1.00\n",
            "scores_prime[159,2]      1.19      2.35      0.34      0.00      3.06     55.45      1.05\n",
            "scores_prime[160,0]      1.18      1.51      0.65      0.00      2.87     64.80      1.02\n",
            "scores_prime[160,1]      1.45      1.91      0.75      0.00      3.63     57.55      1.00\n",
            "scores_prime[160,2]      1.26      3.05      0.45      0.00      3.30     78.58      1.04\n",
            "scores_prime[161,0]      1.31      1.77      0.73      0.00      3.35     53.45      1.02\n",
            "scores_prime[161,1]      1.45      2.03      0.84      0.00      3.24     64.88      1.00\n",
            "scores_prime[161,2]      1.14      2.33      0.40      0.00      2.80     64.07      1.05\n",
            "scores_prime[162,0]      1.27      1.74      0.61      0.00      3.36     43.30      1.03\n",
            "scores_prime[162,1]      1.47      2.74      0.78      0.00      3.11    110.60      1.00\n",
            "scores_prime[162,2]      1.13      2.13      0.39      0.00      2.79     52.49      1.07\n",
            "scores_prime[163,0]      1.32      1.68      0.75      0.00      3.22     35.05      1.05\n",
            "scores_prime[163,1]      1.43      2.34      0.74      0.00      3.37     93.33      1.00\n",
            "scores_prime[163,2]      1.32      4.14      0.36      0.00      3.01    152.56      1.03\n",
            "scores_prime[164,0]      1.28      1.66      0.65      0.00      3.56     43.55      1.03\n",
            "scores_prime[164,1]      1.44      2.05      0.80      0.00      3.66     62.44      1.00\n",
            "scores_prime[164,2]      1.25      2.68      0.41      0.00      3.03     63.44      1.05\n",
            "scores_prime[165,0]      1.40      1.85      0.72      0.00      3.59     41.85      1.04\n",
            "scores_prime[165,1]      1.44      2.59      0.71      0.00      3.13    139.15      1.00\n",
            "scores_prime[165,2]      1.26      2.92      0.41      0.00      2.95     64.13      1.03\n",
            "scores_prime[166,0]      1.13      1.48      0.65      0.00      2.75     43.66      1.04\n",
            "scores_prime[166,1]      1.35      2.01      0.73      0.00      3.38     67.00      1.00\n",
            "scores_prime[166,2]      1.23      3.17      0.39      0.00      3.05    100.25      1.03\n",
            "scores_prime[167,0]      1.30      1.66      0.68      0.00      3.12     64.81      1.03\n",
            "scores_prime[167,1]      1.45      1.95      0.86      0.00      3.35     64.24      1.00\n",
            "scores_prime[167,2]      1.52      4.34      0.37      0.00      3.36     95.51      1.03\n",
            "scores_prime[168,0]      1.31      1.89      0.69      0.00      3.50     42.83      1.05\n",
            "scores_prime[168,1]      1.33      1.86      0.68      0.00      3.17     84.30      1.00\n",
            "scores_prime[168,2]      1.27      2.65      0.42      0.00      3.14     51.04      1.06\n",
            "scores_prime[169,0]      1.22      1.63      0.62      0.00      2.74     40.72      1.03\n",
            "scores_prime[169,1]      1.32      1.80      0.66      0.00      3.52     45.34      1.00\n",
            "scores_prime[169,2]      1.17      2.51      0.36      0.00      3.23     52.90      1.08\n",
            "scores_prime[170,0]      1.31      1.91      0.64      0.00      3.35     49.79      1.02\n",
            "scores_prime[170,1]      1.47      2.11      0.79      0.00      3.35     68.65      1.00\n",
            "scores_prime[170,2]      1.33      3.68      0.38      0.00      3.03    158.64      1.04\n",
            "scores_prime[171,0]      1.19      1.56      0.62      0.00      2.95     37.02      1.04\n",
            "scores_prime[171,1]      1.49      1.88      0.83      0.00      3.97     48.94      1.00\n",
            "scores_prime[171,2]      1.33      2.68      0.40      0.00      3.18     57.35      1.06\n",
            "scores_prime[172,0]      1.23      1.51      0.69      0.00      3.34     49.68      1.02\n",
            "scores_prime[172,1]      1.37      2.05      0.74      0.00      3.19     65.26      1.00\n",
            "scores_prime[172,2]      1.16      2.05      0.45      0.00      3.02     40.35      1.05\n",
            "scores_prime[173,0]      1.21      1.60      0.69      0.00      3.12     41.76      1.03\n",
            "scores_prime[173,1]      1.44      2.24      0.76      0.00      3.67    113.65      1.00\n",
            "scores_prime[173,2]      1.21      2.77      0.41      0.00      3.12     51.98      1.07\n",
            "scores_prime[174,0]      1.28      1.72      0.67      0.00      3.21     43.31      1.03\n",
            "scores_prime[174,1]      1.40      1.96      0.72      0.00      3.59     52.27      1.00\n",
            "scores_prime[174,2]      1.32      2.95      0.40      0.00      3.42     59.75      1.06\n",
            "scores_prime[175,0]      1.33      1.73      0.76      0.00      3.32     66.24      1.02\n",
            "scores_prime[175,1]      1.34      1.86      0.70      0.00      3.28     56.96      1.00\n",
            "scores_prime[175,2]      1.36      3.36      0.43      0.00      3.19     96.70      1.06\n",
            "scores_prime[176,0]      1.31      2.00      0.67      0.00      3.44     76.05      1.03\n",
            "scores_prime[176,1]      1.32      1.98      0.76      0.00      2.95     79.31      1.00\n",
            "scores_prime[176,2]      1.10      2.13      0.41      0.00      2.67     41.18      1.05\n",
            "scores_prime[177,0]      1.23      1.65      0.74      0.00      3.21     57.28      1.03\n",
            "scores_prime[177,1]      1.33      1.84      0.76      0.00      3.15     73.89      1.00\n",
            "scores_prime[177,2]      1.43      4.41      0.37      0.00      3.45    204.57      1.02\n",
            "scores_prime[178,0]      1.22      1.62      0.64      0.00      3.19     32.76      1.03\n",
            "scores_prime[178,1]      1.55      2.91      0.76      0.00      3.74     98.78      1.00\n",
            "scores_prime[178,2]      1.18      2.53      0.37      0.00      3.12     51.13      1.06\n",
            "scores_prime[179,0]      1.37      2.01      0.69      0.00      3.43     43.25      1.05\n",
            "scores_prime[179,1]      1.35      1.74      0.76      0.00      3.39     88.18      1.00\n",
            "scores_prime[179,2]      1.30      3.21      0.40      0.00      2.48     95.05      1.05\n",
            "              sigma      0.21      0.10      0.20      0.06      0.35    282.50      1.00\n",
            "        sigma_prime      0.17      0.09      0.15      0.04      0.30    175.49      1.01\n",
            "           wi0[0,0]      0.01      0.01      0.00      0.00      0.01    371.87      1.00\n",
            "           wi0[0,1]      0.01      0.01      0.00      0.00      0.01    440.00      1.00\n",
            "           wi0[0,2]      0.10      0.12      0.06      0.00      0.21    289.20      1.00\n",
            "           wi0[0,3]      0.00      0.01      0.00      0.00      0.01    388.55      1.00\n",
            "           wi0[0,4]      0.15      0.15      0.10      0.00      0.34    143.41      1.00\n",
            "           wi0[0,5]      0.01      0.02      0.00      0.00      0.01    418.84      1.00\n",
            "           wi0[0,6]      0.10      0.11      0.07      0.00      0.22    298.54      1.00\n",
            "           wi0[0,7]      0.89      0.61      0.78      0.10      1.63    289.77      1.00\n",
            "           wi0[0,8]      0.01      0.02      0.00      0.00      0.02    499.52      1.00\n",
            "           wi0[0,9]      0.11      0.12      0.07      0.00      0.24    343.56      1.00\n",
            "          wi0[0,10]      0.01      0.03      0.00      0.00      0.01    495.16      1.00\n",
            "          wi0[0,11]      0.07      0.08      0.04      0.00      0.19    420.51      1.00\n",
            "          wi0[0,12]      0.27      0.25      0.20      0.03      0.57    201.51      1.01\n",
            "          wi0[0,13]      0.01      0.02      0.00      0.00      0.01    502.83      1.00\n",
            "          wi0[0,14]      0.01      0.01      0.00      0.00      0.01    531.91      1.00\n",
            "          wi0[0,15]      0.08      0.12      0.05      0.00      0.16    289.80      1.00\n",
            "          wi0[0,16]      0.02      0.04      0.01      0.00      0.06    326.01      1.00\n",
            "          wi0[0,17]      0.01      0.01      0.00      0.00      0.01    491.09      1.00\n",
            "          wi0[0,18]      0.01      0.01      0.00      0.00      0.01    459.73      1.00\n",
            "          wi0[0,19]      0.01      0.03      0.00      0.00      0.01    451.95      1.00\n",
            "          wi0[0,20]      0.02      0.03      0.01      0.00      0.05    437.85      1.00\n",
            "          wi0[0,21]      0.05      0.07      0.03      0.00      0.10    425.07      1.00\n",
            "          wi0[0,22]      0.10      0.11      0.07      0.00      0.24    482.06      1.00\n",
            "          wi0[0,23]      0.10      0.12      0.07      0.00      0.21    358.18      1.00\n",
            "          wi0[0,24]      0.12      0.13      0.08      0.01      0.26    285.00      1.00\n",
            "          wi0[0,25]      0.02      0.04      0.01      0.00      0.05    345.05      1.00\n",
            "          wi0[0,26]      0.02      0.05      0.01      0.00      0.05    393.29      1.00\n",
            "          wi0[0,27]      0.05      0.06      0.03      0.00      0.11    472.25      1.00\n",
            "          wi0[0,28]      0.12      0.13      0.08      0.01      0.28    232.74      1.01\n",
            "          wi0[0,29]      0.01      0.02      0.00      0.00      0.01    432.12      1.00\n",
            "          wi0[0,30]      0.05      0.07      0.03      0.00      0.12    374.97      1.00\n",
            "          wi0[0,31]      0.45      0.36      0.34      0.08      0.90    205.59      1.00\n",
            "          wi0[0,32]      0.01      0.01      0.00      0.00      0.02    373.26      1.00\n",
            "          wi0[0,33]      0.01      0.01      0.00      0.00      0.02    385.99      1.00\n",
            "          wi0[0,34]      1.27      0.80      1.05      0.24      2.40    202.13      1.00\n",
            "          wi0[0,35]      0.01      0.03      0.00      0.00      0.02    509.96      1.00\n",
            "          wi0[0,36]      0.01      0.02      0.00      0.00      0.01    503.00      1.00\n",
            "          wi0[0,37]      0.17      0.23      0.10      0.01      0.37    272.25      1.00\n",
            "          wi0[0,38]      0.02      0.03      0.01      0.00      0.06    282.95      1.00\n",
            "          wi0[0,39]      0.02      0.03      0.01      0.00      0.05    238.26      1.00\n",
            "          wi0[0,40]      0.02      0.04      0.01      0.00      0.06    376.78      1.00\n",
            "          wi0[0,41]      0.07      0.09      0.05      0.00      0.16    399.49      1.00\n",
            "          wi0[0,42]      0.00      0.01      0.00      0.00      0.01    525.24      1.00\n",
            "          wi0[0,43]      0.01      0.01      0.00      0.00      0.02    398.91      1.00\n",
            "          wi0[0,44]      0.46      0.35      0.36      0.07      0.95    207.60      1.00\n",
            "          wi0[0,45]      0.46      0.40      0.35      0.04      0.94    191.06      1.01\n",
            "          wi0[0,46]      0.49      0.34      0.41      0.03      0.95    168.40      1.00\n",
            "          wi0[0,47]      0.01      0.02      0.00      0.00      0.02    370.13      1.01\n",
            "          wi0[0,48]      0.40      0.34      0.31      0.03      0.80    378.10      1.00\n",
            "          wi0[0,49]      0.02      0.04      0.01      0.00      0.06    370.67      1.00\n",
            "          wi0[0,50]      0.87      0.56      0.75      0.13      1.68    143.25      1.00\n",
            "          wi0[0,51]      0.23      0.24      0.16      0.01      0.45    231.06      1.00\n",
            "          wi0[0,52]      0.05      0.07      0.03      0.00      0.11    312.78      1.00\n",
            "          wi0[0,53]      0.19      0.18      0.14      0.01      0.40    223.00      1.00\n",
            "          wi0[0,54]      0.01      0.03      0.00      0.00      0.01    498.10      1.00\n",
            "          wi0[0,55]      0.02      0.04      0.01      0.00      0.06    372.43      1.00\n",
            "          wi0[0,56]      0.01      0.02      0.00      0.00      0.02    456.10      1.00\n",
            "          wi0[0,57]      0.01      0.02      0.00      0.00      0.02    398.13      1.00\n",
            "          wi0[0,58]      0.00      0.01      0.00      0.00      0.01    280.28      1.00\n",
            "          wi0[0,59]      0.02      0.04      0.01      0.00      0.06    350.79      1.00\n",
            "          wi0[0,60]      0.01      0.02      0.00      0.00      0.01    405.30      1.00\n",
            "          wi0[0,61]      0.21      0.18      0.16      0.02      0.45    332.62      1.00\n",
            "          wi0[0,62]      0.01      0.02      0.00      0.00      0.02    494.25      1.00\n",
            "          wi0[0,63]      0.01      0.02      0.00      0.00      0.02    526.98      1.00\n",
            "          wi0[0,64]      0.02      0.05      0.01      0.00      0.06    380.62      1.00\n",
            "          wi0[0,65]      0.04      0.06      0.02      0.00      0.09    366.52      1.00\n",
            "          wi0[0,66]      0.05      0.07      0.03      0.00      0.11    375.96      1.00\n",
            "          wi0[0,67]      0.33      0.32      0.25      0.02      0.61    249.59      1.01\n",
            "          wi0[0,68]      0.29      0.25      0.23      0.03      0.58    320.77      1.00\n",
            "          wi0[0,69]      0.02      0.05      0.01      0.00      0.06    489.20      1.00\n",
            "          wi0[0,70]      0.02      0.03      0.01      0.00      0.06    543.99      1.00\n",
            "          wi0[0,71]      0.00      0.01      0.00      0.00      0.01    392.46      1.00\n",
            "          wi0[0,72]      0.17      0.16      0.12      0.01      0.36    183.41      1.00\n",
            "          wi0[0,73]      0.01      0.02      0.00      0.00      0.01    394.54      1.00\n",
            "          wi0[0,74]      0.01      0.01      0.00      0.00      0.01    263.59      1.00\n",
            "          wi0[0,75]      0.01      0.02      0.00      0.00      0.01    491.63      1.00\n",
            "          wi0[0,76]      0.03      0.07      0.01      0.00      0.06    397.60      1.00\n",
            "          wi0[0,77]      0.05      0.07      0.03      0.00      0.13    269.04      1.00\n",
            "          wi0[0,78]      0.01      0.02      0.00      0.00      0.01    493.31      1.00\n",
            "          wi0[0,79]      0.01      0.02      0.00      0.00      0.01    423.45      1.00\n",
            "          wi0[0,80]      0.01      0.01      0.00      0.00      0.01    431.52      1.00\n",
            "          wi0[0,81]      0.01      0.02      0.00      0.00      0.01    497.94      1.00\n",
            "          wi0[0,82]      0.05      0.06      0.03      0.00      0.12    292.27      1.00\n",
            "          wi0[0,83]      0.03      0.05      0.01      0.00      0.07    376.16      1.00\n",
            "          wi0[0,84]      0.01      0.01      0.00      0.00      0.02    420.31      1.00\n",
            "          wi0[0,85]      0.53      0.41      0.43      0.05      1.04    428.10      1.00\n",
            "          wi0[0,86]      0.05      0.08      0.03      0.00      0.12    362.42      1.00\n",
            "          wi0[0,87]      0.03      0.04      0.01      0.00      0.07    394.45      1.00\n",
            "          wi0[0,88]      0.01      0.02      0.00      0.00      0.01    484.16      1.00\n",
            "          wi0[0,89]      0.01      0.01      0.00      0.00      0.02    290.87      1.00\n",
            "          wi0[0,90]      0.01      0.01      0.00      0.00      0.01    381.95      1.00\n",
            "          wi0[0,91]      0.74      0.53      0.61      0.09      1.37    317.76      1.00\n",
            "          wi0[0,92]      0.01      0.02      0.00      0.00      0.01    444.55      1.00\n",
            "          wi0[0,93]      0.25      0.25      0.17      0.02      0.53    263.50      1.00\n",
            "          wi0[0,94]      0.01      0.01      0.00      0.00      0.01    390.34      1.00\n",
            "          wi0[0,95]      0.18      0.18      0.12      0.01      0.39    266.29      1.00\n",
            "          wi0[0,96]      0.08      0.10      0.05      0.00      0.16    250.39      1.00\n",
            "          wi0[0,97]      0.05      0.06      0.03      0.00      0.11    467.63      1.00\n",
            "          wi0[0,98]      0.01      0.02      0.00      0.00      0.02    361.54      1.00\n",
            "          wi0[0,99]      0.01      0.02      0.00      0.00      0.01    452.45      1.00\n",
            "         wi0[0,100]      0.02      0.03      0.01      0.00      0.06    243.75      1.00\n",
            "         wi0[0,101]      0.08      0.11      0.04      0.00      0.16    341.37      1.00\n",
            "         wi0[0,102]      0.02      0.05      0.01      0.00      0.06    335.69      1.01\n",
            "         wi0[0,103]      0.21      0.21      0.14      0.01      0.41    343.47      1.01\n",
            "         wi0[0,104]      0.10      0.12      0.07      0.00      0.19    359.62      1.00\n",
            "         wi0[0,105]      0.05      0.06      0.03      0.00      0.11    395.58      1.00\n",
            "         wi0[0,106]      0.06      0.09      0.03      0.00      0.14    400.08      1.00\n",
            "         wi0[0,107]      0.09      0.10      0.06      0.00      0.18    432.56      1.00\n",
            "         wi0[0,108]      0.08      0.12      0.05      0.00      0.17    383.76      1.00\n",
            "         wi0[0,109]      0.05      0.10      0.02      0.00      0.09    462.76      1.00\n",
            "         wi0[0,110]      0.03      0.05      0.01      0.00      0.06    320.70      1.00\n",
            "         wi0[0,111]      0.01      0.02      0.00      0.00      0.02    522.05      1.00\n",
            "         wi0[0,112]      0.01      0.01      0.00      0.00      0.01    397.84      1.00\n",
            "         wi0[0,113]      1.01      0.71      0.80      0.18      2.06    251.93      1.00\n",
            "         wi0[0,114]      0.01      0.01      0.00      0.00      0.01    364.01      1.00\n",
            "         wi0[0,115]      0.17      0.20      0.11      0.01      0.35    314.81      1.02\n",
            "         wi0[0,116]      0.08      0.10      0.04      0.00      0.17    275.48      1.01\n",
            "         wi0[0,117]      0.04      0.06      0.02      0.00      0.10    397.61      1.00\n",
            "         wi0[0,118]      0.07      0.08      0.05      0.00      0.15    244.19      1.00\n",
            "         wi0[0,119]      0.01      0.01      0.00      0.00      0.01    366.57      1.00\n",
            "         wi0[0,120]      0.01      0.01      0.00      0.00      0.01    445.42      1.00\n",
            "         wi0[0,121]      0.01      0.01      0.00      0.00      0.01    379.00      1.00\n",
            "         wi0[0,122]      0.03      0.07      0.01      0.00      0.06    415.30      1.01\n",
            "         wi0[0,123]      0.38      0.31      0.30      0.04      0.77    149.80      1.00\n",
            "         wi0[0,124]      0.01      0.01      0.00      0.00      0.01    385.77      1.00\n",
            "         wi0[0,125]      0.01      0.01      0.00      0.00      0.01    363.10      1.00\n",
            "         wi0[0,126]      0.01      0.02      0.00      0.00      0.01    437.83      1.00\n",
            "         wi0[0,127]      0.01      0.02      0.00      0.00      0.02    452.30      1.00\n",
            "         wi0[0,128]      0.03      0.05      0.01      0.00      0.07    444.22      1.00\n",
            "         wi0[0,129]      0.08      0.13      0.05      0.00      0.18    385.90      1.00\n",
            "         wi0[0,130]      0.01      0.01      0.00      0.00      0.01    341.64      1.00\n",
            "         wi0[0,131]      0.25      0.27      0.18      0.02      0.50    365.91      1.00\n",
            "         wi0[0,132]      0.01      0.02      0.00      0.00      0.01    332.07      1.00\n",
            "         wi0[0,133]      0.01      0.02      0.00      0.00      0.01    216.72      1.00\n",
            "         wi0[0,134]      0.01      0.01      0.00      0.00      0.02    457.40      1.00\n",
            "         wi0[0,135]      0.01      0.01      0.00      0.00      0.01    395.59      1.00\n",
            "         wi0[0,136]      0.01      0.02      0.00      0.00      0.01    445.13      1.00\n",
            "         wi0[0,137]      0.01      0.02      0.00      0.00      0.02    352.76      1.01\n",
            "         wi0[0,138]      0.03      0.05      0.01      0.00      0.07    421.20      1.00\n",
            "         wi0[0,139]      0.01      0.03      0.00      0.00      0.02    367.27      1.00\n",
            "         wi0[0,140]      0.03      0.05      0.01      0.00      0.06    412.48      1.00\n",
            "         wi0[0,141]      0.01      0.01      0.00      0.00      0.01    491.09      1.00\n",
            "         wi0[0,142]      0.05      0.08      0.03      0.00      0.11    139.56      1.01\n",
            "         wi0[0,143]      0.02      0.06      0.01      0.00      0.05    350.26      1.00\n",
            "         wi0[0,144]      0.05      0.07      0.03      0.00      0.10    416.79      1.00\n",
            "         wi0[0,145]      0.03      0.05      0.01      0.00      0.07    378.10      1.00\n",
            "         wi0[0,146]      0.03      0.07      0.01      0.00      0.05    463.47      1.01\n",
            "         wi0[0,147]      0.01      0.02      0.00      0.00      0.01    391.73      1.00\n",
            "         wi0[0,148]      0.08      0.14      0.05      0.00      0.18    369.60      1.00\n",
            "         wi0[0,149]      0.01      0.01      0.00      0.00      0.01    186.94      1.00\n",
            "         wi0[0,150]      0.13      0.16      0.09      0.01      0.27    253.66      1.01\n",
            "         wi0[0,151]      0.01      0.02      0.00      0.00      0.01    474.23      1.00\n",
            "         wi0[0,152]      0.00      0.01      0.00      0.00      0.01    552.25      1.00\n",
            "         wi0[0,153]      0.05      0.06      0.02      0.00      0.10    297.91      1.00\n",
            "         wi0[0,154]      0.05      0.08      0.03      0.00      0.11    226.95      1.00\n",
            "         wi0[0,155]      0.01      0.03      0.00      0.00      0.01    449.31      1.00\n",
            "         wi0[0,156]      0.01      0.01      0.00      0.00      0.01    474.37      1.00\n",
            "         wi0[0,157]      0.02      0.04      0.01      0.00      0.06    404.58      1.00\n",
            "         wi0[0,158]      0.01      0.01      0.00      0.00      0.02    375.52      1.00\n",
            "         wi0[0,159]      0.01      0.01      0.00      0.00      0.01    446.76      1.00\n",
            "         wi0[0,160]      0.05      0.07      0.03      0.00      0.11    263.31      1.00\n",
            "         wi0[0,161]      0.10      0.11      0.07      0.00      0.21    204.22      1.00\n",
            "         wi0[0,162]      0.01      0.02      0.00      0.00      0.01    301.54      1.00\n",
            "         wi0[0,163]      0.01      0.01      0.00      0.00      0.02    497.65      1.00\n",
            "         wi0[0,164]      0.01      0.02      0.00      0.00      0.01    413.97      1.00\n",
            "         wi0[0,165]      0.01      0.02      0.00      0.00      0.01    421.18      1.00\n",
            "         wi0[0,166]      0.34      0.30      0.26      0.03      0.64    263.06      1.01\n",
            "         wi0[0,167]      0.90      0.62      0.74      0.11      1.75    145.93      1.00\n",
            "         wi0[0,168]      0.01      0.01      0.00      0.00      0.01    498.67      1.00\n",
            "         wi0[0,169]      0.15      0.14      0.11      0.01      0.34    364.83      1.00\n",
            "         wi0[0,170]      0.05      0.07      0.03      0.00      0.10    280.12      1.00\n",
            "         wi0[0,171]      0.01      0.02      0.00      0.00      0.02    378.06      1.00\n",
            "         wi0[0,172]      0.08      0.10      0.04      0.00      0.18    284.34      1.00\n",
            "         wi0[0,173]      0.07      0.10      0.04      0.00      0.17    323.44      1.00\n",
            "         wi0[0,174]      0.01      0.02      0.00      0.00      0.02    396.87      1.00\n",
            "         wi0[0,175]      0.01      0.01      0.00      0.00      0.01    279.19      1.00\n",
            "         wi0[0,176]      0.08      0.11      0.04      0.00      0.17    346.82      1.00\n",
            "         wi0[0,177]      0.03      0.07      0.01      0.00      0.06    459.03      1.01\n",
            "         wi0[0,178]      0.01      0.03      0.00      0.00      0.01    395.51      1.00\n",
            "         wi0[0,179]      0.05      0.07      0.03      0.00      0.12    235.01      1.00\n",
            "         wi0[0,180]      0.01      0.02      0.00      0.00      0.02    433.56      1.00\n",
            "         wi0[0,181]      0.01      0.02      0.00      0.00      0.01    455.64      1.00\n",
            "         wi0[0,182]      0.01      0.01      0.00      0.00      0.01    493.58      1.00\n",
            "         wi0[0,183]      0.01      0.01      0.00      0.00      0.01    347.72      1.00\n",
            "         wi0[0,184]      0.01      0.02      0.00      0.00      0.02    201.06      1.00\n",
            "         wi0[0,185]      0.01      0.01      0.00      0.00      0.01    286.91      1.00\n",
            "         wi0[0,186]      0.01      0.01      0.00      0.00      0.01    459.16      1.00\n",
            "         wi0[0,187]      0.01      0.01      0.00      0.00      0.01    485.79      1.00\n",
            "         wi0[0,188]      0.01      0.02      0.00      0.00      0.01    382.97      1.00\n",
            "         wi0[0,189]      0.01      0.02      0.00      0.00      0.01    482.87      1.00\n",
            "         wi0[0,190]      0.01      0.01      0.00      0.00      0.01    344.97      1.01\n",
            "         wi0[0,191]      0.01      0.02      0.00      0.00      0.01    463.68      1.00\n",
            "         wi0[0,192]      0.01      0.01      0.00      0.00      0.01    308.05      1.00\n",
            "         wi0[0,193]      0.00      0.01      0.00      0.00      0.01    400.31      1.00\n",
            "         wi0[0,194]      0.01      0.02      0.00      0.00      0.02    511.41      1.01\n",
            "         wi0[0,195]      0.01      0.02      0.00      0.00      0.01    418.99      1.00\n",
            "         wi0[0,196]      0.01      0.02      0.00      0.00      0.01    486.13      1.00\n",
            "         wi0[0,197]      0.01      0.01      0.00      0.00      0.01    457.69      1.00\n",
            "         wi0[0,198]      0.01      0.01      0.00      0.00      0.01    256.45      1.00\n",
            "         wi0[0,199]      0.01      0.03      0.00      0.00      0.01    441.39      1.00\n",
            "         wi0[0,200]      0.01      0.01      0.00      0.00      0.01    398.22      1.00\n",
            "         wi0[0,201]      0.01      0.01      0.00      0.00      0.01    485.82      1.00\n",
            "         wi0[0,202]      0.01      0.03      0.00      0.00      0.02    445.66      1.00\n",
            "         wi0[0,203]      0.00      0.01      0.00      0.00      0.01    532.09      1.01\n",
            "         wi0[0,204]      0.01      0.01      0.00      0.00      0.01    470.78      1.00\n",
            "         wi0[0,205]      0.01      0.02      0.00      0.00      0.02    524.00      1.00\n",
            "         wi0[0,206]      0.01      0.02      0.00      0.00      0.01    519.47      1.00\n",
            "         wi0[0,207]      0.00      0.01      0.00      0.00      0.01    414.78      1.00\n",
            "         wi0[0,208]      0.01      0.02      0.00      0.00      0.01    205.87      1.00\n",
            "         wi0[0,209]      0.01      0.01      0.00      0.00      0.02    407.26      1.00\n",
            "         wi0[0,210]      0.01      0.02      0.00      0.00      0.02    475.99      1.00\n",
            "         wi0[0,211]      0.01      0.02      0.00      0.00      0.01    273.39      1.00\n",
            "         wi0[0,212]      0.01      0.02      0.00      0.00      0.01    498.17      1.00\n",
            "         wi0[0,213]      0.01      0.02      0.00      0.00      0.02    378.80      1.00\n",
            "         wi0[0,214]      0.01      0.02      0.00      0.00      0.02    310.54      1.00\n",
            "         wi0[0,215]      0.01      0.02      0.00      0.00      0.02    368.57      1.00\n",
            "         wi0[0,216]      0.01      0.01      0.00      0.00      0.02    187.92      1.01\n",
            "         wi0[0,217]      0.01      0.01      0.00      0.00      0.01    380.10      1.00\n",
            "         wi0[0,218]      0.01      0.01      0.00      0.00      0.01    467.16      1.00\n",
            "         wi0[0,219]      0.01      0.02      0.00      0.00      0.02    484.80      1.00\n",
            "         wi0[0,220]      0.01      0.02      0.00      0.00      0.02    341.54      1.00\n",
            "         wi0[0,221]      0.01      0.03      0.00      0.00      0.02    394.71      1.01\n",
            "         wi0[0,222]      0.01      0.01      0.00      0.00      0.01    368.45      1.00\n",
            "         wi0[0,223]      0.01      0.01      0.00      0.00      0.02    238.12      1.00\n",
            "         wi0[0,224]      0.01      0.02      0.00      0.00      0.01    277.89      1.00\n",
            "         wi0[0,225]      0.01      0.02      0.00      0.00      0.02    479.15      1.00\n",
            "         wi0[0,226]      0.01      0.02      0.00      0.00      0.02    422.13      1.00\n",
            "         wi0[0,227]      0.01      0.02      0.00      0.00      0.01    488.56      1.00\n",
            "         wi0[0,228]      0.00      0.01      0.00      0.00      0.01    344.00      1.00\n",
            "         wi0[0,229]      0.01      0.01      0.00      0.00      0.02    329.36      1.00\n",
            "         wi0[0,230]      0.01      0.02      0.00      0.00      0.01    477.66      1.00\n",
            "         wi0[0,231]      0.01      0.02      0.00      0.00      0.02    454.62      1.00\n",
            "         wi0[0,232]      0.01      0.01      0.00      0.00      0.01    452.43      1.00\n",
            "         wi0[0,233]      0.01      0.01      0.00      0.00      0.01    492.72      1.00\n",
            "         wi0[0,234]      0.01      0.02      0.00      0.00      0.01    334.31      1.00\n",
            "         wi0[0,235]      0.01      0.03      0.00      0.00      0.01    459.87      1.00\n",
            "         wi0[0,236]      0.01      0.01      0.00      0.00      0.01    468.15      1.00\n",
            "         wi0[0,237]      0.01      0.02      0.00      0.00      0.01    463.26      1.00\n",
            "         wi0[0,238]      0.01      0.02      0.00      0.00      0.01    315.53      1.00\n",
            "         wi0[0,239]      0.01      0.02      0.00      0.00      0.01    505.96      1.01\n",
            "         wi0[0,240]      0.01      0.01      0.00      0.00      0.01    451.92      1.00\n",
            "         wi0[0,241]      0.01      0.01      0.00      0.00      0.01    394.28      1.01\n",
            "         wi0[0,242]      0.01      0.01      0.00      0.00      0.02    501.35      1.00\n",
            "         wi0[0,243]      0.01      0.02      0.00      0.00      0.02    451.36      1.00\n",
            "         wi0[0,244]      0.01      0.01      0.00      0.00      0.01    365.90      1.00\n",
            "         wi0[0,245]      0.01      0.01      0.00      0.00      0.02    442.95      1.00\n",
            "         wi0[0,246]      0.01      0.02      0.00      0.00      0.02    332.35      1.00\n",
            "         wi0[0,247]      0.01      0.02      0.00      0.00      0.01    361.94      1.00\n",
            "         wi0[0,248]      0.01      0.03      0.00      0.00      0.02    426.03      1.00\n",
            "         wi0[0,249]      0.00      0.01      0.00      0.00      0.01    453.23      1.00\n",
            "         wi0[0,250]      0.01      0.02      0.00      0.00      0.02    478.33      1.00\n",
            "         wi0[0,251]      0.01      0.02      0.00      0.00      0.02    351.63      1.00\n",
            "         wi0[0,252]      0.01      0.02      0.00      0.00      0.01    444.32      1.00\n",
            "         wi0[0,253]      0.00      0.01      0.00      0.00      0.01    499.95      1.00\n",
            "         wi0[0,254]      0.01      0.02      0.00      0.00      0.01    425.68      1.00\n",
            "         wi0[0,255]      0.01      0.01      0.00      0.00      0.01    483.05      1.00\n",
            "         wi0[0,256]      0.01      0.01      0.00      0.00      0.01    498.22      1.00\n",
            "         wi0[0,257]      0.01      0.01      0.00      0.00      0.01    325.43      1.00\n",
            "         wi0[0,258]      0.01      0.01      0.00      0.00      0.01    281.51      1.00\n",
            "         wi0[0,259]      0.01      0.01      0.00      0.00      0.01    468.62      1.00\n",
            "         wi0[0,260]      0.01      0.01      0.00      0.00      0.01    298.21      1.00\n",
            "         wi0[0,261]      0.01      0.02      0.00      0.00      0.01    468.45      1.00\n",
            "         wi0[0,262]      0.01      0.02      0.00      0.00      0.02    376.74      1.00\n",
            "         wi0[0,263]      0.01      0.01      0.00      0.00      0.01    450.09      1.00\n",
            "         wi0[0,264]      0.01      0.01      0.00      0.00      0.01    190.57      1.00\n",
            "         wi0[0,265]      0.00      0.01      0.00      0.00      0.01    404.42      1.00\n",
            "         wi0[0,266]      0.01      0.01      0.00      0.00      0.01    274.68      1.00\n",
            "         wi0[0,267]      0.01      0.01      0.00      0.00      0.01    351.16      1.00\n",
            "         wi0[0,268]      0.01      0.01      0.00      0.00      0.02    337.46      1.00\n",
            "         wi0[0,269]      0.01      0.01      0.00      0.00      0.01    379.74      1.00\n",
            "           wj0[0,0]      0.01      0.01      0.00      0.00      0.01    458.93      1.00\n",
            "           wj0[0,1]      0.72      0.52      0.62      0.06      1.37     62.90      1.05\n",
            "           wj0[0,2]      0.61      0.44      0.50      0.06      1.15    271.92      1.01\n",
            "           wj0[0,3]      0.01      0.01      0.00      0.00      0.01    466.89      1.00\n",
            "           wj0[0,4]      0.01      0.01      0.00      0.00      0.01    499.06      1.00\n",
            "           wj0[0,5]      0.26      0.23      0.18      0.03      0.52    250.97      1.00\n",
            "           wj0[0,6]      0.01      0.02      0.00      0.00      0.01    329.45      1.00\n",
            "           wj0[0,7]      0.01      0.01      0.00      0.00      0.01    430.66      1.00\n",
            "           wj0[0,8]      0.43      0.37      0.32      0.05      0.90    208.09      1.01\n",
            "           wj0[0,9]      0.01      0.01      0.00      0.00      0.01    463.36      1.01\n",
            "          wj0[0,10]      0.01      0.01      0.00      0.00      0.01    278.99      1.00\n",
            "          wj0[0,11]      0.01      0.01      0.00      0.00      0.02    392.33      1.00\n",
            "          wj0[0,12]      0.07      0.08      0.04      0.00      0.14    323.11      1.00\n",
            "          wj0[0,13]      0.01      0.02      0.00      0.00      0.02    433.46      1.01\n",
            "          wj0[0,14]      0.01      0.04      0.00      0.00      0.01    417.11      1.00\n",
            "          wj0[0,15]      0.01      0.02      0.00      0.00      0.01    473.71      1.00\n",
            "          wj0[0,16]      0.01      0.01      0.00      0.00      0.02    380.39      1.00\n",
            "          wj0[0,17]      0.06      0.10      0.03      0.00      0.13    264.34      1.00\n",
            "          wj0[0,18]      0.01      0.01      0.00      0.00      0.01    454.66      1.00\n",
            "          wj0[0,19]      0.04      0.05      0.02      0.00      0.08    298.58      1.01\n",
            "          wj0[0,20]      0.42      0.37      0.31      0.03      0.85    197.25      1.00\n",
            "          wj0[0,21]      0.02      0.04      0.01      0.00      0.06    211.37      1.00\n",
            "          wj0[0,22]      0.31      0.26      0.24      0.02      0.62    174.98      1.00\n",
            "          wj0[0,23]      0.03      0.05      0.02      0.00      0.07    381.32      1.01\n",
            "          wj0[0,24]      0.02      0.03      0.01      0.00      0.05    520.26      1.01\n",
            "          wj0[0,25]      0.13      0.12      0.09      0.01      0.26    288.90      1.00\n",
            "          wj0[0,26]      0.07      0.07      0.04      0.00      0.16    373.25      1.00\n",
            "          wj0[0,27]      0.02      0.09      0.01      0.00      0.05    404.31      1.00\n",
            "          wj0[0,28]      0.01      0.01      0.00      0.00      0.01    369.38      1.00\n",
            "          wj0[0,29]      0.01      0.01      0.00      0.00      0.02    401.61      1.00\n",
            "          wj0[0,30]      0.89      0.60      0.74      0.08      1.70    199.04      1.00\n",
            "          wj0[0,31]      0.98      0.69      0.81      0.08      1.78     66.82      1.04\n",
            "          wj0[0,32]      0.01      0.02      0.00      0.00      0.02    438.86      1.00\n",
            "          wj0[0,33]      0.06      0.07      0.04      0.00      0.15    216.91      1.00\n",
            "          wj0[0,34]      0.01      0.04      0.00      0.00      0.02    443.06      1.00\n",
            "          wj0[0,35]      0.02      0.03      0.01      0.00      0.04    349.50      1.00\n",
            "          wj0[0,36]      0.12      0.12      0.08      0.01      0.25    289.97      1.00\n",
            "          wj0[0,37]      0.02      0.04      0.01      0.00      0.06    282.85      1.00\n",
            "          wj0[0,38]      0.02      0.04      0.01      0.00      0.06    395.08      1.00\n",
            "          wj0[0,39]      0.17      0.16      0.12      0.01      0.36    307.75      1.00\n",
            "          wj0[0,40]      0.04      0.06      0.02      0.00      0.09    412.72      1.00\n",
            "          wj0[0,41]      0.13      0.15      0.08      0.01      0.28    145.83      1.00\n",
            "          wj0[0,42]      0.02      0.06      0.01      0.00      0.05    322.40      1.00\n",
            "          wj0[0,43]      0.02      0.03      0.01      0.00      0.05    247.47      1.00\n",
            "          wj0[0,44]      0.14      0.18      0.09      0.01      0.29    198.90      1.00\n",
            "          wj0[0,45]      0.04      0.05      0.02      0.00      0.09    380.64      1.00\n",
            "          wj0[0,46]      0.23      0.26      0.17      0.01      0.45    331.83      1.00\n",
            "          wj0[0,47]      0.21      0.19      0.15      0.02      0.47    216.74      1.00\n",
            "          wj0[0,48]      0.01      0.02      0.00      0.00      0.02    156.87      1.00\n",
            "          wj0[0,49]      0.01      0.01      0.00      0.00      0.01    351.13      1.01\n",
            "          wj0[0,50]      0.09      0.11      0.06      0.01      0.19    238.14      1.00\n",
            "          wj0[0,51]      0.01      0.01      0.00      0.00      0.02    294.89      1.00\n",
            "          wj0[0,52]      0.01      0.02      0.00      0.00      0.01    249.12      1.01\n",
            "          wj0[0,53]      0.31      0.27      0.25      0.02      0.56    211.34      1.00\n",
            "          wj0[0,54]      0.05      0.08      0.03      0.00      0.11    293.44      1.00\n",
            "          wj0[0,55]      0.04      0.07      0.02      0.00      0.09    181.79      1.03\n",
            "          wj0[0,56]      0.02      0.06      0.01      0.00      0.05    348.81      1.00\n",
            "          wj0[0,57]      0.01      0.01      0.00      0.00      0.01    254.00      1.00\n",
            "          wj0[0,58]      0.01      0.01      0.00      0.00      0.02    425.39      1.00\n",
            "          wj0[0,59]      0.11      0.12      0.07      0.00      0.22    359.50      1.00\n",
            "          wj0[0,60]      0.03      0.04      0.02      0.00      0.07    342.38      1.01\n",
            "          wj0[0,61]      0.07      0.07      0.05      0.00      0.15    344.72      1.00\n",
            "          wj0[0,62]      0.01      0.01      0.00      0.00      0.01    526.40      1.00\n",
            "          wj0[0,63]      0.29      0.24      0.22      0.03      0.54    354.33      1.00\n",
            "          wj0[0,64]      0.17      0.18      0.12      0.01      0.34    230.22      1.00\n",
            "          wj0[0,65]      0.08      0.08      0.06      0.00      0.17    331.20      1.00\n",
            "          wj0[0,66]      0.07      0.08      0.04      0.00      0.14    230.34      1.01\n",
            "          wj0[0,67]      0.01      0.02      0.00      0.00      0.02    248.87      1.00\n",
            "          wj0[0,68]      0.01      0.01      0.00      0.00      0.02    278.80      1.00\n",
            "          wj0[0,69]      0.01      0.03      0.00      0.00      0.02    466.32      1.00\n",
            "          wj0[0,70]      0.05      0.08      0.02      0.00      0.10    382.01      1.00\n",
            "          wj0[0,71]      0.01      0.01      0.00      0.00      0.01    443.29      1.00\n",
            "          wj0[0,72]      0.07      0.09      0.04      0.00      0.16    356.35      1.00\n",
            "          wj0[0,73]      0.10      0.09      0.07      0.00      0.20    298.03      1.00\n",
            "          wj0[0,74]      0.09      0.10      0.06      0.00      0.21    313.15      1.01\n",
            "          wj0[0,75]      0.01      0.02      0.00      0.00      0.02    365.34      1.00\n",
            "          wj0[0,76]      0.04      0.05      0.02      0.00      0.07    395.96      1.00\n",
            "          wj0[0,77]      0.02      0.07      0.01      0.00      0.04    516.62      1.00\n",
            "          wj0[0,78]      0.45      0.42      0.34      0.05      0.86    177.57      1.00\n",
            "          wj0[0,79]      0.04      0.05      0.02      0.00      0.08    354.89      1.00\n",
            "          wj0[0,80]      0.01      0.01      0.00      0.00      0.01    473.42      1.01\n",
            "          wj0[0,81]      0.06      0.08      0.04      0.00      0.14    179.14      1.00\n",
            "          wj0[0,82]      0.06      0.06      0.04      0.00      0.14    255.45      1.01\n",
            "          wj0[0,83]      0.01      0.02      0.00      0.00      0.01    518.91      1.00\n",
            "          wj0[0,84]      0.01      0.01      0.00      0.00      0.01    283.40      1.00\n",
            "          wj0[0,85]      0.01      0.01      0.00      0.00      0.02    186.47      1.00\n",
            "          wj0[0,86]      0.79      0.51      0.68      0.17      1.50    229.98      1.00\n",
            "          wj0[0,87]      0.36      0.34      0.26      0.03      0.70    232.70      1.00\n",
            "          wj0[0,88]      0.02      0.04      0.01      0.00      0.06    382.20      1.00\n",
            "          wj0[0,89]      0.15      0.17      0.11      0.01      0.31    229.93      1.00\n",
            "          wj0[0,90]      0.02      0.04      0.01      0.00      0.06    324.45      1.00\n",
            "          wj0[0,91]      0.52      0.41      0.42      0.04      0.96    225.82      1.00\n",
            "          wj0[0,92]      0.01      0.02      0.00      0.00      0.02    442.33      1.00\n",
            "          wj0[0,93]      0.63      0.48      0.50      0.07      1.22    265.06      1.01\n",
            "          wj0[0,94]      0.06      0.12      0.03      0.00      0.14    412.37      1.00\n",
            "          wj0[0,95]      0.27      0.23      0.20      0.03      0.55    166.56      1.02\n",
            "          wj0[0,96]      0.08      0.10      0.05      0.01      0.17    248.23      1.00\n",
            "          wj0[0,97]      0.02      0.03      0.01      0.00      0.04    472.81      1.00\n",
            "          wj0[0,98]      0.01      0.02      0.00      0.00      0.01    350.35      1.01\n",
            "          wj0[0,99]      0.05      0.09      0.02      0.00      0.11    219.45      1.01\n",
            "         wj0[0,100]      0.01      0.01      0.00      0.00      0.02    286.28      1.00\n",
            "         wj0[0,101]      0.02      0.03      0.01      0.00      0.04    376.40      1.00\n",
            "         wj0[0,102]      0.01      0.01      0.00      0.00      0.02    212.93      1.00\n",
            "         wj0[0,103]      0.01      0.01      0.00      0.00      0.01    257.76      1.00\n",
            "         wj0[0,104]      0.01      0.01      0.00      0.00      0.01    501.36      1.00\n",
            "         wj0[0,105]      0.04      0.06      0.02      0.00      0.09    442.10      1.00\n",
            "         wj0[0,106]      0.32      0.25      0.27      0.03      0.62    245.53      1.00\n",
            "         wj0[0,107]      0.01      0.02      0.00      0.00      0.01    398.04      1.00\n",
            "         wj0[0,108]      0.04      0.06      0.02      0.00      0.09    274.62      1.00\n",
            "         wj0[0,109]      0.14      0.13      0.10      0.01      0.29    258.65      1.01\n",
            "         wj0[0,110]      0.11      0.13      0.07      0.01      0.26    249.59      1.00\n",
            "         wj0[0,111]      0.09      0.11      0.06      0.00      0.18    361.79      1.00\n",
            "         wj0[0,112]      0.01      0.02      0.00      0.00      0.01    316.22      1.00\n",
            "         wj0[0,113]      0.38      0.32      0.27      0.01      0.81    144.09      1.00\n",
            "         wj0[0,114]      0.23      0.23      0.16      0.02      0.49    188.30      1.00\n",
            "         wj0[0,115]      0.02      0.03      0.01      0.00      0.04    423.23      1.00\n",
            "         wj0[0,116]      0.08      0.09      0.05      0.00      0.18    331.11      1.00\n",
            "         wj0[0,117]      0.05      0.07      0.02      0.00      0.10    188.40      1.01\n",
            "         wj0[0,118]      0.01      0.02      0.00      0.00      0.02    195.96      1.02\n",
            "         wj0[0,119]      0.04      0.05      0.02      0.00      0.09    357.38      1.00\n",
            "         wj0[0,120]      0.01      0.01      0.00      0.00      0.01    495.65      1.00\n",
            "         wj0[0,121]      0.01      0.03      0.00      0.00      0.02    353.47      1.00\n",
            "         wj0[0,122]      0.01      0.02      0.00      0.00      0.02    468.23      1.00\n",
            "         wj0[0,123]      0.01      0.02      0.00      0.00      0.01    318.46      1.00\n",
            "         wj0[0,124]      0.01      0.03      0.00      0.00      0.01    501.76      1.00\n",
            "         wj0[0,125]      0.01      0.02      0.00      0.00      0.01    273.90      1.00\n",
            "         wj0[0,126]      0.01      0.01      0.00      0.00      0.01    339.16      1.00\n",
            "         wj0[0,127]      0.01      0.02      0.00      0.00      0.01    432.72      1.00\n",
            "         wj0[0,128]      0.01      0.02      0.00      0.00      0.02    373.32      1.00\n",
            "         wj0[0,129]      0.01      0.01      0.00      0.00      0.01    459.17      1.00\n",
            "         wj0[0,130]      0.01      0.02      0.00      0.00      0.01    277.76      1.01\n",
            "         wj0[0,131]      0.01      0.02      0.00      0.00      0.02    371.50      1.00\n",
            "         wj0[0,132]      0.01      0.02      0.00      0.00      0.01    460.17      1.01\n",
            "         wj0[0,133]      0.00      0.01      0.00      0.00      0.01    330.24      1.00\n",
            "         wj0[0,134]      0.01      0.02      0.00      0.00      0.01    394.50      1.00\n",
            "         wj0[0,135]      0.01      0.04      0.00      0.00      0.01    423.53      1.00\n",
            "         wj0[0,136]      0.01      0.02      0.00      0.00      0.01    349.34      1.00\n",
            "         wj0[0,137]      0.01      0.03      0.00      0.00      0.02    208.38      1.01\n",
            "         wj0[0,138]      0.01      0.01      0.00      0.00      0.01    483.77      1.00\n",
            "         wj0[0,139]      0.01      0.02      0.00      0.00      0.01    414.89      1.01\n",
            "         wj0[0,140]      0.01      0.02      0.00      0.00      0.02    337.94      1.01\n",
            "         wj0[0,141]      0.01      0.01      0.00      0.00      0.01    274.00      1.00\n",
            "         wj0[0,142]      0.01      0.02      0.00      0.00      0.01    342.60      1.00\n",
            "         wj0[0,143]      0.01      0.02      0.00      0.00      0.01    514.33      1.00\n",
            "         wj0[0,144]      0.01      0.02      0.00      0.00      0.01    495.16      1.00\n",
            "         wj0[0,145]      0.01      0.01      0.00      0.00      0.01    463.54      1.00\n",
            "         wj0[0,146]      0.01      0.01      0.00      0.00      0.01    404.99      1.00\n",
            "         wj0[0,147]      0.01      0.03      0.00      0.00      0.02    491.37      1.00\n",
            "         wj0[0,148]      0.01      0.02      0.00      0.00      0.02    408.97      1.00\n",
            "         wj0[0,149]      0.01      0.01      0.00      0.00      0.01    316.58      1.00\n",
            "         wj0[0,150]      0.01      0.01      0.00      0.00      0.01    377.73      1.01\n",
            "         wj0[0,151]      0.01      0.03      0.00      0.00      0.01    372.34      1.00\n",
            "         wj0[0,152]      0.01      0.02      0.00      0.00      0.02    377.94      1.00\n",
            "         wj0[0,153]      0.01      0.01      0.00      0.00      0.01    360.37      1.00\n",
            "         wj0[0,154]      0.01      0.01      0.00      0.00      0.01    232.94      1.00\n",
            "         wj0[0,155]      0.01      0.02      0.00      0.00      0.02    350.57      1.00\n",
            "         wj0[0,156]      0.01      0.03      0.00      0.00      0.02    375.49      1.00\n",
            "         wj0[0,157]      0.01      0.01      0.00      0.00      0.02    356.00      1.00\n",
            "         wj0[0,158]      0.01      0.01      0.00      0.00      0.01    275.67      1.01\n",
            "         wj0[0,159]      0.01      0.01      0.00      0.00      0.01    300.27      1.00\n",
            "         wj0[0,160]      0.01      0.02      0.00      0.00      0.02    461.10      1.00\n",
            "         wj0[0,161]      0.01      0.02      0.00      0.00      0.02    436.37      1.00\n",
            "         wj0[0,162]      0.01      0.02      0.00      0.00      0.02    378.48      1.01\n",
            "         wj0[0,163]      0.01      0.03      0.00      0.00      0.01    514.46      1.00\n",
            "         wj0[0,164]      0.01      0.02      0.00      0.00      0.01    416.59      1.00\n",
            "         wj0[0,165]      0.01      0.01      0.00      0.00      0.01    419.76      1.00\n",
            "         wj0[0,166]      0.01      0.01      0.00      0.00      0.01    283.02      1.00\n",
            "         wj0[0,167]      0.01      0.01      0.00      0.00      0.01    431.26      1.00\n",
            "         wj0[0,168]      0.01      0.02      0.00      0.00      0.02    451.86      1.00\n",
            "         wj0[0,169]      0.01      0.02      0.00      0.00      0.01    466.20      1.00\n",
            "         wj0[0,170]      0.01      0.02      0.00      0.00      0.01    461.20      1.01\n",
            "         wj0[0,171]      0.01      0.01      0.00      0.00      0.01    434.49      1.00\n",
            "         wj0[0,172]      0.01      0.01      0.00      0.00      0.01    394.06      1.01\n",
            "         wj0[0,173]      0.01      0.02      0.00      0.00      0.02    439.32      1.00\n",
            "         wj0[0,174]      0.00      0.01      0.00      0.00      0.01    350.81      1.01\n",
            "         wj0[0,175]      0.01      0.03      0.00      0.00      0.01    488.32      1.00\n",
            "         wj0[0,176]      0.01      0.02      0.00      0.00      0.02    326.77      1.00\n",
            "         wj0[0,177]      0.01      0.02      0.00      0.00      0.02    341.60      1.00\n",
            "         wj0[0,178]      0.01      0.03      0.00      0.00      0.02    389.97      1.01\n",
            "         wj0[0,179]      0.01      0.02      0.00      0.00      0.02    383.25      1.00\n",
            "\n",
            "Number of divergences: 25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "samples = infer.get_samples()\n",
        "# infer.Plot_Traces()\n",
        "# infer.Plot_Histograms()"
      ],
      "metadata": {
        "id": "xSeks7k0vkxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2r4PG8Vtw8oN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Posterior_distribution(samples_vals=samples, original_val=args,\n",
        "                       num_samples=args_mcmc['num_samples'],\n",
        "                       thinning=args_mcmc['thinning'],\n",
        "                       num_chains=args_mcmc['num_chains'],\n",
        "                       L=L_new, L_prime=L_prime_new)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VTNvjNOwqUwF",
        "outputId": "b9d227f8-e8e0-4e97-aebe-867e8cddd455"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wi0 Shape:(2, 270)\n",
            "Wj0 Shape:(2, 180)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "Too many indices: 1-dimensional array indexed with 3 regular indices.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-8c462b309d73>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m Posterior_distribution(samples_vals=samples, original_val=args,\n\u001b[0m\u001b[1;32m      2\u001b[0m                        \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs_mcmc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_samples'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                        \u001b[0mthinning\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs_mcmc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'thinning'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                        \u001b[0mnum_chains\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs_mcmc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_chains'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                        L=L_new, L_prime=L_prime_new)\n",
            "\u001b[0;32m<ipython-input-52-0e7304cca390>\u001b[0m in \u001b[0;36mPosterior_distribution\u001b[0;34m(samples_vals, original_val, num_samples, thinning, num_chains, L, L_prime)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m       \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'blue'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m       \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWi0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtgraph_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mind_max_wi0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'red'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m   \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'blue'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'95% CI'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/array.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m             out.aval, sharding, [out], committed=False, _skip_checks=True)\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlax_numpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rewriting_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36m_rewriting_take\u001b[0;34m(arr, idx, indices_are_sorted, unique_indices, mode, fill_value)\u001b[0m\n\u001b[1;32m   8951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8952\u001b[0m   \u001b[0mtreedef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatic_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_split_index_for_jit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8953\u001b[0;31m   return _gather(arr, treedef, static_idx, dynamic_idx, indices_are_sorted,\n\u001b[0m\u001b[1;32m   8954\u001b[0m                  unique_indices, mode, fill_value)\n\u001b[1;32m   8955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36m_gather\u001b[0;34m(arr, treedef, static_idx, dynamic_idx, indices_are_sorted, unique_indices, mode, fill_value)\u001b[0m\n\u001b[1;32m   8960\u001b[0m             unique_indices, mode, fill_value):\n\u001b[1;32m   8961\u001b[0m   \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_merge_static_and_dynamic_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtreedef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatic_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8962\u001b[0;31m   \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_index_to_gather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shared with _scatter_update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   8963\u001b[0m   \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36m_index_to_gather\u001b[0;34m(x_shape, idx, normalize_indices)\u001b[0m\n\u001b[1;32m   9068\u001b[0m                      normalize_indices: bool = True) -> _Indexer:\n\u001b[1;32m   9069\u001b[0m   \u001b[0;31m# Remove ellipses and add trailing slice(None)s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 9070\u001b[0;31m   \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_canonicalize_tuple_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   9071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9072\u001b[0m   \u001b[0;31m# Check for scalar boolean indexing: this requires inserting extra dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36m_canonicalize_tuple_index\u001b[0;34m(arr_ndim, idx)\u001b[0m\n\u001b[1;32m   9392\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mnum_dimensions_consumed\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0marr_ndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9393\u001b[0m     \u001b[0mindex_or_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"index\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnum_dimensions_consumed\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"indices\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 9394\u001b[0;31m     raise IndexError(\n\u001b[0m\u001b[1;32m   9395\u001b[0m         \u001b[0;34mf\"Too many indices: {arr_ndim}-dimensional array indexed \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9396\u001b[0m         f\"with {num_dimensions_consumed} regular {index_or_indices}.\")\n",
            "\u001b[0;31mIndexError\u001b[0m: Too many indices: 1-dimensional array indexed with 3 regular indices."
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Posterior_distribution(samples_vals=samples, original_val=args,\n",
        "#                        num_samples=args_mcmc['num_smaples'],\n",
        "#                        thinning=args_mcmc['num_smaples'],\n",
        "#                        num_chains=args_mcmc['num_chains'],\n",
        "#                        L=L_new, L_prime=L_prime_new)"
      ],
      "metadata": {
        "id": "33qFEzEIqUrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xuupyGAXqUXB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}